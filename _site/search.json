[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "lpembleton.site",
    "section": "",
    "text": "GitHub Deploy Keys\n\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nssh key\n\n\nAWS\n\n\nremote\n\n\ncloud\n\n\n\n\nA guide to using GitHub deploy keys to deploy private repos in remote compute sessions\n\n\n\n\n\n\nJul 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nScreen Utility and Terminal Multiplexing\n\n\n\n\n\n\n\nScreen\n\n\nAWS\n\n\nTerminal\n\n\n\n\nA guide to using the screen command in remote sessions\n\n\n\n\n\n\nMay 23, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nEndpoints for AWS VPC\n\n\n\n\n\n\n\nAWS\n\n\nEndpoints\n\n\nGateway\n\n\nBatch\n\n\n\n\nA guide to setting up endpoints for your AWS VPC\n\n\n\n\n\n\nMay 18, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nEBS Auto-scaling on AWS Batch\n\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\nEBS\n\n\n\n\nAn introductory guide to setting up EBS auto-scaling on AWS Batch for use in Nextflow\n\n\n\n\n\n\nFeb 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nNextflow on AWS Batch\n\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\n\n\nAn introductory guide to setting up Nextflow with AWS Batch\n\n\n\n\n\n\nNov 25, 2022\n\n\nLW Pembleton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "üéì Doctor of Philosophy, Molecular Genetics from La Trobe University, 2014\nüéì Bachelor of Agricultural Science, Rural Technology from the University of Queensland, 2010\n\n\nDr Luke Pembleton is a Genomic Breeding Scientist and Strategic Science Manager at Barenbrug. He leads the development and implementation of commercial genomic breeding and associated ‚Äòomic‚Äô technologies.\n\n\n\nMost of my technical work is focused on the fields of üß¨ Genomics, Genomic Selection and Genomic Breeding in plants, primarily forages üå±. Skilled in bioinformatics üë®‚Äçüíª and passionate about the R programing language. Enjoy developing cloud computing ‚òÅÔ∏è environments for research and production pipelines, often orchestrated with the workflow manager Nextflow üîÇ.\n\n\n\n Genomic Breeding Scientist | Strategic Science Manager\nBarenbrug | August 2021 - Current\n~\n Senior Research Scientist\nAgriculture Victoria Research | April 2017 - August 2021\n~\n Research Scientist\nAgriculture Victoria Research | October 2014 - March 2017\n~\n Research Assistant\nVictorian Department of Primary Industries | June 2009 - December 2009"
  },
  {
    "objectID": "posts/aws-batch-ebs-autoscale/index.html",
    "href": "posts/aws-batch-ebs-autoscale/index.html",
    "title": "EBS Auto-scaling on AWS Batch",
    "section": "",
    "text": "Photo¬†by¬†Simon¬†Goetz¬†on¬†Unsplash\n\n\nAnyone who has tried running bioinformatic pipelines on AWS batch with a workflow manager such as Nextflow will be well aware of the common error\nerror: No space left on device\nthat can plague a pipeline. Yes, you can adjust your EBS allocation with specific AMI images or launch configurations and tailor them to specific tasks, but the dynamic nature of bioinformaticslogy means this will likely be an ongoing cat üêà and mouse üêÅ game.\nYes, Nextflow has the fantastic resume feature if your pipeline has already completed a large proportion of tasks, unfortunately though the config file is not reanalysed upon resume, so you cannot point to a new AMI with an increased EBS volume.\nThe solution? Automatic scaling of your EBS volumes in real-time. Essentially there is a script that resides within your AMI that continously monitors disk usage, and just before you reach 100%, it provisions a new EBS volume mounting it directly to your running EC2 instance. You also get the added benefit of better EBS cost optimisation üí∞ as you no longer need to ‚Äòover provision‚Äô your batch EC2 instances.\n\nThe setup can be split into two components, installing the auto-scaling scripts in your AMI and updating your Batch compute environments with appropriate permissions.\n\nSetup an appropriate IAM Role\n\nClick Create role under the IAM AWS console and select AWS service as the trusted entity type and EC2 as the use case, then click Next.\nClick Create policy and select the JSON tab.\nPaste the following JSON code and click Next.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:DescribeVolumeStatus\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeTags\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:DescribeVolumeAttribute\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteVolume\",\n                \"ec2:CreateTags\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\nAdd any tags if applicable and click Next\nGive your policy a name e.g.¬†amazon-ebs-autoscale-policy and click Create policy\nNow under the Add permission menu of your new IAM Role, select your newly created policy, i.e.¬†amazon-ebs-autoscale-policy and click Next\nGive your Role and name, e.g.¬†amazon-ebs-autoscale-role and click Create role\nYou also need to add the amazon-ebs-autoscale-policy policy role to the ecsInstanceRolerole you use in your AWS Batch compute environments.\nUnder Roles in the AWS IAM console, find and click the ecsInstanceRole role.\nClick Add permission and select Attach policies. Find/search for your new amazon-ebs-autoscale-policy, select it and click Attach policies.\n\n\n\nInstall the auto-scale scripts\n\nFetch or clone the amazon-ebs-autoscale repository to your local computer.\nEdit the EBS mount location to the volume that docker utilises by adding the -m /var/lib/docker parameter to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file\nSpecify the initial drive to use for the mount point to be /dev/xvdba with the -d parameter\nBy default, the 100GiB volume will be initially provisioned at startup to change this add the -s parameter again to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file. For example, to reduce it to 30GB use -s 30\nthe runcmd: section should now look something like this:\n\nruncmd:\n  - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\"\n  - unzip -q /tmp/awscliv2.zip -d /tmp && /tmp/aws/install\n  - EBS_AUTOSCALE_VERSION=$(curl --silent \"https://api.github.com/repos/awslabs/amazon-ebs-autoscale/releases/latest\" | jq -r .tag_name)\n  - cd /opt && git clone https://github.com/awslabs/amazon-ebs-autoscale.git\n  - cd /opt/amazon-ebs-autoscale && git checkout $EBS_AUTOSCALE_VERSION\n  - sh /opt/amazon-ebs-autoscale/install.sh -m /var/lib/docker -d /dev/xvdba -s 30 2>&1 > /var/log/ebs-autoscale-install.log\n\nTo install the amazon-ebs-autoscale scripts with your defined parameters into your chosen AMI you can use the aws ec2 run-instance command from the aws-cli. An example of launching your chosen AMI and installing the amazon-ebs-autoscale scripts is\n\naws ec2 run-instances --image-id YOUR-AMI-ID \\\n  --key-name YOUR-KEY-PAIR-NAME \\\n  --subnet-id YOUR-SUBNET-ID \\\n  --user-data file://./templates/cloud-init-userdata.yaml \\\n  --count 1 \\\n  --security-group-ids YOUR-SECURITY-GROUP-ID \\\n  --instance-type t2.micro \\\n  --iam-instance-profile Name=amazon-ebs-autoscale-role\n\nRunning this from your command line will launch an EC2 instance which you can then save as a new AMI with an appropriate name. (see my Nextflow on AWS Batch blog post for details on how to save AMIs and use them in later Batch compute environments)\n\nThe final step is to reconfigure your Batch compute environment to utilise the EBS autoscaling AMI\n\nClick Computer environments in the AWS Batch console and click Create\nSelect Amazon Elastic Compute Cloud (Amazon EC2) as the configuration and Managed as the orchestration type and enter an appropriate name.\nEnsure AWSServiceRoleForBatch is selected as the service role and ecsInstanceRole as the instance role and click Next page\n\n\n\n\n\n\nNote\n\n\n\nThe updated ecsInstanceRole now contains the permissions required for autoscaling.\n\n\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the number of parallel vCPU tasks running in your compute environment. Increase or decrease this to an appropriate number based on your requirements.\nWith EBS autoscaling implemented, you can now generally use optimal as the allowed instance type without many of the previous risks of errors and failed pipelines.\nUnder ‚ÄòAdditional configuration‚Äô you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren‚Äôt available, AWS Batch waits for the additional instances to be available. If there aren‚Äôt enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don‚Äôt run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you‚Äôre using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn‚Äôt supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren‚Äôt available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‚ÄòEC2 configuration‚Äô click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‚ÄòImage ID override‚Äô box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‚ÄòAmazon Elastic Compute Cloud (Amazon EC2)‚Äô as the compute environment.\nEnter a suitable name for your new job queue (üìù take note of this name you will need it later)\nUnder ‚ÄòConnected compute environments‚Äô select the compute environment that you just created\nClick Create job queue\n\nYou now need to update the config files of your Nextflow pipelines to point to the new Batch job queue.\nNow enjoy your Nextflow pipelines with no reduced errors."
  },
  {
    "objectID": "posts/aws-endpoints/index.html",
    "href": "posts/aws-endpoints/index.html",
    "title": "Endpoints for AWS VPC",
    "section": "",
    "text": "Photo¬†by¬†Varun¬†Yadav¬†on¬†Unsplash\n\n\nIf you are using AWS Batch and don‚Äôt have endpoints set up for your VPC, you need to do it right now‚ùó √ç‚Äôm sure AWS have their reasons, but for any bioinformatic VPC, these should be set up by default.\nWithout endpoints, AWS batch jobs in your private VPC requiring access to S3 storage (yes, even your own S3 buckets) or ECR for your docker containers will actually have to go via your NAT gateway and the internet. Not only does this reduce security, but AWS charges exuberant NAT gateway processing fees per GB of data that passes through üí∏ With thousands of jobs and large genomic üß¨ datasets and docker images, you will find these fees (listed as EC2 other) will cost you more than the actual EC2 instances.\n\n\n\nS3 and ECR access without endpoints.\n\n\nAdditional important benefits are reduced intermittent errors and faster run times. Previously with all communication running through your NAT gateway it would easily become overwhelmed and spin out errors if numerous jobs were transferring large volumes of data (which is the norm in bioinformatics). In turn, this processing would slow your job run times.\nEndpoints provide a way for your network traffic to remain ‚Äòlocal‚Äô within amazon‚Äôs networks and avoid any NAT gateway fees, and their setup is easy!\n\n\n\nS3 and ECR access with endpoints.\n\n\nFor communication with S3 you can use a AWS gateway endpoint:\nUnder the AWS services menu, go to the VPC console, select Endpoints and click Create endpoint.\n\n\nEnter a relevant name and leave AWS services selected as default.\nIn the Services search field, enter ‚ÄòS3‚Äô and press ‚Äòenter‚Äô.\nSelect the service name com.amazonaws.[REGIOIN].s3 that has ‚ÄòGateway‚Äô as the ‚ÄòType‚Äô field.\n\nChoose your VPC.\nUnder Route tables, choose the ID that matches your Private subnet (‚ÑπÔ∏è tip: scroll across)\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nAny S3 requests from your private VPC batch jobs will now be processed through the gateway endpoint.\nAlthough your ECR containers are stored in the S3 system, you need to set up ECR and ECS endpoints for the orchestration communication required to setup these containers in your batch jobs.\nSetting up an endpoint for ECR & ECS traffic requires a few more steps:\nAgain under your VPC console, select endpoints and click Create endpoint. Following similar steps as above.\n\nSearch in the Services search bar for ‚ÄòECR‚Äô and select com.amazonaws.[REGION].ecr.api\n\nThe type is now ‚ÄòInterface‚Äô.\nChoose your VPC.\nSelect your private subnet.\nSelect either your default security group or one that has your required restrictions.\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nNow repeat this process three more times for\n\ncom.amazonaws.[REGION].ecr.dkr\ncom.amazonaws.[REGION].ecs-agent\ncom.amazonaws.[REGION].ecs-telemetry\n\nYou are now all setup üéâ If all is setup correctly you should now notice a big drop ‚ÜòÔ∏è in your fees, less ‚ÜòÔ∏è errors and faster üèéÔ∏è job run times.\nThis should be the default‚Ä¶ ü§∑"
  },
  {
    "objectID": "posts/github-deploy-keys/index.html",
    "href": "posts/github-deploy-keys/index.html",
    "title": "GitHub Deploy Keys",
    "section": "",
    "text": "Photo¬†by¬†Chunli¬†Ju¬†on¬†Unsplash\n\n\nIf you use private GitHub repositories and have wondered what is the best way of being able to deploy your repositories to remote compute environments such as AWS, you are not alone. However, there is an elegant and secure solution that doesn‚Äôt require you to store your high-level GitHub SSH key in a remote instance.\nGitHub has a feature called deploy keys which allows you to add a separate SSH key to your private repository without write access. Furthermore, you can secure this key on your remote compute instance with a password, similar to what you might do on your local machine.\n\nThe first step, remote into your remote compute instance ‚òÅÔ∏è\nRun the following command to generate your SSH key\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are only going to use one repository, accept the default name when prompted to ‚ÄúEnter a file in which to save the key‚Äù. However, if you want to connect to multiple repositories don‚Äôt press Enter to accept the defaults. Instead, enter a name that relates to the corresponding repository you are wanting to connect to.\n\n\n\nUse cat to print the public SSH key to the screen and then copy it to your clipboard üìã\ncat /home/<USER-NAME>/.ssh/<KEY-NAME>.pub\nGo to your GitHub repository and click Settings.\n\nThen click Deploy keys from the side menu and then click Add deploy key.\n\nPaste in your public ssh key and add a relevant title and click Add key\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure you leave ‚ÄòAllow write access‚Äô un-ticked.\n\n\nMultiple repositories?\nWhat‚Äôs that you say, you have more than one repository, I would be worried if you didn‚Äôt. However, what you will initially find is that GitHub will not let you use the same ssh deploy key for a second repo. The solution? Generate a second key, but now you have to manually change between keys when you move between repos, and even more so remember which key is for which.\nAs always with Linux and git there is a elegant solution, a ssh config file.\n\nCreate a ssh config file with the following commands\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nUsing a command line text editor like vi edit the config file\nvi ~/.ssh/config\nAdd the following configuration information\nHost repo-alias\n  Hostname github.com\n  IdentityFile=/home/<USER-NAME>/.ssh/<KEY-NAME>\nWhat are these fields?\n\nHost: the repostory‚Äôs alias on GitHub\nHostname: the hosting service i.e.¬†github.com\nIdentityFile: file path to the ssh key you previously generated\n\nRepeat this process for any other repositories and SSH keys\nIf you have added the alias and host name correctly, Git should automatically select the appropriate SSH key.\n\nWith these steps, you have now streamlined your deployment process and ensured the appropriate SSH key is automatically selected. I hope this guide has been helpful in demystifying the process of deploying private repositories and managing SSH keys.\nHappy coding! üßë‚Äçüíªüîë"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html",
    "href": "posts/nextflow-on-aws-batch/index.html",
    "title": "Nextflow on AWS Batch",
    "section": "",
    "text": "Photo¬†by¬†Jenessaa¬†Lu¬†on¬†Unsplash\nThe following is a general guide on how to set up Nextflow with AWS batch as the compute environment. I would highly recommend that you use your local environment or at least a smaller test dataset for pipeline development, transferring to AWS batch when in a working production state.\nAlthough Gandalf üßô trims his beard more often than Amazon updates their AWS user interface, I cannot guarantee the included menu screenshots will look the same on your system. However, hopefully they will still provide sufficient information to determine the appropriate settings and options. Reach out if you feel I need to update this guide."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "href": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "title": "Nextflow on AWS Batch",
    "section": "IAM Setup",
    "text": "IAM Setup\nFirstly you need to create a new IAM with more appropriate permissions tailored to the requirements listed in Nextflow documentation. It is strongly recommended that do not use your root account to run Nextflow pipelines.\n\nOpen the IAM management console on AWS and add a new user\nEnter an appropriate user name for example ‚ÄòNextflow-access‚Äô. Under access type, select programmatic access\n\nNext you need to create a user group for the new user to sit within. Generally, on AWS you will apply permissions to a user group rather than a specific user. Additionally, this allows you to set up multiple separate people within the ‚ÄòNextflow group‚Äô. Again, enter an appropriate name and click Create group\nAdd any metadata tags if appropriate\nClick Create user. You should be greeted with a new page that includes a Access Key ID and SCA (üìù take note of these keys as you will need them towards the end of this guide)\n\nNow that you have your new user and Nextflow group you will need to apply the required permissions.\n\nFrom the IAM user panel click User groups select your recently created ‚Äònextflow‚Äô group, and under the permissions menu click on the Attach policy button\n\nClick Create policy\n\nUse the visual editor to add all the required permissions\n\nMinimal permissions policies to be attached to the AWS account used by Nextflow are:\n\nTo interface AWS Batch:\n\"batch:DescribeJobQueues\"\n\"batch:CancelJob\"\n\"batch:SubmitJob\"\n\"batch:ListJobs\"\n\"batch:DescribeComputeEnvironments\"\n\"batch:TerminateJob\"\n\"batch:DescribeJobs\"\n\"batch:RegisterJobDefinition\"\n\"batch:DescribeJobDefinitions\"\nTo be able to see the EC2 instances:\n\"ecs:DescribeTasks\"\n\"ec2:DescribeInstances\"\n\"ec2:DescribeInstanceTypes\"\n\"ec2:DescribeInstanceAttribute\"\n\"ecs:DescribeContainerInstances\"\n\"ec2:DescribeInstanceStatus\"\nTo pull container images stored in the ECR repositories:\n\"ecr:GetAuthorizationToken\"\n\"ecr:BatchCheckLayerAvailability\"\n\"ecr:GetDownloadUrlForLayer\"\n\"ecr:GetRepositoryPolicy\"\n\"ecr:DescribeRepositories\"\n\"ecr:ListImages\"\n\"ecr:DescribeImages\"\n\"ecr:BatchGetImage\"\n\"ecr:GetLifecyclePolicy\"\n\"ecr:GetLifecyclePolicyPreview\"\n\"ecr:ListTagsForResource\"\n\"ecr:DescribeImageScanFindings\"\n\nYou also need to add permissions for S3 so that nextlflow can pull input data and publish results. Still using the visual editor select S3 as the service and then select the All S3 actions (s3:*) check box under actions. You may get notifications of other ‚Äòdependency‚Äô type permissions that are required, follow the instructions to add these as well.\n\nAdd any metadata tags if appropriate\nGive your new policy a name and click Create policy\nSelect your newly created permission policy to add to the user group and click Add permissions. Hint: you can find your new policy by üîçsearching in the filter box\n\nTo be able to use spot instances you will need to create an additional role.\n\nClick Roles under the IAM access management menu and click Create role\n\nSelect AWS service and EC2 under common use cases, click Next\nSearch for AmazonEC2SpotFleetTaggingRole select it and click Next\nAdd a role name, e.g.¬†AmazonEC2SpotFleetRole and click Create role"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "href": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "title": "Nextflow on AWS Batch",
    "section": "Custom Nextflow AMI",
    "text": "Custom Nextflow AMI\nAWS batch uses Amazon Machine Images (AMIs) to initiate EC2 compute instances that will subsequently run your Nextflow processes. Nextflow tasks submitted to AWS Batch will run under the Amazon Elastic Container Service (ECS). ECS (not to be confused with EC2) uses a base Amazon ECS-optimised AMI (with docker pre-installed). Although Nextflow & Batch will control the CPU and memory resource request and allocation you need to ensure you base ECS AMI has sufficient EBS storage to hold any relevant input and working data files, such as sequence reads, indexes etc. You will also need to install the AWS CLI in the base ECS AMI to allow data movement to and from S3 buckets. To set all this up follow these steps:\n\nNavigate to the EC2 console menu\nClick Instances and then Launch Instances\nUnder ‚Äòquick start‚Äô click Browse more AMIs\nClick AWS Marketplace AMIs and search for ECS\nAt the time of writing amzn2-ami-ecs-hvm-2.0.20221025-x86_64-ebs was the most up-to-date ECS AMI. Select it\n\nSelect the t2.micro instance type\nSelect and relevant key pairs and network settings based on your setup (I would recommend at a minimum a private VPC and IP-restricted connections via a bastion instance)\nEnsure you have at least 30GiB storage üíæ listed under ‚ÄòConfigure storage‚Äô. Also change the storage type from gp2 to gp3 (for a performance boost at no additional cost - see Matt Vaughn‚Äôs NextflowSummit 2022 talk üìΩÔ∏è).\n\n\n\n\n\n\nNote\n\n\n\nFor some Nextflow processes your will need more than 30GiB of EBS storage. I would recommend making additional AMIs (based on this image) for these specific tasks and assigning them to specific Batch job queues, see later on.\n\n\nClick Launch instance üöÄ\nSSH üíª into your new instance where you will need to install AWS CLI\nOnce connected run the following commands to install AWS CLI\ncd $HOME\nsudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nTo verify the install was successful\n$ ./miniconda/bin/aws --version\naws-cli/1.19.79 Python/3.8.5 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.20.79\nUnder the Instances menu in the EC2 console select your relevant instance and click Actions, then Images and Templates, then Create Image\nGive your new image a name e.g.¬†nextflow-30GiB-ecs-ami and click Create image\nüìùTake note of the AMI ID (not the name) that you just generated as you will need this later\n\n\n\n\n\n\n\nNote\n\n\n\nContrary to what is commonly written in other documentation you no longer need to expand your docker üêãstorage volume to match your allocated EBS storage size. The docker storage automatically expands on the Amazon 2 AMIs which are now default (unlike previous Amazon 1 AMIs)."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "href": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "title": "Nextflow on AWS Batch",
    "section": "Batch Environment",
    "text": "Batch Environment\nNow it is time to create your Batch environment which entails at least one compute environment and one job queue that Nextflow will submit processes to.\nNavigate to the Batch AWS console and click on Compute environments.\n\nClick Create and select Amazon Elastic Compute Cloud (Amazon EC2) as the compute environment.\nSelect Managed as the orchestration type and enter a suitable name for your new compute environment.\nIf this is your first time setting up a Batch environment AWS will create the relevant service role and instance role. Just ensure ‚ÄòCreate new role‚Äô is selected. Alternatively, under ‚ÄòService role‚Äô select AWSServiceRoleForBatch and under ‚ÄòInstance Role‚Äô select ecsInstanceRole. Click Next Page\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the allowed maximum number of parallel vCPU tasks that can run in your compute environment at any one time. Increase or decrease this to an appropriate number based on your requirements.\n‚ÄòAllowed instance type‚Äô allows you to control the type of instances that AWS is allowed to try and run your jobs on. Your CPU and memory requirements defined in your Nextflow config will apply a second tier of filtering (i.e.¬†if your memory request is higher than an allowed instance type, obviously that instance type won‚Äôt be used). You can leave this as ‚Äòoptimal‚Äô and AWS will attempt to find the best instance type match to your CPU and memory request.\n\n\n\n\n\n\nNote\n\n\n\nAWS will generally group multiple jobs onto the one large instance, however, this can result in errors, particularly from noisy neighbors, and I/O and/or network intensive tasks.\nIf you want to prevent AWS from grouping multiple jobs onto the one larger instance, then you need to specifically define smaller instances types, e.g.¬†r6i.xlarge, r6i.2xlarge, to prevent AWS using super instances such as r6i.24xlarge r6i.32xlarge.\n\n\nTo use spot instances toggle the Use EC2 Spot instances button at the top and define your maximum cut-off for on-demand price under ‚ÄòMaximum % on-demand price‚Äô. Under ‚Äòspot fleet role‚Äô you will also need to select the AmazonEC2SpotFleetRole role that you created earlier.\nUnder ‚ÄòAdditional configuration‚Äô you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren‚Äôt available, AWS Batch waits for the additional instances to be available. If there aren‚Äôt enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don‚Äôt run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you‚Äôre using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn‚Äôt supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren‚Äôt available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‚ÄòEC2 configuration‚Äô click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‚ÄòImage ID override‚Äô box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‚ÄòAmazon Elastic Compute Cloud (Amazon EC2)‚Äô as the compute environment.\nEnter a suitable name for your new job queue (üìù take note of this name you will need it later)\nUnder ‚ÄòConnected compute environments‚Äô select the compute environment that you just created\nClick Create job queue\n\nYou will want Nextflow to use an S3 bucket to store all the working files and results rather than a local connection.\n\nNavigate to the S3 service under the AWS management console and create a new private bucket in your relevant region.\nCreate a new folder within the bucket to serve as the Nextflow working directory (üìù take note of the S3 URI address as you will need this next)"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "href": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "title": "Nextflow on AWS Batch",
    "section": "Nextflow Config",
    "text": "Nextflow Config\nNow all you now need to do is set up your Nextflow config with the relevant details of your AWS setup. An example of initial config file is:\n//Select the awsbatch executor\nprocess.executor = 'awsbatch'\n\n//Name of the AWS Batch job queue that you just created\nprocess.queue = 'my-batch-queue'\n\n//region where we want to run this in\naws.region = 'ap-southeast-2'\n\n//Path to the aws cli tool you installed in your AMI\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n\n//S3 working directory that you just created\nworkDir = 's3://bucket_you_created/work/'\nThe last step is setting up your security credentials üîê to allow Nextflow to securely communicate and submit jobs to AWS batch. The best approach is to install AWS CLI locally (or in a EC2 instance if submitting from EC2).\nThen run AWS configure and enter the relevant Key ID, Access Key, and Region when prompted. These are the keys that AWS provided when you generated your Nextflow programmatic user at the start of this guide.\n\n\n\n\n\n\nWarning\n\n\n\nDO NOT store your credentials in your Nextflow configuration file as some tutorials suggest."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "href": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "title": "Nextflow on AWS Batch",
    "section": "üóíÔ∏èAdditional Notes:",
    "text": "üóíÔ∏èAdditional Notes:\n\nAWS batch jobs can take a few minutes to spin up, be patient before assuming you have set something up wrong\nIf you are using spot instances and your maximum % on-demand price is set too low your jobs make take a long time to start or may not run at all\nYou can view the log stream of your jobs by clicking through the ‚ÄòRunning‚Äô job numbers in the Batch dashboard and clicking the Log stream name - helpful to determine where a job is up to in a script\nThe Nextflow slack channel is a great place to raise any questions if you are still experiencing issues after following this setup guide, or want to experiment with some more advanced configurations and setups."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "href": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "title": "Nextflow on AWS Batch",
    "section": "Common errors",
    "text": "Common errors\nBelow are a list of common errors. Although the proposed solution has been demonstrated to work it may not always work in your specific scenario.\nTask failed to start - CannotPullContainerError: context canceled\nProposed solution: Increase your AMI EBS storage."
  },
  {
    "objectID": "posts/screen/index.html",
    "href": "posts/screen/index.html",
    "title": "Screen Utility and Terminal Multiplexing",
    "section": "",
    "text": "Photo¬†by¬†Leif Christoph Gottwald¬†on¬†Unsplash\n\n\nIf you are using AWS (or other cloud computing platforms), particularly in an interactive or development mode, you will have experienced the frustration of network disconnects resulting in either termination of processes or loss of access to console output.\nEven when running production pipelines that harness AWS batch with workflow managers such as Nextflow it is still valuable to maintain access to console output and messages over the lifetime of the pipelines, which can be days.\nThankfully there is a very simple Linux command that solves all these problems, üåü screen üåü\n\nScreen is a linux utility, known as a terminal multiplier, which in very simple terms, allows you to detach and reattach to different terminal sessions. Importantly this also allows remote processes to continue even after you (purposely or unexpectedly) disconnect.\nIt is likely that this utility is already installed on your remote instance. Screen comes pre-installed in both the Amazon Linux 2023 and Ubuntu Server 22.04 LTS AMIs.\nTo get you started I have compiled a few simple commands that should keep your covered.\nFirst, you will want to initiate and label a screen session.\n\nThe -S flag and name are optional, but if you are going to have more than one session, you should probably label it, save your brain üß† for more meaningful tasks.\nNow you can start running your interactive scripts or initiate a pipeline with a workflow manager such as Nextflow.\nWant to step away from this session to disconnect or run a different script or pipeline‚ùì\n\nTo see a list of available screen sessions\n\nTo return to an active screen session.\n\nOr to terminate a session.\n\nThis is certainly not the complete list of screen functions. For power users üßë‚Äçüíª there is an array of other great features. Maybe something for a future post‚Ä¶"
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "publications/publications.html#publications-by-date",
    "href": "publications/publications.html#publications-by-date",
    "title": "Publications",
    "section": "Publications by date",
    "text": "Publications by date\nüìÑ M. M. Malmberg, C. Smith, P. Thakur, M. C. Drayton, J. Wilson, M. Shinozuka, W. Clayton, C. Inch, G. C. Spangenberg, K. F. Smith, N. O. I. Cogan, L. W. Pembleton (2023). Developing an integrated genomic selection approach beyond biomass for varietal protection and nutritive traits in perennial ryegrass (Lolium perenne L.) Theoretical and Applied Genetics 136, 44.\nüìÑ Erez Naim-Feil, Edmond J. Breen, Luke W. Pembleton, Laura E. Spooner, German C. Spangenberg, Noel O. I. Cogan (2022). Empirical Evaluation of Inflorescences‚Äô Morphological Attributes for Yield Optimization of Medicinal Cannabis Cultivars. Frontiers in Plant Science, 13.\nüìÑ Yongjun Li, Sukhjiwan Kaur, Luke W. Pembleton, Hossein Valipour-Kahrood, Garry M. Rosewarne, Hans D. Daetwyler (2022). Strategies of preserving genetic diversity while maximizing genetic response from implementing genomic selection in pulse breeding programs. Theoretical and Applied Genetics, 1-16.\nüìÑ Erez Naim-Feil, Luke W. Pembleton, Laura E. Spooner, Alix L. Malthouse, Amy Miner, Melinda Quinn, Renata M. Polotnianka, Rebecca C. Baillie, German C. Spangenberg, Noel O. I. Cogan (2021). The characterization of key physiological traits of medicinal cannabis (Cannabis sativa L.) as a tool for precision breeding. BMC Plant Biology, 21(1).\nüìÑ Lydia M. Cranston, Keith G. Pembleton, Lucy L. Burkitt, Andrew Curtis, Daniel J. Donaghy, Cameron J. P. Gourley, Kerry C. Harrington, James L. Hills, Luke W. Pembleton, Richard P. Rawnsley (2020). The role of forage management in addressing challenges facing Australasian dairy farming. Anim. Prod. Sci., 60(1).\nüìÑ Abdulqader Jighly, Zibei Lin, Luke W. Pembleton, Noel O. I. Cogan, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2019). Boosting Genetic Gain in Allogamous Crops via Speed Breeding and Genomic Selection. Frontiers in Plant Science, 10.\nüìÑ Junping Wang, Pieter Badenhorst, Andrew Phelan, Luke Pembleton, Fan Shi, Noel Cogan, German Spangenberg, Kevin Smith (2019). Using Sensors and Unmanned Aircraft Systems for High-Throughput Phenotyping of Biomass in Perennial Ryegrass Breeding Trials. Frontiers in Plant Science, 10.\nüìÑ B. M. Caruana, L. W. Pembleton, F. Constable, B. Rodoni, A. T. Slater, N. O. I. Cogan (2019). Validation of Genotyping by Sequencing Using Transcriptomics for Diversity and Application of Genomic Selection in Tetraploid Potato. Frontiers in Plant Science, 10.\nüìÑ Luke W. Pembleton, Courtney Inch, Rebecca C. Baillie, Michelle C. Drayton, Preeti Thakur, Yvonne O. Ogaji, German C. Spangenberg, John W. Forster, Hans D. Daetwyler, Noel O. I. Cogan (2018). Exploitation of data from breeding programs supports rapid implementation of genomic selection for key agronomic traits in perennial ryegrass. Theoretical and Applied Genetics, 131(9).\nüìÑ M. Michelle Malmberg, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Shimna Sudheesh, Sukhjiwan Kaur, Hiroshi Shinozuka, Preeti Verma, German C. Spangenberg, Hans D. Daetwyler, John W. Forster, Noel O.I. Cogan (2018). Genotyping-by-sequencing through transcriptomics: implementation in a range of crop species with varying reproductive habits and ploidy levels. Plant Biotechnology Journal, 16(4).\nüìÑ Rebecca C. Baillie, Michelle C. Drayton, Luke W. Pembleton, Sukhjiwan Kaur, Richard A. Culvenor, Kevin F. Smith, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2017). Generation and Characterisation of a Reference Transcriptome for Phalaris (Phalaris aquatica L.). Agronomy, 7(1).\nüìÑ Zibei Lin, Junping Wang, Noel O.I. Cogan, Luke W. Pembleton, Pieter Badenhorst, John W. Forster, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2017). Optimizing Resource Allocation in a Genomic Breeding Program for Perennial Ryegrass to Balance Genetic Gain, Cost, and Inbreeding. Crop Science, 57(1).\nüìÑ Luke W. Pembleton, Michelle C. Drayton, Melissa Bain, Rebecca C. Baillie, Courtney Inch, German C. Spangenberg, Junping Wang, John W. Forster, Noel O. I. Cogan (2016). Targeted genotyping-by-sequencing permits cost-effective identification and discrimination of pasture grass species and cultivars.Theoretical and Applied Genetics, 129(5).\nüìÑ Junping Wang, Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2016). Evidence for Heterosis in Italian Ryegrass (Lolium multiflorum Lam.) Based on Inbreeding Depression in F2 Generation Offspring from Biparental Crosses. Agronomy, 6(4).\nüìÑ L. W. Pembleton, J. Wang, G. C. Spangenberg, J. W. Forster, N. O. I. Cogan (2016). Low-cost automated biochemical phenotyping for optimised nutrient quality components in ryegrass breeding. Crop Pasture Sci., 67(8).\nüìÑ Zibei Lin, Noel O. I. Cogan, Luke W. Pembleton, German C. Spangenberg, John W. Forster, Ben J. Hayes, Hans D. Daetwyler (2016). Genetic Gain and Inbreeding from Genomic Selection in a Simulated Commercial Breeding Program for Perennial Ryegrass. The Plant Genome, 9(1).\nüìÑ Luke Pembleton, Hiroshi Shinozuka, Junping Wang, German Spangenberg, John Forster, Noel Cogan (2015). Design of an F1 hybrid breeding strategy for ryegrasses based on selection of self-incompatibility locus-specific alleles. Frontiers in Plant Science, 6.\nüìÑ J. Wang, N. O. I. Cogan, L. W. Pembleton, J. W. Forster (2015). Variance, inter-trait correlation, heritability and trait-marker association of herbage yield, nutritive values, and morphological characteristics in Italian ryegrass (Lolium multiflorum Lam.). Crop Pasture Sci., 66(9).\nüìÑ Junping Wang, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Melanie L. Hand, Melissa Bain, Timothy I. Sawbridge, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2014). Development and implementation of a multiplexed single nucleotide polymorphism genotyping tool for differentiation of ryegrass species and cultivars. Molecular Breeding, 33(2).\nüìÑ L. W. Pembleton, J. Wang, N. O. I. Cogan, J. E. Pryce, G. Ye, C. K. Bandaranayake, M. L. Hand, R. C. Baillie, M. C. Drayton, K. Lawless, S. Erb, M. P. Dobrowolski, T. I. Sawbridge, G. C. Spangenberg, K. F. Smith, J. W. Forster (2013). Candidate gene-based association genetics analysis of herbage quality traits in perennial ryegrass (Lolium perenne L.). Crop Pasture Sci., 64(3).\nüìÑ Benjamin J. Hayes, Noel O. I. Cogan, Luke W. Pembleton, Michael E. Goddard, Junping Wang, German C. Spangenberg, John W. Forster (2013). Prospects for genomic selection in forage plant species. Plant Breeding, 132(2).\nüìÑ Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2013). StAMPP: an R package for calculation of genetic differentiation and structure of mixed-ploidy level populations. Molecular Ecology Resources, 13(5).\nüìÑ Sukhjiwan Kaur, Luke W. Pembleton, Noel O. I. Cogan, Keith W. Savin, Tony Leonforte, Jeffrey Paull, Michael Materne, John W. Forster (2012). Transcriptome sequencing of field pea and faba bean for discovery and validation of SSR genetic markers. BMC Genomics, 13(1).\nüìÑ Sukhjiwan Kaur, Noel O. I. Cogan, Luke W. Pembleton, Maiko Shinozuka, Keith W. Savin, Michael Materne, John W. Forster (2011).Transcriptome sequencing of lentil based on second-generation technology permits large-scale unigene assembly and SSR marker discovery. BMC Genomics, 12(1)."
  }
]