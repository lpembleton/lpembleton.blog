[
  {
    "objectID": "ramblings.html",
    "href": "ramblings.html",
    "title": "Ramblings",
    "section": "",
    "text": "Bash\n\n\n\n\n\n\n\nBash\n\n\n\n\nA collection of short Bash commonds and tricks that individually are too short for a blog post\n\n\n\n\n\n\nLW Pembleton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "publications/publications.html#publications-by-date",
    "href": "publications/publications.html#publications-by-date",
    "title": "Publications",
    "section": "Publications by date",
    "text": "Publications by date\n📄 M. M. Malmberg, C. Smith, P. Thakur, M. C. Drayton, J. Wilson, M. Shinozuka, W. Clayton, C. Inch, G. C. Spangenberg, K. F. Smith, N. O. I. Cogan, L. W. Pembleton (2023). Developing an integrated genomic selection approach beyond biomass for varietal protection and nutritive traits in perennial ryegrass (Lolium perenne L.) Theoretical and Applied Genetics 136, 44.\n📄 Erez Naim-Feil, Edmond J. Breen, Luke W. Pembleton, Laura E. Spooner, German C. Spangenberg, Noel O. I. Cogan (2022). Empirical Evaluation of Inflorescences’ Morphological Attributes for Yield Optimization of Medicinal Cannabis Cultivars. Frontiers in Plant Science, 13.\n📄 Yongjun Li, Sukhjiwan Kaur, Luke W. Pembleton, Hossein Valipour-Kahrood, Garry M. Rosewarne, Hans D. Daetwyler (2022). Strategies of preserving genetic diversity while maximizing genetic response from implementing genomic selection in pulse breeding programs. Theoretical and Applied Genetics, 1-16.\n📄 Erez Naim-Feil, Luke W. Pembleton, Laura E. Spooner, Alix L. Malthouse, Amy Miner, Melinda Quinn, Renata M. Polotnianka, Rebecca C. Baillie, German C. Spangenberg, Noel O. I. Cogan (2021). The characterization of key physiological traits of medicinal cannabis (Cannabis sativa L.) as a tool for precision breeding. BMC Plant Biology, 21(1).\n📄 Lydia M. Cranston, Keith G. Pembleton, Lucy L. Burkitt, Andrew Curtis, Daniel J. Donaghy, Cameron J. P. Gourley, Kerry C. Harrington, James L. Hills, Luke W. Pembleton, Richard P. Rawnsley (2020). The role of forage management in addressing challenges facing Australasian dairy farming. Anim. Prod. Sci., 60(1).\n📄 Abdulqader Jighly, Zibei Lin, Luke W. Pembleton, Noel O. I. Cogan, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2019). Boosting Genetic Gain in Allogamous Crops via Speed Breeding and Genomic Selection. Frontiers in Plant Science, 10.\n📄 Junping Wang, Pieter Badenhorst, Andrew Phelan, Luke Pembleton, Fan Shi, Noel Cogan, German Spangenberg, Kevin Smith (2019). Using Sensors and Unmanned Aircraft Systems for High-Throughput Phenotyping of Biomass in Perennial Ryegrass Breeding Trials. Frontiers in Plant Science, 10.\n📄 B. M. Caruana, L. W. Pembleton, F. Constable, B. Rodoni, A. T. Slater, N. O. I. Cogan (2019). Validation of Genotyping by Sequencing Using Transcriptomics for Diversity and Application of Genomic Selection in Tetraploid Potato. Frontiers in Plant Science, 10.\n📄 Luke W. Pembleton, Courtney Inch, Rebecca C. Baillie, Michelle C. Drayton, Preeti Thakur, Yvonne O. Ogaji, German C. Spangenberg, John W. Forster, Hans D. Daetwyler, Noel O. I. Cogan (2018). Exploitation of data from breeding programs supports rapid implementation of genomic selection for key agronomic traits in perennial ryegrass. Theoretical and Applied Genetics, 131(9).\n📄 M. Michelle Malmberg, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Shimna Sudheesh, Sukhjiwan Kaur, Hiroshi Shinozuka, Preeti Verma, German C. Spangenberg, Hans D. Daetwyler, John W. Forster, Noel O.I. Cogan (2018). Genotyping-by-sequencing through transcriptomics: implementation in a range of crop species with varying reproductive habits and ploidy levels. Plant Biotechnology Journal, 16(4).\n📄 Rebecca C. Baillie, Michelle C. Drayton, Luke W. Pembleton, Sukhjiwan Kaur, Richard A. Culvenor, Kevin F. Smith, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2017). Generation and Characterisation of a Reference Transcriptome for Phalaris (Phalaris aquatica L.). Agronomy, 7(1).\n📄 Zibei Lin, Junping Wang, Noel O.I. Cogan, Luke W. Pembleton, Pieter Badenhorst, John W. Forster, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2017). Optimizing Resource Allocation in a Genomic Breeding Program for Perennial Ryegrass to Balance Genetic Gain, Cost, and Inbreeding. Crop Science, 57(1).\n📄 Luke W. Pembleton, Michelle C. Drayton, Melissa Bain, Rebecca C. Baillie, Courtney Inch, German C. Spangenberg, Junping Wang, John W. Forster, Noel O. I. Cogan (2016). Targeted genotyping-by-sequencing permits cost-effective identification and discrimination of pasture grass species and cultivars.Theoretical and Applied Genetics, 129(5).\n📄 Junping Wang, Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2016). Evidence for Heterosis in Italian Ryegrass (Lolium multiflorum Lam.) Based on Inbreeding Depression in F2 Generation Offspring from Biparental Crosses. Agronomy, 6(4).\n📄 L. W. Pembleton, J. Wang, G. C. Spangenberg, J. W. Forster, N. O. I. Cogan (2016). Low-cost automated biochemical phenotyping for optimised nutrient quality components in ryegrass breeding. Crop Pasture Sci., 67(8).\n📄 Zibei Lin, Noel O. I. Cogan, Luke W. Pembleton, German C. Spangenberg, John W. Forster, Ben J. Hayes, Hans D. Daetwyler (2016). Genetic Gain and Inbreeding from Genomic Selection in a Simulated Commercial Breeding Program for Perennial Ryegrass. The Plant Genome, 9(1).\n📄 Luke Pembleton, Hiroshi Shinozuka, Junping Wang, German Spangenberg, John Forster, Noel Cogan (2015). Design of an F1 hybrid breeding strategy for ryegrasses based on selection of self-incompatibility locus-specific alleles. Frontiers in Plant Science, 6.\n📄 J. Wang, N. O. I. Cogan, L. W. Pembleton, J. W. Forster (2015). Variance, inter-trait correlation, heritability and trait-marker association of herbage yield, nutritive values, and morphological characteristics in Italian ryegrass (Lolium multiflorum Lam.). Crop Pasture Sci., 66(9).\n📄 Junping Wang, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Melanie L. Hand, Melissa Bain, Timothy I. Sawbridge, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2014). Development and implementation of a multiplexed single nucleotide polymorphism genotyping tool for differentiation of ryegrass species and cultivars. Molecular Breeding, 33(2).\n📄 L. W. Pembleton, J. Wang, N. O. I. Cogan, J. E. Pryce, G. Ye, C. K. Bandaranayake, M. L. Hand, R. C. Baillie, M. C. Drayton, K. Lawless, S. Erb, M. P. Dobrowolski, T. I. Sawbridge, G. C. Spangenberg, K. F. Smith, J. W. Forster (2013). Candidate gene-based association genetics analysis of herbage quality traits in perennial ryegrass (Lolium perenne L.). Crop Pasture Sci., 64(3).\n📄 Benjamin J. Hayes, Noel O. I. Cogan, Luke W. Pembleton, Michael E. Goddard, Junping Wang, German C. Spangenberg, John W. Forster (2013). Prospects for genomic selection in forage plant species. Plant Breeding, 132(2).\n📄 Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2013). StAMPP: an R package for calculation of genetic differentiation and structure of mixed-ploidy level populations. Molecular Ecology Resources, 13(5).\n📄 Sukhjiwan Kaur, Luke W. Pembleton, Noel O. I. Cogan, Keith W. Savin, Tony Leonforte, Jeffrey Paull, Michael Materne, John W. Forster (2012). Transcriptome sequencing of field pea and faba bean for discovery and validation of SSR genetic markers. BMC Genomics, 13(1).\n📄 Sukhjiwan Kaur, Noel O. I. Cogan, Luke W. Pembleton, Maiko Shinozuka, Keith W. Savin, Michael Materne, John W. Forster (2011).Transcriptome sequencing of lentil based on second-generation technology permits large-scale unigene assembly and SSR marker discovery. BMC Genomics, 12(1)."
  },
  {
    "objectID": "posts/screen/index.html",
    "href": "posts/screen/index.html",
    "title": "Screen Utility and Terminal Multiplexing",
    "section": "",
    "text": "Photo by Leif Christoph Gottwald on Unsplash\n\n\nIf you are using AWS (or other cloud computing platforms), particularly in an interactive or development mode, you will have experienced the frustration of network disconnects resulting in either termination of processes or loss of access to console output.\nEven when running production pipelines that harness AWS batch with workflow managers such as Nextflow it is still valuable to maintain access to console output and messages over the lifetime of the pipelines, which can be days.\nThankfully there is a very simple Linux command that solves all these problems, 🌟 screen 🌟\n\nScreen is a linux utility, known as a terminal multiplier, which in very simple terms, allows you to detach and reattach to different terminal sessions. Importantly this also allows remote processes to continue even after you (purposely or unexpectedly) disconnect.\nIt is likely that this utility is already installed on your remote instance. Screen comes pre-installed in both the Amazon Linux 2023 and Ubuntu Server 22.04 LTS AMIs.\nTo get you started I have compiled a few simple commands that should keep your covered.\n\nStarting a screen session\nTo get started, you’ll want to initiate and label a screen session. While the -S flag and name are optional, it’s advisable to label your session, sparing your brain 🧠 for more meaningful tasks. Here’s how to do it:\n\nNow that you have your screen session set up, you can run your interactive scripts or initiate a pipeline with a workflow manager such as Nextflow. Screen ensures that these processes continue running even if you need to step away, disconnect, or work on other tasks.\n\n\nDetach from screen session\nIf you want to step away from your active session to disconnect or to run a different script use the follow keyboard shortcut\n\n\n\nList screen sessions\nTo see a list of available screen sessions\n\n\n\nRe-attached to a screen session\nTo return to an active screen session.\n\n\n\n\n\n\n\nWarning\n\n\n\nEncountering the error message ‘there is no screen to be resumed’? Don’t worry, there’s a simple solution. If you unexpectedly disconnected from your previous session you may find that screen fails to recognise the detachment. Adding the -d flag to your command can save the day. This flag instructs screen to detach from any existing session before you attempt to reattach. By doing so, you’re ensuring a clean reconnection even in scenarios where your previous session’s state might not have been properly recognized due to an abrupt disconnection.\n\n\n\n\nTerminate a screen session\nIf you are currently within the screen session you wish to terminate it is as simple as typing exit and hitting enter\nIf you are detached from the session you want to terminate then you need to pass a quit command through to the relevant screen session using the -X flag.\n\n\n\nScrolling in a screen session\nYou will soon notice that you cannot scroll inside a screen session as you would in your normal terminal. Initially this is pretty frustrating 😧 especially when you are trying to work through large log files or errors printed to the terminal.\nHowever, there is a simple way to scroll within a screen session. First, use the following keyboard shortcut\n\nNow press\n\nYou can now scroll up and down with the keyboard or mouse wheel.\n\nWhen finished just press q or\n\nThis is certainly not the complete list of screen functions. For power users 🧑‍💻 there is an array of other great features. Maybe something for a future post…"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html",
    "href": "posts/nextflow-on-aws-batch/index.html",
    "title": "Nextflow on AWS Batch",
    "section": "",
    "text": "Photo by Jenessaa Lu on Unsplash\nThe following is a general guide on how to set up Nextflow with AWS batch as the compute environment. I would highly recommend that you use your local environment or at least a smaller test dataset for pipeline development, transferring to AWS batch when in a working production state.\nAlthough Gandalf 🧙 trims his beard more often than Amazon updates their AWS user interface, I cannot guarantee the included menu screenshots will look the same on your system. However, hopefully they will still provide sufficient information to determine the appropriate settings and options. Reach out if you feel I need to update this guide."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "href": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "title": "Nextflow on AWS Batch",
    "section": "IAM Setup",
    "text": "IAM Setup\nFirstly you need to create a new IAM with more appropriate permissions tailored to the requirements listed in Nextflow documentation. It is strongly recommended that do not use your root account to run Nextflow pipelines.\n\nOpen the IAM management console on AWS and add a new user\nEnter an appropriate user name for example ‘Nextflow-access’. Under access type, select programmatic access\n\nNext you need to create a user group for the new user to sit within. Generally, on AWS you will apply permissions to a user group rather than a specific user. Additionally, this allows you to set up multiple separate people within the ‘Nextflow group’. Again, enter an appropriate name and click Create group\nAdd any metadata tags if appropriate\nClick Create user. You should be greeted with a new page that includes a Access Key ID and SCA (📝 take note of these keys as you will need them towards the end of this guide)\n\nNow that you have your new user and Nextflow group you will need to apply the required permissions.\n\nFrom the IAM user panel click User groups select your recently created ‘nextflow’ group, and under the permissions menu click on the Attach policy button\n\nClick Create policy\n\nUse the visual editor to add all the required permissions\n\nMinimal permissions policies to be attached to the AWS account used by Nextflow are:\n\nTo interface AWS Batch:\n\"batch:DescribeJobQueues\"\n\"batch:CancelJob\"\n\"batch:SubmitJob\"\n\"batch:ListJobs\"\n\"batch:DescribeComputeEnvironments\"\n\"batch:TerminateJob\"\n\"batch:DescribeJobs\"\n\"batch:RegisterJobDefinition\"\n\"batch:DescribeJobDefinitions\"\nTo be able to see the EC2 instances:\n\"ecs:DescribeTasks\"\n\"ec2:DescribeInstances\"\n\"ec2:DescribeInstanceTypes\"\n\"ec2:DescribeInstanceAttribute\"\n\"ecs:DescribeContainerInstances\"\n\"ec2:DescribeInstanceStatus\"\nTo pull container images stored in the ECR repositories:\n\"ecr:GetAuthorizationToken\"\n\"ecr:BatchCheckLayerAvailability\"\n\"ecr:GetDownloadUrlForLayer\"\n\"ecr:GetRepositoryPolicy\"\n\"ecr:DescribeRepositories\"\n\"ecr:ListImages\"\n\"ecr:DescribeImages\"\n\"ecr:BatchGetImage\"\n\"ecr:GetLifecyclePolicy\"\n\"ecr:GetLifecyclePolicyPreview\"\n\"ecr:ListTagsForResource\"\n\"ecr:DescribeImageScanFindings\"\n\nYou also need to add permissions for S3 so that nextlflow can pull input data and publish results. Still using the visual editor select S3 as the service and then select the All S3 actions (s3:*) check box under actions. You may get notifications of other ‘dependency’ type permissions that are required, follow the instructions to add these as well.\n\nAdd any metadata tags if appropriate\nGive your new policy a name and click Create policy\nSelect your newly created permission policy to add to the user group and click Add permissions. Hint: you can find your new policy by 🔍searching in the filter box\n\nTo be able to use spot instances you will need to create an additional role.\n\nClick Roles under the IAM access management menu and click Create role\n\nSelect AWS service and EC2 under common use cases, click Next\nSearch for AmazonEC2SpotFleetTaggingRole select it and click Next\nAdd a role name, e.g. AmazonEC2SpotFleetRole and click Create role"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "href": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "title": "Nextflow on AWS Batch",
    "section": "Custom Nextflow AMI",
    "text": "Custom Nextflow AMI\nAWS batch uses Amazon Machine Images (AMIs) to initiate EC2 compute instances that will subsequently run your Nextflow processes. Nextflow tasks submitted to AWS Batch will run under the Amazon Elastic Container Service (ECS). ECS (not to be confused with EC2) uses a base Amazon ECS-optimised AMI (with docker pre-installed). Although Nextflow & Batch will control the CPU and memory resource request and allocation you need to ensure you base ECS AMI has sufficient EBS storage to hold any relevant input and working data files, such as sequence reads, indexes etc. You will also need to install the AWS CLI in the base ECS AMI to allow data movement to and from S3 buckets. To set all this up follow these steps:\n\nNavigate to the EC2 console menu\nClick Instances and then Launch Instances\nUnder ‘quick start’ click Browse more AMIs\nClick AWS Marketplace AMIs and search for ECS\nAt the time of writing amzn2-ami-ecs-hvm-2.0.20221025-x86_64-ebs was the most up-to-date ECS AMI. Select it\n\nSelect the t2.micro instance type\nSelect and relevant key pairs and network settings based on your setup (I would recommend at a minimum a private VPC and IP-restricted connections via a bastion instance)\nEnsure you have at least 30GiB storage 💾 listed under ‘Configure storage’. Also change the storage type from gp2 to gp3 (for a performance boost at no additional cost - see Matt Vaughn’s NextflowSummit 2022 talk 📽️).\n\n\n\n\n\n\nNote\n\n\n\nFor some Nextflow processes your will need more than 30GiB of EBS storage. I would recommend making additional AMIs (based on this image) for these specific tasks and assigning them to specific Batch job queues, see later on.\n\n\nClick Launch instance 🚀\nSSH 💻 into your new instance where you will need to install AWS CLI\nOnce connected run the following commands to install AWS CLI\ncd $HOME\nsudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nTo verify the install was successful\n$ ./miniconda/bin/aws --version\naws-cli/1.19.79 Python/3.8.5 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.20.79\nUnder the Instances menu in the EC2 console select your relevant instance and click Actions, then Images and Templates, then Create Image\nGive your new image a name e.g. nextflow-30GiB-ecs-ami and click Create image\n📝Take note of the AMI ID (not the name) that you just generated as you will need this later\n\n\n\n\n\n\n\nNote\n\n\n\nContrary to what is commonly written in other documentation you no longer need to expand your docker 🐋storage volume to match your allocated EBS storage size. The docker storage automatically expands on the Amazon 2 AMIs which are now default (unlike previous Amazon 1 AMIs)."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "href": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "title": "Nextflow on AWS Batch",
    "section": "Batch Environment",
    "text": "Batch Environment\nNow it is time to create your Batch environment which entails at least one compute environment and one job queue that Nextflow will submit processes to.\nNavigate to the Batch AWS console and click on Compute environments.\n\nClick Create and select Amazon Elastic Compute Cloud (Amazon EC2) as the compute environment.\nSelect Managed as the orchestration type and enter a suitable name for your new compute environment.\nIf this is your first time setting up a Batch environment AWS will create the relevant service role and instance role. Just ensure ‘Create new role’ is selected. Alternatively, under ‘Service role’ select AWSServiceRoleForBatch and under ‘Instance Role’ select ecsInstanceRole. Click Next Page\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the allowed maximum number of parallel vCPU tasks that can run in your compute environment at any one time. Increase or decrease this to an appropriate number based on your requirements.\n‘Allowed instance type’ allows you to control the type of instances that AWS is allowed to try and run your jobs on. Your CPU and memory requirements defined in your Nextflow config will apply a second tier of filtering (i.e. if your memory request is higher than an allowed instance type, obviously that instance type won’t be used). You can leave this as ‘optimal’ and AWS will attempt to find the best instance type match to your CPU and memory request.\n\n\n\n\n\n\nNote\n\n\n\nAWS will generally group multiple jobs onto the one large instance, however, this can result in errors, particularly from noisy neighbors, and I/O and/or network intensive tasks.\nIf you want to prevent AWS from grouping multiple jobs onto the one larger instance, then you need to specifically define smaller instances types, e.g. r6i.xlarge, r6i.2xlarge, to prevent AWS using super instances such as r6i.24xlarge r6i.32xlarge.\n\n\nTo use spot instances toggle the Use EC2 Spot instances button at the top and define your maximum cut-off for on-demand price under ‘Maximum % on-demand price’. Under ‘spot fleet role’ you will also need to select the AmazonEC2SpotFleetRole role that you created earlier.\nUnder ‘Additional configuration’ you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren’t available, AWS Batch waits for the additional instances to be available. If there aren’t enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don’t run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you’re using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn’t supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren’t available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‘EC2 configuration’ click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‘Image ID override’ box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‘Amazon Elastic Compute Cloud (Amazon EC2)’ as the compute environment.\nEnter a suitable name for your new job queue (📝 take note of this name you will need it later)\nUnder ‘Connected compute environments’ select the compute environment that you just created\nClick Create job queue\n\nYou will want Nextflow to use an S3 bucket to store all the working files and results rather than a local connection.\n\nNavigate to the S3 service under the AWS management console and create a new private bucket in your relevant region.\nCreate a new folder within the bucket to serve as the Nextflow working directory (📝 take note of the S3 URI address as you will need this next)"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "href": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "title": "Nextflow on AWS Batch",
    "section": "Nextflow Config",
    "text": "Nextflow Config\nNow all you now need to do is set up your Nextflow config with the relevant details of your AWS setup. An example of initial config file is:\n//Select the awsbatch executor\nprocess.executor = 'awsbatch'\n\n//Name of the AWS Batch job queue that you just created\nprocess.queue = 'my-batch-queue'\n\n//region where we want to run this in\naws.region = 'ap-southeast-2'\n\n//Path to the aws cli tool you installed in your AMI\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n\n//S3 working directory that you just created\nworkDir = 's3://bucket_you_created/work/'\nThe last step is setting up your security credentials 🔐 to allow Nextflow to securely communicate and submit jobs to AWS batch. The best approach is to install AWS CLI locally (or in a EC2 instance if submitting from EC2).\nThen run AWS configure and enter the relevant Key ID, Access Key, and Region when prompted. These are the keys that AWS provided when you generated your Nextflow programmatic user at the start of this guide.\n\n\n\n\n\n\nWarning\n\n\n\nDO NOT store your credentials in your Nextflow configuration file as some tutorials suggest."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "href": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "title": "Nextflow on AWS Batch",
    "section": "🗒️Additional Notes:",
    "text": "🗒️Additional Notes:\n\nAWS batch jobs can take a few minutes to spin up, be patient before assuming you have set something up wrong\nIf you are using spot instances and your maximum % on-demand price is set too low your jobs make take a long time to start or may not run at all\nYou can view the log stream of your jobs by clicking through the ‘Running’ job numbers in the Batch dashboard and clicking the Log stream name - helpful to determine where a job is up to in a script\nThe Nextflow slack channel is a great place to raise any questions if you are still experiencing issues after following this setup guide, or want to experiment with some more advanced configurations and setups."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "href": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "title": "Nextflow on AWS Batch",
    "section": "Common errors",
    "text": "Common errors\nBelow are a list of common errors. Although the proposed solution has been demonstrated to work it may not always work in your specific scenario.\nTask failed to start - CannotPullContainerError: context canceled\nProposed solution: Increase your AMI EBS storage."
  },
  {
    "objectID": "posts/aws-endpoints/index.html",
    "href": "posts/aws-endpoints/index.html",
    "title": "Endpoints for AWS VPC",
    "section": "",
    "text": "Photo by Varun Yadav on Unsplash\n\n\nIf you are using AWS Batch and don’t have endpoints set up for your VPC, you need to do it right now❗ Í’m sure AWS have their reasons, but for any bioinformatic VPC, these should be set up by default.\nWithout endpoints, AWS batch jobs in your private VPC requiring access to S3 storage (yes, even your own S3 buckets) or ECR for your docker containers will actually have to go via your NAT gateway and the internet. Not only does this reduce security, but AWS charges exuberant NAT gateway processing fees per GB of data that passes through 💸 With thousands of jobs and large genomic 🧬 datasets and docker images, you will find these fees (listed as EC2 other) will cost you more than the actual EC2 instances.\n\n\n\nS3 and ECR access without endpoints.\n\n\nAdditional important benefits are reduced intermittent errors and faster run times. Previously with all communication running through your NAT gateway it would easily become overwhelmed and spin out errors if numerous jobs were transferring large volumes of data (which is the norm in bioinformatics). In turn, this processing would slow your job run times.\nEndpoints provide a way for your network traffic to remain ‘local’ within amazon’s networks and avoid any NAT gateway fees, and their setup is easy!\n\n\n\nS3 and ECR access with endpoints.\n\n\nFor communication with S3 you can use a AWS gateway endpoint:\nUnder the AWS services menu, go to the VPC console, select Endpoints and click Create endpoint.\n\n\nEnter a relevant name and leave AWS services selected as default.\nIn the Services search field, enter ‘S3’ and press ‘enter’.\nSelect the service name com.amazonaws.[REGIOIN].s3 that has ‘Gateway’ as the ‘Type’ field.\n\nChoose your VPC.\nUnder Route tables, choose the ID that matches your Private subnet (ℹ️ tip: scroll across)\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nAny S3 requests from your private VPC batch jobs will now be processed through the gateway endpoint.\nAlthough your ECR containers are stored in the S3 system, you need to set up ECR and ECS endpoints for the orchestration communication required to setup these containers in your batch jobs.\nSetting up an endpoint for ECR & ECS traffic requires a few more steps:\nAgain under your VPC console, select endpoints and click Create endpoint. Following similar steps as above.\n\nSearch in the Services search bar for ‘ECR’ and select com.amazonaws.[REGION].ecr.api\n\nThe type is now ‘Interface’.\nChoose your VPC.\nSelect your private subnet.\nSelect either your default security group or one that has your required restrictions.\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nNow repeat this process three more times for\n\ncom.amazonaws.[REGION].ecr.dkr\ncom.amazonaws.[REGION].ecs-agent\ncom.amazonaws.[REGION].ecs-telemetry\n\nYou are now all setup 🎉 If all is setup correctly you should now notice a big drop ↘️ in your fees, less ↘️ errors and faster 🏎️ job run times.\nThis should be the default… 🤷"
  },
  {
    "objectID": "posts/aws-batch-ebs-autoscale/index.html",
    "href": "posts/aws-batch-ebs-autoscale/index.html",
    "title": "EBS Auto-scaling on AWS Batch",
    "section": "",
    "text": "Photo by Simon Goetz on Unsplash\n\n\nAnyone who has tried running bioinformatic pipelines on AWS batch with a workflow manager such as Nextflow will be well aware of the common error\nerror: No space left on device\nthat can plague a pipeline. Yes, you can adjust your EBS allocation with specific AMI images or launch configurations and tailor them to specific tasks, but the dynamic nature of bioinformaticslogy means this will likely be an ongoing cat 🐈 and mouse 🐁 game.\nYes, Nextflow has the fantastic resume feature if your pipeline has already completed a large proportion of tasks, unfortunately though the config file is not reanalysed upon resume, so you cannot point to a new AMI with an increased EBS volume.\nThe solution? Automatic scaling of your EBS volumes in real-time. Essentially there is a script that resides within your AMI that continously monitors disk usage, and just before you reach 100%, it provisions a new EBS volume mounting it directly to your running EC2 instance. You also get the added benefit of better EBS cost optimisation 💰 as you no longer need to ‘over provision’ your batch EC2 instances.\n\nThe setup can be split into two components, installing the auto-scaling scripts in your AMI and updating your Batch compute environments with appropriate permissions.\n\nSetup an appropriate IAM Role\n\nClick Create role under the IAM AWS console and select AWS service as the trusted entity type and EC2 as the use case, then click Next.\nClick Create policy and select the JSON tab.\nPaste the following JSON code and click Next.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:DescribeVolumeStatus\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeTags\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:DescribeVolumeAttribute\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteVolume\",\n                \"ec2:CreateTags\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\nAdd any tags if applicable and click Next\nGive your policy a name e.g. amazon-ebs-autoscale-policy and click Create policy\nNow under the Add permission menu of your new IAM Role, select your newly created policy, i.e. amazon-ebs-autoscale-policy and click Next\nGive your Role and name, e.g. amazon-ebs-autoscale-role and click Create role\nYou also need to add the amazon-ebs-autoscale-policy policy role to the ecsInstanceRolerole you use in your AWS Batch compute environments.\nUnder Roles in the AWS IAM console, find and click the ecsInstanceRole role.\nClick Add permission and select Attach policies. Find/search for your new amazon-ebs-autoscale-policy, select it and click Attach policies.\n\n\n\nInstall the auto-scale scripts\n\nFetch or clone the amazon-ebs-autoscale repository to your local computer.\nEdit the EBS mount location to the volume that docker utilises by adding the -m /var/lib/docker parameter to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file\nSpecify the initial drive to use for the mount point to be /dev/xvdba with the -d parameter\nBy default, the 100GiB volume will be initially provisioned at startup to change this add the -s parameter again to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file. For example, to reduce it to 30GB use -s 30\nthe runcmd: section should now look something like this:\n\nruncmd:\n  - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\"\n  - unzip -q /tmp/awscliv2.zip -d /tmp && /tmp/aws/install\n  - EBS_AUTOSCALE_VERSION=$(curl --silent \"https://api.github.com/repos/awslabs/amazon-ebs-autoscale/releases/latest\" | jq -r .tag_name)\n  - cd /opt && git clone https://github.com/awslabs/amazon-ebs-autoscale.git\n  - cd /opt/amazon-ebs-autoscale && git checkout $EBS_AUTOSCALE_VERSION\n  - sh /opt/amazon-ebs-autoscale/install.sh -m /var/lib/docker -d /dev/xvdba -s 30 2&gt;&1 &gt; /var/log/ebs-autoscale-install.log\n\nTo install the amazon-ebs-autoscale scripts with your defined parameters into your chosen AMI you can use the aws ec2 run-instance command from the aws-cli. An example of launching your chosen AMI and installing the amazon-ebs-autoscale scripts is\n\naws ec2 run-instances --image-id YOUR-AMI-ID \\\n  --key-name YOUR-KEY-PAIR-NAME \\\n  --subnet-id YOUR-SUBNET-ID \\\n  --user-data file://./templates/cloud-init-userdata.yaml \\\n  --count 1 \\\n  --security-group-ids YOUR-SECURITY-GROUP-ID \\\n  --instance-type t2.micro \\\n  --iam-instance-profile Name=amazon-ebs-autoscale-role\n\nRunning this from your command line will launch an EC2 instance which you can then save as a new AMI with an appropriate name. (see my Nextflow on AWS Batch blog post for details on how to save AMIs and use them in later Batch compute environments)\n\nThe final step is to reconfigure your Batch compute environment to utilise the EBS autoscaling AMI\n\nClick Computer environments in the AWS Batch console and click Create\nSelect Amazon Elastic Compute Cloud (Amazon EC2) as the configuration and Managed as the orchestration type and enter an appropriate name.\nEnsure AWSServiceRoleForBatch is selected as the service role and ecsInstanceRole as the instance role and click Next page\n\n\n\n\n\n\nNote\n\n\n\nThe updated ecsInstanceRole now contains the permissions required for autoscaling.\n\n\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the number of parallel vCPU tasks running in your compute environment. Increase or decrease this to an appropriate number based on your requirements.\nWith EBS autoscaling implemented, you can now generally use optimal as the allowed instance type without many of the previous risks of errors and failed pipelines.\nUnder ‘Additional configuration’ you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren’t available, AWS Batch waits for the additional instances to be available. If there aren’t enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don’t run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you’re using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn’t supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren’t available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‘EC2 configuration’ click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‘Image ID override’ box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‘Amazon Elastic Compute Cloud (Amazon EC2)’ as the compute environment.\nEnter a suitable name for your new job queue (📝 take note of this name you will need it later)\nUnder ‘Connected compute environments’ select the compute environment that you just created\nClick Create job queue\n\nYou now need to update the config files of your Nextflow pipelines to point to the new Batch job queue.\nNow enjoy your Nextflow pipelines with no reduced errors."
  },
  {
    "objectID": "pages/conferences.html",
    "href": "pages/conferences.html",
    "title": "Virtual Conferences & Meetups",
    "section": "",
    "text": "Photo by Sigmund on Unsplash\n\n\nA list of virtual 🧑‍💻 (or hybrid) conferences and meetups for the data science 📊 life science 🧪🧬 and agricultural science community 🌾\n\nReoccurring 🔁\n📌Posit Data Science Hangout - Live Every Thursday at 12 PM ET.\nEach week, host Rachael Dempsey invites an accomplished data science leader to talk about their experience and answer questions from the audience. The discussion focuses mainly on the human elements of data science leadership. There’s no sales or marketing fluff, just great insights from inspiring professionals.\n📹 Past hangouts are also available to watch.\n\n\n📌Channels the nextflow podcast - Pre-recorded every fortnight.\nTalking about news in the Nextflow ecosytem and speaking with pioneers in the field.\n\n\n\nScheduled 📆\n\n2023\n📌dockercon 2023 - October 4-5, 2023.\nDockerCon returns in-person in Los Angeles and online from anywhere in the world this October. Meet and learn from thousands of developers and industry leaders to power up your technical knowledge.\nRegister | Follow on Twitter\n\n\n📌Nextflow SUMMIT - October 16-20, 2023\nA showcase of the latest developments and innovations from the Nextflow world held in Barcelona and streamed online. Also includes a virtual nf-core Hackathon.\nRegister | Follow on Twitter\n\n\n📌GitHub Universe - November 8-9, 2023.\nThe annual developer event is back, better, and bigger than ever at San Francisco’s Yerba Buena Center for the Arts and online.\nRegister | Follow on Twitter\n\n\n2024\n📌useR! 2024 - July 8-11, 2024\nuseR! is the main meeting of the R user and developer community, its program consists of both invited and user-contributed presentations.\nRegistration opens soon | Follow on Twitter | Follow on Mastodon\n\n\n\n\nPast Events 🗄️\n✅ShinyConf2023 - 15-17 March, 2023\nShinyConf is a virtual event hosted by Appsilon with support from Posit. This year’s Shiny conference Showcases recent advancements in R Shiny technology such as open source packages, Shiny interoperability, and commercial applications\nConference recordings\n\n✅New York R Conference - July 11-12, 2023.\nThe R Conference hosts one of the most elite gatherings of data scientists and data professionals who come together to explore, share, and inspire ideas, and to promote the growth of open source ideals.\nConference recordings.\n\n\n✅Bioconductor Conference, August 2-4, 2023.\nBioconductor conference highlights current developments within and beyond the Bioconductor project.\n✅Posit::conf(2023) - September 17-20, 2023.\nTwo days of workshops, two days of conference keynotes and talks, and endless opportunities to connect our data science community. Experience it in person or virtually."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ponderings in Diagon Alley",
    "section": "",
    "text": "Streamlining Your AWS CLI S3 File Operations\n\n\n\n\n\n\n\nAWS\n\n\nS3\n\n\n\n\nA guide to using the AWS Command Line Interface to interact with S3\n\n\n\n\n\n\nSep 25, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nPushing Docker Images to AWS ECR\n\n\n\n\n\n\n\nDocker\n\n\nAWS\n\n\nECR\n\n\n\n\nA guide to transfer your docker images to your private AWS container repository\n\n\n\n\n\n\nSep 24, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nAnnotate equations\n\n\n\n\n\n\n\nequations\n\n\nquarto\n\n\nannotate\n\n\nLaTeX\n\n\n\n\nA guide to annotating equations in quarto documents\n\n\n\n\n\n\nSep 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nHow to Cast Blue Iris UI3\n\n\n\n\n\n\n\nSmarthome\n\n\nBlue Iris\n\n\nSecurity\n\n\nChromecast\n\n\n\n\nA guide to casting Blue Iris UI3 to a Chromecast device and monitor\n\n\n\n\n\n\nAug 28, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nGitHub Deploy Keys\n\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nssh key\n\n\nAWS\n\n\nremote\n\n\ncloud\n\n\n\n\nA guide to using GitHub deploy keys to deploy private repos in remote compute sessions\n\n\n\n\n\n\nJul 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nScreen Utility and Terminal Multiplexing\n\n\n\n\n\n\n\nScreen\n\n\nAWS\n\n\nTerminal\n\n\n\n\nA guide to using the screen command in remote sessions\n\n\n\n\n\n\nMay 23, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nEndpoints for AWS VPC\n\n\n\n\n\n\n\nAWS\n\n\nEndpoints\n\n\nGateway\n\n\nBatch\n\n\n\n\nA guide to setting up endpoints for your AWS VPC\n\n\n\n\n\n\nMay 18, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nEBS Auto-scaling on AWS Batch\n\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\nEBS\n\n\n\n\nAn introductory guide to setting up EBS auto-scaling on AWS Batch for use in Nextflow\n\n\n\n\n\n\nFeb 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n  \n\n\n\n\nNextflow on AWS Batch\n\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\n\n\nAn introductory guide to setting up Nextflow with AWS Batch\n\n\n\n\n\n\nNov 25, 2022\n\n\nLW Pembleton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "🎓 Doctor of Philosophy, Molecular Genetics from La Trobe University, 2014\n🎓 Bachelor of Agricultural Science, Rural Technology from the University of Queensland, 2010\n\n\nDr Luke Pembleton is a Genomic Breeding Scientist and Strategic Science Manager at Barenbrug. He leads the development and implementation of commercial genomic breeding and associated ‘omic’ technologies.\n\n\n\nMost of my technical work is focused on the fields of 🧬 Genomics, Genomic Selection and Genomic Breeding in plants, primarily forages 🌱. Skilled in bioinformatics 👨‍💻 and passionate about the R programing language. Enjoy developing cloud computing ☁️ environments for research and production pipelines, often orchestrated with the workflow manager Nextflow 🔂.\n\n\n\n Genomic Breeding Scientist | Global Strategic Science Manager\nBarenbrug | August 2021 - Current\n~\n Senior Research Scientist\nAgriculture Victoria Research | April 2017 - August 2021\n~\n Research Scientist\nAgriculture Victoria Research | October 2014 - March 2017\n~\n Research Assistant\nVictorian Department of Primary Industries | June 2009 - December 2009"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "Dr Luke Pembleton is a Genomic Breeding Scientist and Strategic Science Manager at Barenbrug. He leads the development and implementation of commercial genomic breeding and associated ‘omic’ technologies."
  },
  {
    "objectID": "index.html#interest-and-skills",
    "href": "index.html#interest-and-skills",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "Most of my technical work is focused on the fields of 🧬 Genomics, Genomic Selection and Genomic Breeding in plants, primarily forages 🌱. Skilled in bioinformatics 👨‍💻 and passionate about the R programing language. Enjoy developing cloud computing ☁️ environments for research and production pipelines, often orchestrated with the workflow manager Nextflow 🔂."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "Genomic Breeding Scientist | Global Strategic Science Manager\nBarenbrug | August 2021 - Current\n~\n Senior Research Scientist\nAgriculture Victoria Research | April 2017 - August 2021\n~\n Research Scientist\nAgriculture Victoria Research | October 2014 - March 2017\n~\n Research Assistant\nVictorian Department of Primary Industries | June 2009 - December 2009"
  },
  {
    "objectID": "posts/annotate-equations/index.html#annotating-equations",
    "href": "posts/annotate-equations/index.html#annotating-equations",
    "title": "Annotate equations",
    "section": "Annotating Equations",
    "text": "Annotating Equations\nUsing the LaTeX annotate-equations package in Quarto allows you to easily annotate equations within your quarto reports, documents or manuscripts. Currently as far as I am aware this only works for pdfs.\n\nStep-by-step example\nHere’s a straightforward step-by-step guide to annotating equations using the annotate-equations package. We’ll use the genomic selection GEBV equation as the example.\n\nQuarto Setup\nQuarto itself will attempt to automatically download and install the annotate-equations package into its LaTeX engine, all you need to do is tell Quarto that you need it by adding the following to your yaml at the top of your .qmd file\nheader-includes:\n  - \\usepackage{annotate-equations}\n\n\nBegin with Your LaTeX Equation\nStart by writing the equation you want to annotate in LaTeX format:\n\\begin{equation*}\n  GEBV = \\sum_{i}^{n} X_{i} b_{i}\n\\end{equation*}\n\n\n\n\nSplit the Equation into Nodes\nBreak down the equation into individual nodes. To highlight a node, enclose it in \\eqnmarkbox[color]{node name}{equation term(s)}. If you only want to color the node’s text, use \\eqnmark[color]{node name}{equation term(s)}.\nFor example:\n\\begin{equation*}\n  \\eqnmark[purple]{node1}{GEBV}\n  \\tikzmarknode{node2}{=}\n  \\eqnmark[black]{node3}{\\sum_{i}^{n}}\n  \\eqnmarkbox[blue]{node4}{X_{i}}\n  \\eqnmarkbox[red]{node5}{b_{i}}\n\\end{equation*} \n\n\n\n\nAdd Annotations\nNow, introduce annotations to each node using the \\annotate[tikzoptions]{annotate keys}{node name[,…]}{annotation text} command:\n\nFor {annotate keys}, choose whether the annotation appears above or below, to the right or left of the node.\nFor [tikzoptions], include a yshift value to adjust the annotation’s position above or below (use negative values for below). You may need to fine-tune these values, especially in complex equations. xshift can be defined if necessary.\n\nFor example:\n\\begin{equation*}\n  \\eqnmark[purple]{node1}{GEBV}\n  \\tikzmarknode{node2}{=}\n  \\eqnmark[black]{node3}{\\sum_{i}^{n}}\n  \\eqnmarkbox[blue]{node4}{X_{i}}\n  \\eqnmarkbox[red]{node5}{b_{i}}\n\\end{equation*}\n\\annotate[yshift=1em]{left}{node3}{sum across n loci} \n\\annotate[yshift=-1em]{below,left}{node1}{Genomic Estimated Breeding Value} \n\\annotate[yshift=-1em]{below,right}{node5}{effect at locus i} \n\\annotate[yshift=-2.5em]{below,right}{node4}{genotype at locus i}\n\n\n\n\n\n\n\nOther example\n\nAnnotate to multiple nodes\nFor some equations a variable will appear multiple times. In these scenarios you may want your annotation to link both occurrences. To archive this you just use the \\annotatetwo definition and add both node names.\nAn example using the Hardy-Weinberg equation:\n\n\\begin{equation*}\n  \\eqnmarkbox[green]{node1}{p}\n  \\tikzmarknode{node2}{^{2}}\n  \\tikzmarknode{node3}{+}\n  \\tikzmarknode{node4}{2}\n  \\eqnmarkbox[green]{node5}{p}\n  \\eqnmarkbox[blue]{node6}{q}\n  \\tikzmarknode{node7}{+}\n  \\eqnmarkbox[blue]{node8}{q}\n  \\tikzmarknode{node9}{^{2}}\n  \\tikzmarknode{node10}{=}\n  \\tikzmarknode{node11}{1}\n\\end{equation*}\n\\annotatetwo[yshift=1.5em]{above, label above}{node1}{node5}{frequency of allele A} \n\\annotatetwo[yshift=-1.5em]{below, label below}{node6}{node8}{frequency of allele B} \n\n\n\n\n\nAdditional Information\nIf you want to see how this looks in a pdf and/or see the LaTeX code inside the correpsonding quarto .qmd file visit my github repo.\nFor more in-depth instructions and possibilities with annotate-equations in LaTeX, refer to the documentation at https://github.com/st–/annotate-equations/blob/main/annotate-equations.pdf"
  },
  {
    "objectID": "posts/aws-cli-s3/index.html",
    "href": "posts/aws-cli-s3/index.html",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "",
    "text": "Photo by Jesse Vermeulen on Unsplash\nAmazon Web Services Command Line Interface (AWS CLI) is your trusty companion for seamlessly interacting with AWS services directly from your terminal. It empowers you to manage and automate various AWS resources and operations effortlessly. In this blog post, we’ll explore two essential aspects of AWS CLI: moving files in and out of AWS S3 buckets  using the aws s3 cp and aws s3 sync commands."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#aws-cli-s3-sync-your-data-synchronisation-solution",
    "href": "posts/aws-cli-s3/index.html#aws-cli-s3-sync-your-data-synchronisation-solution",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "AWS CLI S3 Sync: Your Data Synchronisation Solution",
    "text": "AWS CLI S3 Sync: Your Data Synchronisation Solution\n\nThe aws s3 sync Command \naws s3 sync is a command that should be in the toolkit of anyone dealing with AWS S3. This command enables you to synchronise files and directories between a source and a destination, which can be an S3 bucket, a local drive, or even between S3 buckets. Here’s the basic syntax:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\naws s3 sync will only copy new and updated files by checking the destination folder first. There is also the ability to delete files in the destination that are no longer present in the source by applying the --delete flag.\nIn addition to the ability to synchronise files and directories, aws s3 sync has the ability to handle specific file filtering. You can precisely control which files to include or exclude using two essential parameters:\n\n–exclude : Exclude files matching the specified pattern.\n–include : Include files matching the specified pattern.\n\nBy default, all files are included, so when you need to sync only certain files, like .png images, you must first exclude all files and then include only those with the .png extension:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\\\n  --exclude '*'\\\n  --include '*.png'\nBe mindful of the order of these filters; they are applied from left to right. For instance, the following command may seem similar but produces a different outcome:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\\\n  --include `*.png`\\\n  --exclude '\\*'\nHere, all .png files are initially included, but then all files are excluded, resulting in no transfers.\n\nTo preview the actions of an AWS CLI command without executing it, add the –dryrun flag. It’s a handy way to ensure your command behaves as expected."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#aws-cli-s3-copy-efficient-file-transfers-fa-regular-copy",
    "href": "posts/aws-cli-s3/index.html#aws-cli-s3-copy-efficient-file-transfers-fa-regular-copy",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "AWS CLI S3 Copy: Efficient File Transfers ",
    "text": "AWS CLI S3 Copy: Efficient File Transfers \n\nThe aws s3 cp Command\nWhile aws s3 sync focuses on synchronisation and can be used to update files at the destination, aws s3 cp is primarily designed for simple and efficient file transfers. It copies files and directories from a source to a destination, which can be local, remote, or even between S3 buckets. The basic syntax is as follows:\naws s3 cp &lt;SOURCE&gt; &lt;DESTINATION&gt;\nUnlike aws s3 sync, aws s3 cp doesn’t handle recursive synchronization. It’s a straightforward file copy operation, regardless of whether the file is already present in the destination. Here’s an example:\naws s3 cp my-local-file.txt s3://my-destination-bucket/\nIn this case, my-local-file.txt is copied to the specified S3 bucket. aws s3 cp excels in scenarios where you need to upload or download individual files without worrying about folder structures.\n\n\n\n\n\n\nTip\n\n\n\nSimilar to aws s3 sync you can use the --include and --exclude parameters to filter, just make sure you also add the --recursive flag or else it wont work."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#differentiating-aws-s3-cp-and-aws-s3-sync",
    "href": "posts/aws-cli-s3/index.html#differentiating-aws-s3-cp-and-aws-s3-sync",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "Differentiating aws s3 cp and aws s3 sync",
    "text": "Differentiating aws s3 cp and aws s3 sync\n\nSynchronisation vs. Copy: aws s3 sync is designed for synchronising files between a source and a destination, ensuring that both locations have the same set of files. aws s3 cp, on the other hand, focuses on copying individual files or directories from a source to a destination.\nComplexity: aws s3 sync offers advanced features like file filtering and recursive synchronisation, making it suitable for managing complex data synchronization tasks. aws s3 cp is simpler and more suitable for straightforward efficient file transfers.\n\nIn conclusion, AWS CLI’s aws s3 cp and aws s3 sync commands cater to different aspects of managing files in AWS S3. By understanding their strengths and use cases, you can efficiently handle a wide range of file-related tasks in your AWS environment."
  },
  {
    "objectID": "posts/cast-bi-ui3/index.html",
    "href": "posts/cast-bi-ui3/index.html",
    "title": "How to Cast Blue Iris UI3",
    "section": "",
    "text": "When it comes to setting up a wall-mounted security display or displaying cameras discreetly in a bookcase, you may initially think incorporating it into your Home Assistant dashboard is the best option or that you need to mini PC attached to a monitor.\nHome Assistant, while an excellent smart home management platform, will struggle to handle multiple live camera streams resulting in unbearable delays or glitchy streams. The best option is actually relatively inexpensive and simple to setup. It involves using a Google TV chromecast and a computer monitor (with HDMI port) or a small TV, of which you might likely find you already have these lying around.\nIn this blog post, I will guide you through the process of casting the Blue Iris UI3 interface to both a Google TV Chromecast and a computer monitor, ensuring a high-quality display and lag-free security camera video streams. Although you can just use cast all the things to case the Blue Iris UI3 directly to your Chromecast, you will soon find that after 10min your Chromecast will enter ambient mode and you will lose connection with the display of your camera feeds. The following guide will sort all of this out for your\n\nPrerequisites.\nBefore we begin, make sure you have the following:\n\nA Google TV Chromecast connected to your monitor or TV.\nBlue Iris software installed and running on a computer.\nPython 3\n\n\n\n\n\n\n\nNote\n\n\n\nThis should work with other Chromecast devices, just note that some of the menus and steps listed below may differ.\n\n\n\n\nStep 1: Enable Developer Mode and USB Debugging on Chromecast\n\nConnect your Chromecast to the same Wi-Fi network as your computer and TV/monitor.\nOn your Chromecast device go to Settings and the System submenu.\nIn System click on about and scroll down until you find Android TV OS Build.\nPress the OK or Enter button 7 times to enable developer mode. - You should see a message saying ‘You are now a developer.’\nNote that at about 5 presses it should start to notify that you are about the enable developer mode.\nNow, go back to the main settings menu and select Developer options.\nTurn on Enable developer options if not already\nThen turn on USB debugging.\n\n\n\nStep 2: Get the IP Address of the Chromecast\n\nOpen the Network and Internet menu in the Chromecast and the IP address should be displayed in the right-hand panel.\nOr alternatively;\nOpen the Google Home app on your smartphone or tablet.\nTap on your Chromecast device, then tap on the gear icon to access the device settings.\nThe IP address should be displayed under the Device information section.\n\n\n\nStep 3: Install Android Developer Kit (ADB)\n\nDownload the Android SDK Platform-Tools package from the official Android website.\nExtract the downloaded zip file to a location on your computer.\nOpen a command prompt or terminal window and navigate to the folder containing the extracted files.\nExecute the following command in the command prompt or terminal:\n\nadb connect &lt;YOUR_CHROMECAST_IP_ADDRESS&gt;\n\nReplace &lt;YOUR_CHROMECAST_IP_ADDRESS&gt; with the IP address you obtained earlier.\nOn your Chromecast-connected TV or monitor, you will see a pop-up notification or a prompt asking for your permission to allow the connection from the specified IP address.\nAccept the connection by selecting “Allow” or “Accept” on your Chromecast device.\n\nOnce you’ve accepted the connection request, you can proceed with Step 4 and enter the command to change the screen timeout value.\n\n\nStep 4: Change the Screen Timeout Value on Chromecast\n\nEnter the following command to change the screen timeout value to one month (in seconds):\n\nadb shell settings put system screen_off_timeout 2592000000\n\n\nStep 5: Turn on ‘Stay Awake’ on Chromecast\n\nOn your Chromecast device still within the Developer options.\nTurn on Stay awake.\n\n\n\nStep 6: Set Up Camera Groups in Blue Iris\n\nLaunch the Blue Iris software on your computer.\nGo to the ‘Cameras’ tab and select the cameras you want to group together to display on the Chromecast.\nRight-click on the selected cameras and choose ‘Group Cameras.’\nGive the camera group a name and click ‘OK.’\n\n\n\nStep 7: Setup Cast All The Things (CATT)\n\nOpen a command prompt or terminal window on your computer.\nInstall CATT using pipx\npipx install catt\n\n\n\n\n\n\nNote\n\n\n\nYou can using pip, however pipx is recommended by the developer of CATT\n\n\nNext you need to identify your chromecast device. To scan you local network run the following\n\ncatt scan\n\nTake note of the relevant chromecase device name. You will need it in the next step.\n\n\n\nStep 8: Cast Blue Iris UI3 to Chromecast\n\nOpen a web browser and navigate to http://&lt;BLUEIRIS-IP-ADDRESS&gt;:81/ui3.htm? to check the Blue Iris UI3 interface is functioning.\nTo cast the UI3 web interface to your chromecast and monitor enter the following command\n\ncatt cast_url http://\\&lt;BLUEIRIS-IP-ADDRESS\\&gt;:81/ui3.htm?timeout=0\n\nReplace  with the IP of your Blue Iris server.\n\n\n\n\n\n\nNote\n\n\n\nNote we have added timeout=0 to the url to prevent the web interface timing out\n\n\nTo stop casting or if there is a disconnection and you need to recast, you first need to run the stop command\n\ncatt -d &lt;name_of_chromecast&gt; stop\n\nTo cast only the camera group you created add the group=groupname parameter to the url\n\ncatt cast_url http://\\&lt;BLUEIRIS-IP-ADDRESS\\&gt;:81/ui3.htm?timeout=0&group=groupname\n\n\n\n\n\n\nNote\n\n\n\nI would also recommended adding the clipview=confirmed parameter if you have AI detection on. This will show thumbnails of confirmed events.\n\n\n\nFurther url parameters can been found in the UI3 documentation.\n\n\n\nConclusion\nBy following these steps, you can easily cast the Blue Iris UI3 interface to your Google TV Chromecast and attached screen. The use of CATT ensures a smooth and high-quality display, while the screen timeout changes and ‘stay awake’ settings on the Chromecast prevent disruptions during extended use. Enjoy easy access to your Blue Iris cameras on the big screen!"
  },
  {
    "objectID": "posts/github-deploy-keys/index.html",
    "href": "posts/github-deploy-keys/index.html",
    "title": "GitHub Deploy Keys",
    "section": "",
    "text": "Photo by Chunli Ju on Unsplash\n\n\nIf you use private GitHub repositories and have wondered what is the best way of being able to deploy your repositories to remote compute environments such as AWS, you are not alone. However, there is an elegant and secure solution that doesn’t require you to store your high-level GitHub SSH key in a remote instance.\nGitHub has a feature called deploy keys which allows you to add a separate SSH key to your private repository without write access. Furthermore, you can secure this key on your remote compute instance with a password, similar to what you might do on your local machine.\n\nThe first step, remote into your remote compute instance ☁️\nRun the following command to generate your SSH key\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are only going to use one repository, accept the default name when prompted to “Enter a file in which to save the key”. However, if you want to connect to multiple repositories don’t press Enter to accept the defaults. Instead, enter a name that relates to the corresponding repository you are wanting to connect to.\n\n\n\nUse cat to print the public SSH key to the screen and then copy it to your clipboard 📋\ncat /home/&lt;USER-NAME&gt;/.ssh/&lt;KEY-NAME&gt;.pub\nGo to your GitHub repository and click Settings.\n\nThen click Deploy keys from the side menu and then click Add deploy key.\n\nPaste in your public ssh key and add a relevant title and click Add key\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure you leave ‘Allow write access’ un-ticked.\n\n\nMultiple repositories?\nWhat’s that you say, you have more than one repository, I would be worried if you didn’t. However, what you will initially find is that GitHub will not let you use the same ssh deploy key for a second repo. The solution? Generate a second key, but now you have to manually change between keys when you move between repos, and even more so remember which key is for which.\nAs always with Linux and git there is an elegant solution, a SSH config file.\n\nCreate a ssh config file with the following commands\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nUsing a command line text editor like vi edit the config file\nvi ~/.ssh/config\nAdd the following configuration information\nHost repo-alias\n  Hostname github.com\n  User git\n  IdentityFile /home/&lt;USER-NAME&gt;/.ssh/&lt;KEY-NAME&gt;\nWhat are these fields?\n\nHost: An alias to assign to the corresponding repository (does not need to be the same as the repo name)\nHostname: the hosting service i.e. github.com\nUser: service using the ssh key (in this case git)\nIdentityFile: file path to the ssh key you previously generated\n\n\n\n\n\n\n\nTip\n\n\n\nWhile the alias you choose doesn’t have to match the repository name, it’s crucial to make it memorable. Using the repository name as the alias is a straightforward and effective approach.\n\n\nRepeat this process for any other repositories and SSH keys\n\nNow that you have added all your SSH keys along with a corresponding alias you can easily direct git to use the relevant key when working with different repositories.\nTo do this you insert the alias into the repository address between git@ and :repo-owner-name/repo-name.git\ngit clone git@&lt;alias&gt;:repo-owner-name/repo-name.git .\nOnce you have cloned a repository and want to run other commands such as git pull all you need to do is include the repo alias and address similar to the clone command.\ngit pull &lt;alias&gt;:repo-owner-name/repo-name\nFor instance, imagine Harry Potter 🧙🔮 maintains a GitHub repository named lumos and wishes to link it to a remote compute instance using the alias pipeline1. In his SSH config file, he would set it up like this:\nHost pipeline1\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/pipeline1_id_ed25519\nNow, when he wants to clone the repository, he can simply execute:\ngit clone git@pipeline1:harrypotter/lumos.git .\nTo fetch updates, he can use:\ngit pull pipeline1:harrypotter/lumos\nWith these steps, you have now streamlined your deployment process and ensured the appropriate SSH key can be selected. I hope this guide has been helpful in demystifying the process of deploying private repositories and managing SSH keys.\nHappy coding! 🧑‍💻🔑"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html",
    "href": "posts/push-docker-to-ecr/index.html",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "",
    "text": "Photo by Barrett Ward on Unsplash"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#prerequisites",
    "href": "posts/push-docker-to-ecr/index.html#prerequisites",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Prerequisites ✅",
    "text": "Prerequisites ✅\nBefore you begin, ensure you have the following prerequisites in place:\n\nAWS Account: You must have an AWS account to create and use an ECR repository.\nDocker: Docker must be installed on your local machine. You can download it from Docker’s official website.\nAWS CLI: Make sure the AWS Command Line Interface (CLI) is installed and configured with your AWS credentials. You can configure it using aws configure."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-1-create-an-ecr-repository-destination-setup",
    "href": "posts/push-docker-to-ecr/index.html#step-1-create-an-ecr-repository-destination-setup",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 1: Create an ECR Repository (Destination Setup) 📭",
    "text": "Step 1: Create an ECR Repository (Destination Setup) 📭\nFirst, you need to set up an ECR repository to store your Docker images. Follow these steps:\n\nLogin to AWS Console: Log in to your AWS Management Console.\nOpen ECR: Navigate to the Amazon Elastic Container Registry (ECR) service.\nCreate a Repository: Click on the “Create repository” button.\nRepository Settings: Enter a unique name for your repository and configure any additional settings, such as Private vs. Public visibility.\nCreate Repository: Click the “Create repository” button to create your ECR repository.\n\nNow that you have your ECR repository set up, it’s time to prepare your local Docker image."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-2-prepare-your-local-docker-image-label-your-cargo",
    "href": "posts/push-docker-to-ecr/index.html#step-2-prepare-your-local-docker-image-label-your-cargo",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 2: Prepare Your Local Docker Image (Label Your Cargo) 🏷️",
    "text": "Step 2: Prepare Your Local Docker Image (Label Your Cargo) 🏷️\nBefore you can push your Docker image to ECR, you need to tag it with the relevant ECR URI. Here’s how you can do it:\ndocker image tag &lt;IMAGE_NAME&gt;:&lt;IMAGE_TAG&gt; &lt;REPOSITORY_URI&gt;:&lt;IMAGE_TAG&gt;\nFor example, if your image is named “myapp” and has a tag “v1.0,” and your repository URI is “123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo,” you would run:\ndocker image tag myapp:v1.0 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo:v1.0\nYou can verify that your image is correctly tagged by running:\ndocker images"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-3-authenticate-the-docker-cli-with-aws-secure-the-payment",
    "href": "posts/push-docker-to-ecr/index.html#step-3-authenticate-the-docker-cli-with-aws-secure-the-payment",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 3: Authenticate the Docker CLI with AWS (Secure the Payment) 🔑",
    "text": "Step 3: Authenticate the Docker CLI with AWS (Secure the Payment) 🔑\nTo enable secure communication between Docker and your ECR repository, you need to authenticate the Docker CLI with AWS. AWS provides a temporary authentication token for this purpose. Here’s how you can do it:\naws ecr get-login-password --region &lt;REPOSITORY_REGION&gt; | docker login --username AWS --password-stdin &lt;REPOSITORY_URI&gt;\nReplace &lt;REPOSITORY_REGION&gt; with the AWS region where your ECR repository is located (e.g., us-east-1), and &lt;REPOSITORY_URI&gt; with the URI of your ECR repository.\nPlease note that the authentication token provided by AWS is temporary and will expire after 12 hours."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-4-push-the-docker-image-to-ecr-send-it-off",
    "href": "posts/push-docker-to-ecr/index.html#step-4-push-the-docker-image-to-ecr-send-it-off",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 4: Push the Docker Image to ECR (Send It Off) 🚚",
    "text": "Step 4: Push the Docker Image to ECR (Send It Off) 🚚\nWith authentication in place, it’s time to push your Docker image to Amazon ECR:\ndocker image push &lt;REPOSITORY_URI&gt;:&lt;IMAGE_TAG&gt;\nFor example:\ndocker image push 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo:v1.0\nThis command will upload your Docker image to your ECR repository.\nAnd that’s it! You have successfully pushed your Docker image to your AWS Private ECR repository. You can now use this image for deploying containers on AWS services like Batch just point to its URI in your relevant scripts."
  },
  {
    "objectID": "ramblings/bash/index.html#using-tee-to-log-command-outputs.",
    "href": "ramblings/bash/index.html#using-tee-to-log-command-outputs.",
    "title": "Bash",
    "section": "Using {tee} to log command outputs.",
    "text": "Using {tee} to log command outputs.\nWhen working in a Bash terminal and you want to log the output of a command for future reference, the tee command is a handy tool, especially in multi step analyses. Here’s a quick tip on how to use it effectively:\nBasic Usage: To log the output of a command and simultaneously view it on the terminal, use the following format:\nmy_command | tee my_log.txt\nReplace my_command with your actual command and my_log.txt with the desired log file name.\nAppending to Existing Logs: To append output to an existing log file (useful for logging multiple command or runs of the same command), use the -a option with tee:\nmy_command | tee -a my_existing_log.txt\nCapture standard error aswell: By default tee captures standard out (stdout) and saves it to file whilst also printing to screen. However if you want to capture standard error (stderr) as well as stdout just add 2&gt;&1 after your command before you pipe to tee:\nmy_command 2&gt;&1 | tee my_log.txt"
  },
  {
    "objectID": "ramblings/bash/index.html#using-basename-in-bash-for-file-name-extraction",
    "href": "ramblings/bash/index.html#using-basename-in-bash-for-file-name-extraction",
    "title": "Bash",
    "section": "Using {basename} in Bash for File Name Extraction",
    "text": "Using {basename} in Bash for File Name Extraction\nIn Bash, you can use the basename command to extract the file name from a given file path. This is particularly useful when you’re working on tasks like data processing pipelines or automation scripts and need to isolate the file’s name from its full path.\nBasic Syntax:\nbasename [PATH]\nUsage Example: Suppose you have the following file path: /path/to/your/file.txt. Using the basename command as follows:\nbasename /path/to/your/file.txt\nYou will obtain just the file name file.txt as output:\nIf you also want to remove the trailing suffix simply add the suffix as a second argument to the end of the command:\nbasename file.txt .txt\nWhich in this example will return file"
  }
]