[
  {
    "objectID": "ramblings.html",
    "href": "ramblings.html",
    "title": "",
    "section": "",
    "text": "Bash\n\n\n\n\n\n\nBash\n\n\n\nA collection of short Bash commands and tricks that individually are too short for a blog post\n\n\n\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nColours\n\n\n\n\n\n\nColours\n\n\n\nA collection colour palettes and reference panels with copy paste hex codes\n\n\n\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\nQuarto\n\n\n\nA collection of short Quarto code snippets and tricks that individually are too short for a blog post\n\n\n\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\nR\n\n\n\nA worthy collection of short R commands and tricks\n\n\n\n\n\nLW Pembleton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ramblings/Quarto/index.html",
    "href": "ramblings/Quarto/index.html",
    "title": "Quarto",
    "section": "",
    "text": "~Photo¬†by¬†Solstice Hannan¬†on¬†Unsplash~"
  },
  {
    "objectID": "ramblings/Quarto/index.html#folding-non-executable-code-in-quarto-documents",
    "href": "ramblings/Quarto/index.html#folding-non-executable-code-in-quarto-documents",
    "title": "Quarto",
    "section": "Folding Non-Executable Code in Quarto Documents",
    "text": "Folding Non-Executable Code in Quarto Documents\nIn Quarto, folding non-executable code blocks may seem challenging since the built-in code-folding `#| code-fold: true` for non-executed blocks is not supported. This can be particularly inconvenient when working with extensive external commands like bash scripts used for data preprocessing.\nFortunately, a simple solution is available, as shared in response to a Quarto GitHub issue. You can achieve code folding for non-executable code by wrapping it in HTML &lt;details&gt; tags. Here‚Äôs an example:\n&lt;details&gt;\n&lt;summary&gt;Code: My non-executable code&lt;/summary&gt;\n\n```bash\ncat vcf_file | sed -n '/#CHROM/,$p' | cut -f 1,2,4,5 &gt; variant_info.txt \n```\n\n&lt;/details&gt;\nWhich when rendered will look like üëá\n\n\nCode: My non-executable code\n\ncat vcf_file | sed -n '/#CHROM/,$p' | cut -f 1,2,4,5 &gt; variant_info.txt \n\nThis method allows you to organize and present non-executable code blocks more effectively in your Quarto documents."
  },
  {
    "objectID": "ramblings/bash/index.html#working-with-tar-and-gz-files",
    "href": "ramblings/bash/index.html#working-with-tar-and-gz-files",
    "title": "Bash",
    "section": "Working with Tar and Gz Files üóúÔ∏è",
    "text": "Working with Tar and Gz Files üóúÔ∏è\nAlthough in bioinformatics we like our data, the more the merrier. System admins and our cloud computing bills often hold a different view. Consquently more often then not in bioinformatics you will encounter files that have the .gz extension. These have been compressed by the popular Gzip algorithm to save disk space.\nUncompressing and recompressing such files, is straighforwards and even viewing their contents, is akin to using the cat command:\n# Uncompress a .gz file\ngunzip sample01.vcf.gz\n\n# Compress a file\ngzip sample01.vcf\n\n# View contents of a .gz file\nzcat sample01.vcf.gz\n\n\n\n\n\n\nTip\n\n\n\nYou can use zcat to pipe the uncompressed contents of a .gz file into other bash commands like sed, grep, awk etc, without writing the uncompressed file to disk.\n\n\nAnother prevalent file type you may come across is one with a .tar.gz extension. These files are Tar archives that have undergone compression with the Gzip algorithm. Essentially, they are a collection of files or directories bundled into an archive. They find common usage in backups, software distribution, or as a convenient way to store related files.\nWhen extracting content from compressed tar files, the tar command is your go-to:\ntar -xvf archive.tar.gz\n-x is the extract option, -f specifies that the filename to extract will follow after, while -v is optional, it will print the names of the files being extracted from the archive to the terminal.\nYou can also extract just specific files from the archive, provided you know their names üîÆ\ntar -xf archive.tar.gz file1 file2\nHowever, if the specific file names elude you (we are only human), you can list the contents of the archive with:\ntar -tf archive.tar.gz\nIf you get a bit of variety in your bioinformatic journey and stumble upon a .tar.bz2 file, no fear it has just been compressed with the bzip2 algorithm. Just treat it the same as the .tar.gz file, and tar will do all the hard work.\ntar -xvf archive.tar.bz2"
  },
  {
    "objectID": "ramblings/bash/index.html#monitoring-process-times",
    "href": "ramblings/bash/index.html#monitoring-process-times",
    "title": "Bash",
    "section": "Monitoring process times ‚è±Ô∏è",
    "text": "Monitoring process times ‚è±Ô∏è\nAn often overlooked tool that significantly aids in optimizing processes is the ‚Äòtime‚Äô command in Unix-based systems. This powerful tool not only executes a given command but also provides valuable information about how much time the command took to run.\nTo quantify the duration of a process, simply append the time command at the beginning:\ntime bcftools filter -O z -o filtered.vcf.gz -i '%QUAL&gt;50' in.vcf.gz\nThe output of the time command will typically have three components:\nReal time (real): This is the actual clock time elapsed during the execution of the command.\nUser CPU time (user): This represents the amount of CPU time spent in user-mode code (i.e., within the process).\nSystem CPU time (sys): This denotes the CPU time spent in the kernel within the system calls made by the command.\nIn the world of bioinformatics where there is 101 ways to do the same time, it is often wise to do some benchmarking with the time command to ensure you are using the most efficient tool."
  },
  {
    "objectID": "ramblings/bash/index.html#using-tee-to-log-command-outputs.",
    "href": "ramblings/bash/index.html#using-tee-to-log-command-outputs.",
    "title": "Bash",
    "section": "Using {tee} to log command outputs.",
    "text": "Using {tee} to log command outputs.\nWhen working in a Bash terminal and you want to log the output of a command for future reference, the tee command is a handy tool, especially in multi step analyses. Here‚Äôs a quick tip on how to use it effectively:\nBasic Usage: To log the output of a command and simultaneously view it on the terminal, use the following format:\nmy_command | tee my_log.txt\nReplace my_command with your actual command and my_log.txt with the desired log file name.\nAppending to Existing Logs: To append output to an existing log file (useful for logging multiple command or runs of the same command), use the -a option with tee:\nmy_command | tee -a my_existing_log.txt\nCapture standard error aswell: By default tee captures standard out (stdout) and saves it to file whilst also printing to screen. However if you want to capture standard error (stderr) as well as stdout just add 2&gt;&1 after your command before you pipe to tee:\nmy_command 2&gt;&1 | tee my_log.txt"
  },
  {
    "objectID": "ramblings/bash/index.html#using-basename-in-bash-for-file-name-extraction",
    "href": "ramblings/bash/index.html#using-basename-in-bash-for-file-name-extraction",
    "title": "Bash",
    "section": "Using {basename} in Bash for File Name Extraction",
    "text": "Using {basename} in Bash for File Name Extraction\nIn Bash, you can use the basename command to extract the file name from a given file path. This is particularly useful when you‚Äôre working on tasks like data processing pipelines or automation scripts and need to isolate the file‚Äôs name from its full path.\nBasic Syntax:\nbasename [PATH]\nUsage Example: Suppose you have the following file path: /path/to/your/file.txt. Using the basename command as follows:\nbasename /path/to/your/file.txt\nYou will obtain just the file name file.txt as output:\nIf you also want to remove the trailing suffix simply add the suffix as a second argument to the end of the command:\nbasename file.txt .txt\nWhich in this example will return file"
  },
  {
    "objectID": "ramblings/bash/index.html#streamling-file-transfer-via-a-bastion-host",
    "href": "ramblings/bash/index.html#streamling-file-transfer-via-a-bastion-host",
    "title": "Bash",
    "section": "Streamling File Transfer via a Bastion Host",
    "text": "Streamling File Transfer via a Bastion Host\n\nAccessing private servers through a bastion host, like AWS private cloud, and transferring local files doesn‚Äôt have to be a complex process. With a straightforward command utilising scp and the ProxyJump feature, you can seamlessly accomplish this task. Assuming your SSH keys are configured correctly on your local machine, here‚Äôs how to automate the transfer to the endpoint private server via the public-facing bastion host.\nscp -J &lt;user-name&gt;@&lt;Public-IPv4-DNS&gt; \\\n    &lt;file-to-transfer&gt; \\\n    &lt;user-name&gt;@&lt;IP-address&gt;:/path/for/remote/file\nFor systems where ProxyJump isn‚Äôt available, you can opt for the ProxyCommand method instead.\nscp -o 'ProxyCommand ssh &lt;user-name&gt;@&lt;Public-IPv4-DNS&gt; -W %h:%p' \\\n  &lt;file-to-transfer&gt; \\\n  &lt;user-name&gt;@&lt;IP-address&gt;:/path/for/remote/file\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs worth noting that while both methods facilitate the transfer, ProxyJump is recommended over ProxyCommand as it adds an extra layer of encryption to the traffic, enhancing security during file transfers."
  },
  {
    "objectID": "ramblings/bash/index.html#extract-data-lines-from-a-vcf-file",
    "href": "ramblings/bash/index.html#extract-data-lines-from-a-vcf-file",
    "title": "Bash",
    "section": "Extract Data Lines From a VCF File",
    "text": "Extract Data Lines From a VCF File\n\nVisual inspection üîç of the data lines (i.e.¬†genotype fields) of VCF file is often impeded ‚õî by the large number of meta-information lines at the top. A simple sed command that looks for the mandatory body header line can get your past this hurdle üöß\nsed -n '/#CHROM/,$p' filename.vcf\nIn short this command searches for the pattern ‚Äò#CHROM‚Äô and then starts printing to the screen until the end of the file. Pipe this into a head command (| head) and you now have a quick and simple way üü¢ to inspect the start of a VCF file."
  },
  {
    "objectID": "ramblings/bash/index.html#separate-columns-in-file-with-multiple-whitespaces",
    "href": "ramblings/bash/index.html#separate-columns-in-file-with-multiple-whitespaces",
    "title": "Bash",
    "section": "Separate columns in file with multiple whitespaces",
    "text": "Separate columns in file with multiple whitespaces\nWhen dealing with tabular text files where columns are inconsistently separated by varying numbers of whitespaces, a quick solution comes through using the translate command tr and the -s squeeze repeats feature. By employing tr to convert multiple consecutive whitespaces into a single whitespace, you can seamlessly tidy üßπ the data before applying cut for column extraction.\nFor instance, to isolate the 3rd and 4th columns from a file:\ntr -s ' ' &lt; file.txt | cut -d ' ' -f 2,3\n#  ^^^                            ^^^^^^\n#   |                                |\n#   |                                |\n# squeeze repeats            get 2nd & 3rd field"
  },
  {
    "objectID": "ramblings/bash/index.html#human-readable-file-and-directory-sizes",
    "href": "ramblings/bash/index.html#human-readable-file-and-directory-sizes",
    "title": "Bash",
    "section": "Human-readable file and directory sizes",
    "text": "Human-readable file and directory sizes\nWant human readable file sizes in your ls commands? Simply add - ld\n# Size in bytes\nls -l\n\n# Human readable sizes\nls -h \nSame principle applies to directory sizes with the du disk usage command\n# List directory sizes 1 level deep\ndu -hd1\n\n# List the total size of the current directory\ndu -hs"
  },
  {
    "objectID": "ramblings/bash/index.html#quickly-search-your-bash-history-for-a-previous-command",
    "href": "ramblings/bash/index.html#quickly-search-your-bash-history-for-a-previous-command",
    "title": "Bash",
    "section": "Quickly search your bash history for a previous command",
    "text": "Quickly search your bash history for a previous command\nSay goodbye to pressing ‚ñ≤‚ñ≤ endlessly in bash to get back to a previously used command. Instead your can press CtrlCtrl + RR and search for any matching text in your previous command history. Keep pressing CtrlCtrl + RR to cycle through more matches.\nüí° Think ahead and if there is a command you will likely need to run again, tag it with a comment # that your can then easily search for. For instance say you have a specific command you know you will want to run again later, append #cmd for later after the command. That way, you have a unique CtrlCtrl + RR search phrase for quicker retrieval.\nOn some systems the default bash history can be short which will limit how far your can search. To check your current bash history size run\necho $HISTSIZE\nIf you want to increase your history size you need to edit your ~/.bashrc and add (if not already present):\nHISTSIZE=50000 # or a different number\nAlso if you want to avoid some commands appearing in your history just append a whitespace for example\necho 'this will appear in my bash history'\n\n echo 'this will not appear in my bash history'"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html",
    "href": "posts/push-docker-to-ecr/index.html",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "",
    "text": "Photo¬†by¬†Barrett¬†Ward¬†on¬†Unsplash"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#prerequisites",
    "href": "posts/push-docker-to-ecr/index.html#prerequisites",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Prerequisites ‚úÖ",
    "text": "Prerequisites ‚úÖ\nBefore you begin, ensure you have the following prerequisites in place:\n\nAWS Account: You must have an AWS account to create and use an ECR repository.\nDocker: Docker must be installed on your local machine. You can download it from Docker‚Äôs official website.\nAWS CLI: Make sure the AWS Command Line Interface (CLI) is installed and configured with your AWS credentials. You can configure it using aws configure."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-1-create-an-ecr-repository-destination-setup",
    "href": "posts/push-docker-to-ecr/index.html#step-1-create-an-ecr-repository-destination-setup",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 1: Create an ECR Repository (Destination Setup) üì≠",
    "text": "Step 1: Create an ECR Repository (Destination Setup) üì≠\nFirst, you need to set up an ECR repository to store your Docker images. Follow these steps:\n\nLogin to AWS Console: Log in to your AWS Management Console.\nOpen ECR: Navigate to the Amazon Elastic Container Registry (ECR) service.\nCreate a Repository: Click on the ‚ÄúCreate repository‚Äù button.\nRepository Settings: Enter a unique name for your repository and configure any additional settings, such as Private vs.¬†Public visibility.\nCreate Repository: Click the ‚ÄúCreate repository‚Äù button to create your ECR repository.\n\nNow that you have your ECR repository set up, it‚Äôs time to prepare your local Docker image."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-2-prepare-your-local-docker-image-label-your-cargo",
    "href": "posts/push-docker-to-ecr/index.html#step-2-prepare-your-local-docker-image-label-your-cargo",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 2: Prepare Your Local Docker Image (Label Your Cargo) üè∑Ô∏è",
    "text": "Step 2: Prepare Your Local Docker Image (Label Your Cargo) üè∑Ô∏è\nBefore you can push your Docker image to ECR, you need to tag it with the relevant ECR URI. Here‚Äôs how you can do it:\ndocker image tag &lt;IMAGE_NAME&gt;:&lt;IMAGE_TAG&gt; &lt;REPOSITORY_URI&gt;:&lt;IMAGE_TAG&gt;\nFor example, if your image is named ‚Äúmyapp‚Äù and has a tag ‚Äúv1.0,‚Äù and your repository URI is ‚Äú123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo,‚Äù you would run:\ndocker image tag myapp:v1.0 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo:v1.0\nYou can verify that your image is correctly tagged by running:\ndocker images"
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-3-authenticate-the-docker-cli-with-aws-secure-the-payment",
    "href": "posts/push-docker-to-ecr/index.html#step-3-authenticate-the-docker-cli-with-aws-secure-the-payment",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 3: Authenticate the Docker CLI with AWS (Secure the Payment) üîë",
    "text": "Step 3: Authenticate the Docker CLI with AWS (Secure the Payment) üîë\nTo enable secure communication between Docker and your ECR repository, you need to authenticate the Docker CLI with AWS. AWS provides a temporary authentication token for this purpose. Here‚Äôs how you can do it:\naws ecr get-login-password --region &lt;REPOSITORY_REGION&gt; | docker login --username AWS --password-stdin &lt;REPOSITORY_URI&gt;\nReplace &lt;REPOSITORY_REGION&gt; with the AWS region where your ECR repository is located (e.g., us-east-1), and &lt;REPOSITORY_URI&gt; with the URI of your ECR repository.\nPlease note that the authentication token provided by AWS is temporary and will expire after 12 hours."
  },
  {
    "objectID": "posts/push-docker-to-ecr/index.html#step-4-push-the-docker-image-to-ecr-send-it-off",
    "href": "posts/push-docker-to-ecr/index.html#step-4-push-the-docker-image-to-ecr-send-it-off",
    "title": "Pushing Docker Images to AWS ECR",
    "section": "Step 4: Push the Docker Image to ECR (Send It Off) üöö",
    "text": "Step 4: Push the Docker Image to ECR (Send It Off) üöö\nWith authentication in place, it‚Äôs time to push your Docker image to Amazon ECR:\ndocker image push &lt;REPOSITORY_URI&gt;:&lt;IMAGE_TAG&gt;\nFor example:\ndocker image push 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp-repo:v1.0\nThis command will upload your Docker image to your ECR repository.\nAnd that‚Äôs it! You have successfully pushed your Docker image to your AWS Private ECR repository. You can now use this image for deploying containers on AWS services like Batch just point to its URI in your relevant scripts."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html",
    "href": "posts/nextflow-on-aws-batch/index.html",
    "title": "Nextflow on AWS Batch",
    "section": "",
    "text": "Photo¬†by¬†Jenessaa¬†Lu¬†on¬†Unsplash\nThe following is a general guide on how to set up Nextflow with AWS batch as the compute environment. I would highly recommend that you use your local environment or at least a smaller test dataset for pipeline development, transferring to AWS batch when in a working production state.\nAlthough Gandalf üßô trims his beard more often than Amazon updates their AWS user interface, I cannot guarantee the included menu screenshots will look the same on your system. However, hopefully they will still provide sufficient information to determine the appropriate settings and options. Reach out if you feel I need to update this guide."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "href": "posts/nextflow-on-aws-batch/index.html#iam-setup",
    "title": "Nextflow on AWS Batch",
    "section": "IAM Setup",
    "text": "IAM Setup\nFirstly you need to create a new IAM with more appropriate permissions tailored to the requirements listed in Nextflow documentation. It is strongly recommended that do not use your root account to run Nextflow pipelines.\n\nOpen the IAM management console on AWS and add a new user\nEnter an appropriate user name for example ‚ÄòNextflow-access‚Äô. Under access type, select programmatic access\n\nNext you need to create a user group for the new user to sit within. Generally, on AWS you will apply permissions to a user group rather than a specific user. Additionally, this allows you to set up multiple separate people within the ‚ÄòNextflow group‚Äô. Again, enter an appropriate name and click Create group\nAdd any metadata tags if appropriate\nClick Create user. You should be greeted with a new page that includes a Access Key ID and SCA (üìù take note of these keys as you will need them towards the end of this guide)\n\nNow that you have your new user and Nextflow group you will need to apply the required permissions.\n\nFrom the IAM user panel click User groups select your recently created ‚Äònextflow‚Äô group, and under the permissions menu click on the Attach policy button\n\nClick Create policy\n\nUse the visual editor to add all the required permissions\n\nMinimal permissions policies to be attached to the AWS account used by Nextflow are:\n\nTo interface AWS Batch:\n\"batch:DescribeJobQueues\"\n\"batch:CancelJob\"\n\"batch:SubmitJob\"\n\"batch:ListJobs\"\n\"batch:DescribeComputeEnvironments\"\n\"batch:TerminateJob\"\n\"batch:DescribeJobs\"\n\"batch:RegisterJobDefinition\"\n\"batch:DescribeJobDefinitions\"\nTo be able to see the EC2 instances:\n\"ecs:DescribeTasks\"\n\"ec2:DescribeInstances\"\n\"ec2:DescribeInstanceTypes\"\n\"ec2:DescribeInstanceAttribute\"\n\"ecs:DescribeContainerInstances\"\n\"ec2:DescribeInstanceStatus\"\nTo pull container images stored in the ECR repositories:\n\"ecr:GetAuthorizationToken\"\n\"ecr:BatchCheckLayerAvailability\"\n\"ecr:GetDownloadUrlForLayer\"\n\"ecr:GetRepositoryPolicy\"\n\"ecr:DescribeRepositories\"\n\"ecr:ListImages\"\n\"ecr:DescribeImages\"\n\"ecr:BatchGetImage\"\n\"ecr:GetLifecyclePolicy\"\n\"ecr:GetLifecyclePolicyPreview\"\n\"ecr:ListTagsForResource\"\n\"ecr:DescribeImageScanFindings\"\n\nYou also need to add permissions for S3 so that nextlflow can pull input data and publish results. Still using the visual editor select S3 as the service and then select the All S3 actions (s3:*) check box under actions. You may get notifications of other ‚Äòdependency‚Äô type permissions that are required, follow the instructions to add these as well.\n\nAdd any metadata tags if appropriate\nGive your new policy a name and click Create policy\nSelect your newly created permission policy to add to the user group and click Add permissions. Hint: you can find your new policy by üîçsearching in the filter box\n\nTo be able to use spot instances you will need to create an additional role.\n\nClick Roles under the IAM access management menu and click Create role\n\nSelect AWS service and EC2 under common use cases, click Next\nSearch for AmazonEC2SpotFleetTaggingRole select it and click Next\nAdd a role name, e.g.¬†AmazonEC2SpotFleetRole and click Create role"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "href": "posts/nextflow-on-aws-batch/index.html#custom-nextflow-ami",
    "title": "Nextflow on AWS Batch",
    "section": "Custom Nextflow AMI",
    "text": "Custom Nextflow AMI\nAWS batch uses Amazon Machine Images (AMIs) to initiate EC2 compute instances that will subsequently run your Nextflow processes. Nextflow tasks submitted to AWS Batch will run under the Amazon Elastic Container Service (ECS). ECS (not to be confused with EC2) uses a base Amazon ECS-optimised AMI (with docker pre-installed). Although Nextflow & Batch will control the CPU and memory resource request and allocation you need to ensure you base ECS AMI has sufficient EBS storage to hold any relevant input and working data files, such as sequence reads, indexes etc. You will also need to install the AWS CLI in the base ECS AMI to allow data movement to and from S3 buckets. To set all this up follow these steps:\n\nNavigate to the EC2 console menu\nClick Instances and then Launch Instances\nUnder ‚Äòquick start‚Äô click Browse more AMIs\nClick AWS Marketplace AMIs and search for ECS\nAt the time of writing amzn2-ami-ecs-hvm-2.0.20221025-x86_64-ebs was the most up-to-date ECS AMI. Select it\n\nSelect the t2.micro instance type\nSelect and relevant key pairs and network settings based on your setup (I would recommend at a minimum a private VPC and IP-restricted connections via a bastion instance)\nEnsure you have at least 30GiB storage üíæ listed under ‚ÄòConfigure storage‚Äô. Also change the storage type from gp2 to gp3 (for a performance boost at no additional cost - see Matt Vaughn‚Äôs NextflowSummit 2022 talk üìΩÔ∏è).\n\n\n\n\n\n\nNote\n\n\n\nFor some Nextflow processes your will need more than 30GiB of EBS storage. I would recommend making additional AMIs (based on this image) for these specific tasks and assigning them to specific Batch job queues, see later on.\n\n\nClick Launch instance üöÄ\nSSH üíª into your new instance where you will need to install AWS CLI\nOnce connected run the following commands to install AWS CLI\ncd $HOME\nsudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nTo verify the install was successful\n$ ./miniconda/bin/aws --version\naws-cli/1.19.79 Python/3.8.5 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.20.79\nUnder the Instances menu in the EC2 console select your relevant instance and click Actions, then Images and Templates, then Create Image\nGive your new image a name e.g.¬†nextflow-30GiB-ecs-ami and click Create image\nüìùTake note of the AMI ID (not the name) that you just generated as you will need this later\n\n\n\n\n\n\n\nNote\n\n\n\nContrary to what is commonly written in other documentation you no longer need to expand your docker üêãstorage volume to match your allocated EBS storage size. The docker storage automatically expands on the Amazon 2 AMIs which are now default (unlike previous Amazon 1 AMIs)."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "href": "posts/nextflow-on-aws-batch/index.html#batch-environment",
    "title": "Nextflow on AWS Batch",
    "section": "Batch Environment",
    "text": "Batch Environment\nNow it is time to create your Batch environment which entails at least one compute environment and one job queue that Nextflow will submit processes to.\nNavigate to the Batch AWS console and click on Compute environments.\n\nClick Create and select Amazon Elastic Compute Cloud (Amazon EC2) as the compute environment.\nSelect Managed as the orchestration type and enter a suitable name for your new compute environment.\nIf this is your first time setting up a Batch environment AWS will create the relevant service role and instance role. Just ensure ‚ÄòCreate new role‚Äô is selected. Alternatively, under ‚ÄòService role‚Äô select AWSServiceRoleForBatch and under ‚ÄòInstance Role‚Äô select ecsInstanceRole. Click Next Page\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the allowed maximum number of parallel vCPU tasks that can run in your compute environment at any one time. Increase or decrease this to an appropriate number based on your requirements.\n‚ÄòAllowed instance type‚Äô allows you to control the type of instances that AWS is allowed to try and run your jobs on. Your CPU and memory requirements defined in your Nextflow config will apply a second tier of filtering (i.e.¬†if your memory request is higher than an allowed instance type, obviously that instance type won‚Äôt be used). You can leave this as ‚Äòoptimal‚Äô and AWS will attempt to find the best instance type match to your CPU and memory request.\n\n\n\n\n\n\nNote\n\n\n\nAWS will generally group multiple jobs onto the one large instance, however, this can result in errors, particularly from noisy neighbors, and I/O and/or network intensive tasks.\nIf you want to prevent AWS from grouping multiple jobs onto the one larger instance, then you need to specifically define smaller instances types, e.g.¬†r6i.xlarge, r6i.2xlarge, to prevent AWS using super instances such as r6i.24xlarge r6i.32xlarge.\n\n\nTo use spot instances toggle the Use EC2 Spot instances button at the top and define your maximum cut-off for on-demand price under ‚ÄòMaximum % on-demand price‚Äô. Under ‚Äòspot fleet role‚Äô you will also need to select the AmazonEC2SpotFleetRole role that you created earlier.\nUnder ‚ÄòAdditional configuration‚Äô you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren‚Äôt available, AWS Batch waits for the additional instances to be available. If there aren‚Äôt enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don‚Äôt run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you‚Äôre using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn‚Äôt supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren‚Äôt available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‚ÄòEC2 configuration‚Äô click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‚ÄòImage ID override‚Äô box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‚ÄòAmazon Elastic Compute Cloud (Amazon EC2)‚Äô as the compute environment.\nEnter a suitable name for your new job queue (üìù take note of this name you will need it later)\nUnder ‚ÄòConnected compute environments‚Äô select the compute environment that you just created\nClick Create job queue\n\nYou will want Nextflow to use an S3 bucket to store all the working files and results rather than a local connection.\n\nNavigate to the S3 service under the AWS management console and create a new private bucket in your relevant region.\nCreate a new folder within the bucket to serve as the Nextflow working directory (üìù take note of the S3 URI address as you will need this next)"
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "href": "posts/nextflow-on-aws-batch/index.html#nextflow-config",
    "title": "Nextflow on AWS Batch",
    "section": "Nextflow Config",
    "text": "Nextflow Config\nNow all you now need to do is set up your Nextflow config with the relevant details of your AWS setup. An example of initial config file is:\n//Select the awsbatch executor\nprocess.executor = 'awsbatch'\n\n//Name of the AWS Batch job queue that you just created\nprocess.queue = 'my-batch-queue'\n\n//region where we want to run this in\naws.region = 'ap-southeast-2'\n\n//Path to the aws cli tool you installed in your AMI\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n\n//S3 working directory that you just created\nworkDir = 's3://bucket_you_created/work/'\nThe last step is setting up your security credentials üîê to allow Nextflow to securely communicate and submit jobs to AWS batch. The best approach is to install AWS CLI locally (or in a EC2 instance if submitting from EC2).\nThen run AWS configure and enter the relevant Key ID, Access Key, and Region when prompted. These are the keys that AWS provided when you generated your Nextflow programmatic user at the start of this guide.\n\n\n\n\n\n\nWarning\n\n\n\nDO NOT store your credentials in your Nextflow configuration file as some tutorials suggest."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "href": "posts/nextflow-on-aws-batch/index.html#additional-notes",
    "title": "Nextflow on AWS Batch",
    "section": "üóíÔ∏èAdditional Notes:",
    "text": "üóíÔ∏èAdditional Notes:\n\nAWS batch jobs can take a few minutes to spin up, be patient before assuming you have set something up wrong\nIf you are using spot instances and your maximum % on-demand price is set too low your jobs make take a long time to start or may not run at all\nYou can view the log stream of your jobs by clicking through the ‚ÄòRunning‚Äô job numbers in the Batch dashboard and clicking the Log stream name - helpful to determine where a job is up to in a script\nThe Nextflow slack channel is a great place to raise any questions if you are still experiencing issues after following this setup guide, or want to experiment with some more advanced configurations and setups."
  },
  {
    "objectID": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "href": "posts/nextflow-on-aws-batch/index.html#common-errors",
    "title": "Nextflow on AWS Batch",
    "section": "Common errors",
    "text": "Common errors\nBelow are a list of common errors. Although the proposed solution has been demonstrated to work it may not always work in your specific scenario.\nTask failed to start - CannotPullContainerError: context canceled\nProposed solution: Increase your AMI EBS storage."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html",
    "href": "posts/ec2-tips-and-tricks/index.html",
    "title": "EC2 Village",
    "section": "",
    "text": "Photo¬†by¬†Karl¬†Pawlowicz¬†on¬†Unsplash\nThis originally started as a collection of notes I had made after progressively setting up multiple EC2 dev environments. Then I thought it might as well tidy it up üßπüßº a bit and push it out as a blog post in the hope it will help others. I plan to progressively add to it, so shout-out üí¨ if you have suggestions.\nThe following is written with the default assumption of a Ubuntu 22.04 LTS base image."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#notes-when-creating-a-new-ec2-instance",
    "href": "posts/ec2-tips-and-tricks/index.html#notes-when-creating-a-new-ec2-instance",
    "title": "EC2 Village",
    "section": "Notes when creating a new EC2 instance",
    "text": "Notes when creating a new EC2 instance\nWhen creating your EC2 instance ensure that under the storage settings that you change the default gp2 to gp3. gp3 has higher baseline performance, adjustable IOPS and throughput. Plus it‚Äôs cheaper per GiB ($0.08 vs $0.10/GiB-month) ü§î hmmm, maybe that is why AWS hasn‚Äôt gotten around to changing it to the default when creating EC2 instances.\n\nWhen you are selecting an instance type, you should check out the ec2-instance-selector and my blog post on Navigating EC2 Pricing - The Better Way\n\n\n\n\n\n\nNote\n\n\n\ngp3 does now appear to be the default of Amazon Linux AMIs, however gp2 is still the default for Ubuntus AMIs."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#installing-aws-cli",
    "href": "posts/ec2-tips-and-tricks/index.html#installing-aws-cli",
    "title": "EC2 Village",
    "section": "Installing AWS CLI",
    "text": "Installing AWS CLI\nAWS CLI is a must to enable you to transfer files and results between S3 and your EC2 instance\nFirst, you will need to install unzip\nsudo apt-get install unzip\nNow you can download and install the AWS CLI\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nThis should automatically appear in your path, and you can call it with aws\nYou will need to configure it with your credentials to grant the necessary permissions üîë\naws configure\nAlternatively, a more secure option is to pass the relevant S3 permissions to your EC2 instance using an IAM instance profile. Details on setting up an IAM role with S3 permissions can be found here.\nYou can add the IAM profile when first creating your instance under ‚ÄòAdvanced details‚Äô\n\nor to an existing instance by selecting the instance, clicking the ‚ÄòActions‚Äô drop-down menu and then ‚ÄòModify IAM role‚Äô under ‚ÄòSecurity‚Äô and then choosing the relevant IAM role. Then press ‚ÄòUpdate IAM role‚Äô."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#mount-a-secondary-volume-such-as-a-nitro-nvme-ssd",
    "href": "posts/ec2-tips-and-tricks/index.html#mount-a-secondary-volume-such-as-a-nitro-nvme-ssd",
    "title": "EC2 Village",
    "section": "Mount a secondary volume such as a Nitro NVME SSD",
    "text": "Mount a secondary volume such as a Nitro NVME SSD\nIf you launch a storage-optimised EC2 instance such as one from the I4i family to take advantage of the super-fast local storage performance you might get a rude shock at first, where is the SSD üòï? All you initially have access to is the EBS boot volume‚Ä¶ Nothing to fear; it is there, it just hasn‚Äôt been mounted yet.\nuse the lsblk command to view your available disk devices and their mount points.\nNAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0          7:0    0  24.4M  1 loop /snap/amazon-ssm-agent/6312\nloop1          7:1    0  55.6M  1 loop /snap/core18/2745\nloop2          7:2    0  63.3M  1 loop /snap/core20/1879\nloop3          7:3    0 111.9M  1 loop /snap/lxd/24322\nloop4          7:4    0  53.2M  1 loop /snap/snapd/19122\nnvme1n1      259:0    0   1.7T  0 disk \nnvme0n1      259:1    0     8G  0 disk \n‚îú‚îÄnvme0n1p1  259:2    0   7.9G  0 part /\n‚îú‚îÄnvme0n1p14 259:3    0     4M  0 part \n‚îî‚îÄnvme0n1p15 259:4    0   106M  0 part /boot/efi\nIdentify the name of the missing drive you want to mount, for example, nvme1n1\nNow check whether there is a file system already on the device\nsudo file -s /dev/&lt;DRIVE-NAME&gt;\nIf this just shows data as the screen output there is no file system currently on the device. However, if it shows other information for example\n[ubuntu ~]$ sudo file -s /dev/nvme1n1\n/dev/nvme1n1: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)\nIt means there is already a file system.\nAssuming there is no current file system (likely scenario for new instances) you need to make one with the mkfs -t command before you can mount it.\nsudo mkfs -t xfs /dev/&lt;DRIVE-NAME&gt;\n\n\n\n\n\n\nTip\n\n\n\nIf you get a mkfs.xfs is not found error. You just need to install the XFS tools first using:\nsudo apt-get install xfsprog\n\n\nOnce the filesystem is established, all you need to do is mount it and point it at a directory.\nYou can point it at any directory, but in this example, we are going to create a directory called data\nmkdir data\nsudo mount /dev/&lt;DRIVE-NAME&gt; /home/ubuntu/data\nTo check this has all worked correctly if you run lsblk again you should see your drive of interest show up with the associated mountpoint directory.\nNAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0          7:0    0  24.4M  1 loop /snap/amazon-ssm-agent/6312\nloop1          7:1    0  55.6M  1 loop /snap/core18/2745\nloop2          7:2    0  63.3M  1 loop /snap/core20/1879\nloop3          7:3    0 111.9M  1 loop /snap/lxd/24322\nloop4          7:4    0  53.2M  1 loop /snap/snapd/19122\nnvme1n1      259:0    0   1.7T  0 disk /home/ubuntu/data\nnvme0n1      259:1    0     8G  0 disk \n‚îú‚îÄnvme0n1p1  259:2    0   7.9G  0 part /\n‚îú‚îÄnvme0n1p14 259:3    0     4M  0 part \n‚îî‚îÄnvme0n1p15 259:4    0   106M  0 part /boot/efi\nIt is likely you will need to update the ownership permission of the mounted directory as it would have been mounted as root\nTo change the permissions to reflect your current user\nsudo chown -R $USER /home/ubuntu/data\nFor more details visit the aws help page\n\n\n\n\n\n\nCaution\n\n\n\nnvme drives on EC2 instances are typically ephemeral disks, which means they physically exist on the actual host instance and therefore they do not persist after reboots or shutdown (as the new physical host will change). All data will be lost and you will need to remount after a restart."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#resource-monitoring",
    "href": "posts/ec2-tips-and-tricks/index.html#resource-monitoring",
    "title": "EC2 Village",
    "section": "Resource Monitoring",
    "text": "Resource Monitoring\n\natop\natop is a Linux tool that provides real-time monitoring and detailed insights into system resource usage, displaying data on CPU, memory, disk, and network activities. It offers a comprehensive overview of processes, their resource consumption, and system-level performance metrics.\nHowever, more importantly, it logs resource usage so you can go back and review CPU and/or memory spikes to determine what is causing errors, or commands to terminate early and whether a different instance size is necessary.\ninstalling atop:\nsudo apt-get update\nsudo apt-get -y install atop sysstat\nConfigure atop reporting frequency and what statistics it captures: atop by default logs every 600 seconds (1 min), lets increase that to 60 second to help pick up on memory spikes that might be killing tasks.\nsudo sed -i 's/^LOGINTERVAL=600.*/LOGINTERVAL=60/' /usr/share/atop/atop.daily\nsudo sed -i -e 's|5-55/10|*/1|' -e 's|every 10 minutes|every 1 minute|' -e 's|debian-sa1|debian-sa1 -S XALL|g' /etc/cron.d/sysstat\nsudo bash -c \"echo 'SA1_OPTIONS=\\\"-S XALL\\\"' &gt;&gt; /etc/default/sysstat\"\nActivate and restart the services:\nsudo systemctl enable atop.service crond.service sysstat.service\n\nsudo systemctl restart atop.service crond.service sysstat.service\natop creates log files in /var/log/atop saves with the format atop_ccyymmdd\nTo view historical log results, run the command:\natop -r logfile\nWhere logfile is the path and name of the log file of the relevant day. Once viewing the log results, press tt to move forwards to the next resource usage snapshot and ShiftShift + tt to move back."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#installing-the-latest-stable-r-on-ubuntu",
    "href": "posts/ec2-tips-and-tricks/index.html#installing-the-latest-stable-r-on-ubuntu",
    "title": "EC2 Village",
    "section": "Installing the latest stable R on Ubuntu",
    "text": "Installing the latest stable R on Ubuntu\n\nStep 1: Obtain the R Project GPG Key\nTo start, you need to acquire the R Project‚Äôs public GPG key. This key will ensure the authenticity and security of the software you‚Äôre about to install. Open your terminal and use the following command:\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo gpg --dearmor -o /usr/share/keyrings/r-project.gpg \n\n\nStep 2: Add the R Source List to APT\nWith the GPG key in place, it‚Äôs time to add the R source list to APT. This list is essential for Ubuntu to recognise the repository where R is located. Run the following command:\necho \"deb [signed-by=/usr/share/keyrings/r-project.gpg] https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/\" | sudo tee -a /etc/apt/sources.list.d/r-project.list \n\n\nStep 3: Update Your Package List\nTo make Ubuntu aware of the new source you‚Äôve added, update your package list with the following command:\nsudo apt update \n\n\nStep 4: Install R with APT\nNow, you‚Äôre ready to install R on your system. Execute this command:\nsudo apt install --no-install-recommends r-base \n\n\n\n\n\n\nTip\n\n\n\nIf prompted to confirm the installation, simply press ‚Äòy‚Äô and then ‚ÄòEnter‚Äô to proceed.\n\n\nThat‚Äôs it! You‚Äôve successfully installed the latest stable version of R on your Ubuntu machine."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#install-relevant-r-packages",
    "href": "posts/ec2-tips-and-tricks/index.html#install-relevant-r-packages",
    "title": "EC2 Village",
    "section": "Install relevant R packages",
    "text": "Install relevant R packages\nInstalling packages in R on Windows is a relatively straightforward process, typically accomplished using the `install.packages()` function. However, the process can be a bit more involved on Linux, as you‚Äôll often need to manually install external system dependencies. In this article, I‚Äôll provide you with essential details for common packages, making the installation process smoother.\nIn many cases, you‚Äôll find that the ‚Äòmake‚Äô command is a fundamental requirement. While you can install ‚Äòmake‚Äô as a standalone package, it‚Äôs usually more beneficial to install a comprehensive set of build tools through the ‚Äòbuild-essential‚Äô package.\nTo install ‚Äòbuild-essential‚Äô on a Debian-based Linux system like Ubuntu, run the following command in your terminal:\nsudo apt-get install build-essential\nThe remotes R package provides a simple method to find out what system requirements certain R packages have on linux. Below is an example that lists out the requirements of the tidyverse package.\ninstall.packages(\"remotes\") \nremotes::system_requirements(os = \"ubuntu\", os_release = \"20.04\", package = \"tidyverse\")\n\nTidyverse\nthe tidyverse is a large collection of packages, and therefore, naturally, there is a reasonable list of dependencies. The following should install all the required libraries.\nsudo apt install libssl-dev libcurl4-openssl-dev unixodbc-dev libxml2-dev libmariadb-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev zlib1g-dev libxml2-dev libfontconfig1-dev libfribidi-dev libharfbuzz-dev libicu-dev\nTo install tidyverse via CRAN:\ninstall.packages(\"tidyverse\")\nWhile using remotes to determine dependencies works most of the time there are still cases where the dependencies are unknown until your try and install it. In these scenarios you will need to resort back to the painstaking back and forth install deficiencies, retry R package installation, install next required dependency and so on. BGLR is one such example üëá\n\n\nBLGR\nThe genomic selection package BLGR has a few additional dependencies.\nsudo apt-get install libblas-dev liblapack-dev gfortran\nTo install BGLR via CRAN:\ninstall.packages(\"BGLR\")\n\n\nPosit Package Manager\nShoutout to Josiah for this tip.\nIf you frequently find yourself installing numerous or large packages, it‚Äôs worth exploring the Posit package manager. Among its various features, one stands out‚Äî the ability to serve precompiled binary packages tailored for different system environments. This feature proves particularly beneficial when dealing with sizable R packages like dplyr that traditionally require compilation.\nTo integrate Posit into your workflow, visit the official website at https://packagemanager.posit.co/client/#/repos/cran/setup. Provide your system information, and under ‚ÄòEnvironment,‚Äô choose ‚Äòsome other R outside RStudio IDE.‚Äô Now, copy the command listed under ‚ÄòSetup Instructions‚Äô and paste it into your R session. It should resemble something similar to the following:\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"))\nBy executing this command, you redirect the install.packages() function to retrieve binaries from the Posit package manager, potentially saving valuable time during installations.\n\n\nWorking with S3 objects within R\nIf you search for a R packages for working with AWS S3 files within R, you will likely come across aws.s3. It is a great package and makes the process of dealing with S3 file within R super simple. While aws.s3 is great, it doesn‚Äôt appear to be maintained anymore and there have been no updates since May 2020. If you are storing your credentials within your EC2 instance with aws configure (which I don‚Äôt recommed) then aws.s3 seems to still work great, and many people still use it. However, if you provide your S3 credentials via an IAM profile it appears to be problematic, and the aws.ec2metadata update trick no longer works (mileage for others may vary).\nFor this reason I suggest you check out paws which is still in active development. It should automatically detect your credentials and there is a code example for downloading and uploading S3 files.\nI plan to add an example snippet in time of reading a file directly from S3 into an R object using paws as the process is not as straight forwards as some would expect."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#installing-bioinformatic-software",
    "href": "posts/ec2-tips-and-tricks/index.html#installing-bioinformatic-software",
    "title": "EC2 Village",
    "section": "Installing bioinformatic software",
    "text": "Installing bioinformatic software\n\nPLINK2\nInstalling plink2 is as simple get grabbing the Linux-64bit Intel zipped binary from the website and unpacking it. Then just move the executable to your bin directory.\nFor example:\nwget https://s3.amazonaws.com/plink2-assets/alpha5/plink2_linux_x86_64_20231011.zip\nunzip plink2_linux_x86_64_20231011.zip\nsudo mv plink2 /bin/\n\n\nVCFtools\nGrab the tarball from the GitHub release page and unpack it.\nwget https://github.com/vcftools/vcftools/releases/download/v0.1.16/vcftools-0.1.16.tar.gz\ntar -xvf vcftools-0.1.16.tar.gz\nThe PERL5LIB environment variable needs to be set to include the vcf.pm module.\nexport PERL5LIB=/path/to/your/vcftools-directory/src/perl/\nBefore you can compile the program you need to install the following dependency\nsudo apt install build-essential\nsudo apt install pkg-config\nsudo apt install zlib1g-dev\nYou can now compile the program.\ncd vcftools-0.1.16/\n./configure\nmake\nsudo make install\nVCFtools will be automatically added to your user path so you should now be able to call it with vcftools\n\n\nBCFtools\nA similar process to VCFtools, grab the BCFtools tarball from the latest releases page and run configure, make and install."
  },
  {
    "objectID": "posts/ec2-tips-and-tricks/index.html#whats-next",
    "href": "posts/ec2-tips-and-tricks/index.html#whats-next",
    "title": "EC2 Village",
    "section": "What‚Äôs next",
    "text": "What‚Äôs next\nmore to come‚Ä¶"
  },
  {
    "objectID": "posts/aws-endpoints/index.html",
    "href": "posts/aws-endpoints/index.html",
    "title": "Endpoints for AWS VPC",
    "section": "",
    "text": "Photo¬†by¬†Varun¬†Yadav¬†on¬†Unsplash\n\n\nIf you are using AWS Batch and don‚Äôt have endpoints set up for your VPC, you need to do it right now‚ùó √ç‚Äôm sure AWS have their reasons, but for any bioinformatic VPC, these should be set up by default.\nWithout endpoints, AWS batch jobs in your private VPC requiring access to S3 storage (yes, even your own S3 buckets) or ECR for your docker containers will actually have to go via your NAT gateway and the internet. Not only does this reduce security, but AWS charges exuberant NAT gateway processing fees per GB of data that passes through üí∏ With thousands of jobs and large genomic üß¨ datasets and docker images, you will find these fees (listed as EC2 other) will cost you more than the actual EC2 instances.\n\n\n\nS3 and ECR access without endpoints.\n\n\nAdditional important benefits are reduced intermittent errors and faster run times. Previously with all communication running through your NAT gateway it would easily become overwhelmed and spin out errors if numerous jobs were transferring large volumes of data (which is the norm in bioinformatics). In turn, this processing would slow your job run times.\nEndpoints provide a way for your network traffic to remain ‚Äòlocal‚Äô within amazon‚Äôs networks and avoid any NAT gateway fees, and their setup is easy!\n\n\n\nS3 and ECR access with endpoints.\n\n\nFor communication with S3 you can use a AWS gateway endpoint:\nUnder the AWS services menu, go to the VPC console, select Endpoints and click Create endpoint.\n\n\nEnter a relevant name and leave AWS services selected as default.\nIn the Services search field, enter ‚ÄòS3‚Äô and press ‚Äòenter‚Äô.\nSelect the service name com.amazonaws.[REGIOIN].s3 that has ‚ÄòGateway‚Äô as the ‚ÄòType‚Äô field.\n\nChoose your VPC.\nUnder Route tables, choose the ID that matches your Private subnet (‚ÑπÔ∏è tip: scroll across)\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nAny S3 requests from your private VPC batch jobs will now be processed through the gateway endpoint.\nAlthough your ECR containers are stored in the S3 system, you need to set up ECR and ECS endpoints for the orchestration communication required to setup these containers in your batch jobs.\nSetting up an endpoint for ECR & ECS traffic requires a few more steps:\nAgain under your VPC console, select endpoints and click Create endpoint. Following similar steps as above.\n\nSearch in the Services search bar for ‚ÄòECR‚Äô and select com.amazonaws.[REGION].ecr.api\n\nThe type is now ‚ÄòInterface‚Äô.\nChoose your VPC.\nSelect your private subnet.\nSelect either your default security group or one that has your required restrictions.\nLeave the Policy as full access unless your want to make custom restrictions.\nClick Create endpoint\n\nNow repeat this process three more times for\n\ncom.amazonaws.[REGION].ecr.dkr\ncom.amazonaws.[REGION].ecs-agent\ncom.amazonaws.[REGION].ecs-telemetry\n\nYou are now all setup üéâ If all is setup correctly you should now notice a big drop ‚ÜòÔ∏è in your fees, less ‚ÜòÔ∏è errors and faster üèéÔ∏è job run times.\nThis should be the default‚Ä¶ ü§∑"
  },
  {
    "objectID": "posts/aws-cli-s3/index.html",
    "href": "posts/aws-cli-s3/index.html",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "",
    "text": "Photo¬†by¬†Jesse¬†Vermeulen¬†on¬†Unsplash\nAmazon Web Services Command Line Interface (AWS CLI) is your trusty companion for seamlessly interacting with AWS services directly from your terminal. It empowers you to manage and automate various AWS resources and operations effortlessly. In this blog post, we‚Äôll explore two essential aspects of AWS CLI: moving files in and out of AWS S3 buckets  using the aws s3 cp and aws s3 sync commands."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#aws-cli-s3-sync-your-data-synchronisation-solution",
    "href": "posts/aws-cli-s3/index.html#aws-cli-s3-sync-your-data-synchronisation-solution",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "AWS CLI S3 Sync: Your Data Synchronisation Solution",
    "text": "AWS CLI S3 Sync: Your Data Synchronisation Solution\n\nThe aws s3 sync Command \naws s3 sync is a command that should be in the toolkit of anyone dealing with AWS S3. This command enables you to synchronise files and directories between a source and a destination, which can be an S3 bucket, a local drive, or even between S3 buckets. Here‚Äôs the basic syntax:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\naws s3 sync will only copy new and updated files by checking the destination folder first. There is also the ability to delete files in the destination that are no longer present in the source by applying the --delete flag.\nIn addition to the ability to synchronise files and directories, aws s3 sync has the ability to handle specific file filtering. You can precisely control which files to include or exclude using two essential parameters:\n\n‚Äìexclude : Exclude files matching the specified pattern.\n‚Äìinclude : Include files matching the specified pattern.\n\nBy default, all files are included, so when you need to sync only certain files, like .png images, you must first exclude all files and then include only those with the .png extension:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\\\n  --exclude '*'\\\n  --include '*.png'\nBe mindful of the order of these filters; they are applied from left to right. For instance, the following command may seem similar but produces a different outcome:\naws s3 sync &lt;SOURCE&gt; &lt;DESTINATION&gt;\\\n  --include `*.png`\\\n  --exclude '\\*'\nHere, all .png files are initially included, but then all files are excluded, resulting in no transfers.\n\nTo preview the actions of an AWS CLI command without executing it, add the ‚Äìdryrun flag. It‚Äôs a handy way to ensure your command behaves as expected."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#aws-cli-s3-copy-efficient-file-transfers",
    "href": "posts/aws-cli-s3/index.html#aws-cli-s3-copy-efficient-file-transfers",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "AWS CLI S3 Copy: Efficient File Transfers ",
    "text": "AWS CLI S3 Copy: Efficient File Transfers \n\nThe aws s3 cp Command\nWhile aws s3 sync focuses on synchronisation and can be used to update files at the destination, aws s3 cp is primarily designed for simple and efficient file transfers. It copies files and directories from a source to a destination, which can be local, remote, or even between S3 buckets. The basic syntax is as follows:\naws s3 cp &lt;SOURCE&gt; &lt;DESTINATION&gt;\nUnlike aws s3 sync, aws s3 cp doesn‚Äôt handle recursive synchronization. It‚Äôs a straightforward file copy operation, regardless of whether the file is already present in the destination. Here‚Äôs an example:\naws s3 cp my-local-file.txt s3://my-destination-bucket/\nIn this case, my-local-file.txt is copied to the specified S3 bucket. aws s3 cp excels in scenarios where you need to upload or download individual files without worrying about folder structures.\n\n\n\n\n\n\nTip\n\n\n\nSimilar to aws s3 sync you can use the --include and --exclude parameters to filter, just make sure you also add the --recursive flag or else it wont work."
  },
  {
    "objectID": "posts/aws-cli-s3/index.html#differentiating-aws-s3-cp-and-aws-s3-sync",
    "href": "posts/aws-cli-s3/index.html#differentiating-aws-s3-cp-and-aws-s3-sync",
    "title": "Streamlining Your AWS CLI S3 File Operations",
    "section": "Differentiating aws s3 cp and aws s3 sync",
    "text": "Differentiating aws s3 cp and aws s3 sync\n\nSynchronisation vs.¬†Copy: aws s3 sync is designed for synchronising files between a source and a destination, ensuring that both locations have the same set of files. aws s3 cp, on the other hand, focuses on copying individual files or directories from a source to a destination.\nComplexity: aws s3 sync offers advanced features like file filtering and recursive synchronisation, making it suitable for managing complex data synchronization tasks. aws s3 cp is simpler and more suitable for straightforward efficient file transfers.\n\nIn conclusion, AWS CLI‚Äôs aws s3 cp and aws s3 sync commands cater to different aspects of managing files in AWS S3. By understanding their strengths and use cases, you can efficiently handle a wide range of file-related tasks in your AWS environment."
  },
  {
    "objectID": "posts/annotate-equations/index.html",
    "href": "posts/annotate-equations/index.html",
    "title": "Annotate equations",
    "section": "",
    "text": "üëç Featured as a highlight ‚ú® in R Weekly 2023-W37 and the R Weekly Podcast üéß"
  },
  {
    "objectID": "posts/annotate-equations/index.html#annotating-equations",
    "href": "posts/annotate-equations/index.html#annotating-equations",
    "title": "Annotate equations",
    "section": "Annotating Equations",
    "text": "Annotating Equations\nUsing the LaTeX annotate-equations package in Quarto allows you to easily annotate equations within your quarto reports, documents or manuscripts. Currently as far as I am aware this only works for pdfs.\n\nStep-by-step example\nHere‚Äôs a straightforward step-by-step guide to annotating equations using the annotate-equations package. We‚Äôll use the genomic selection GEBV equation as the example.\n\nQuarto Setup\nQuarto itself will attempt to automatically download and install the annotate-equations package into its LaTeX engine, all you need to do is tell Quarto that you need it by adding the following to your yaml at the top of your .qmd file\nheader-includes:\n  - \\usepackage{annotate-equations}\n\n\nBegin with Your LaTeX Equation\nStart by writing the equation you want to annotate in LaTeX format:\n\\begin{equation*}\n  GEBV = \\sum_{i}^{n} X_{i} b_{i}\n\\end{equation*}\n\n\n\n\nSplit the Equation into Nodes\nBreak down the equation into individual nodes. To highlight a node, enclose it in \\eqnmarkbox[color]{node name}{equation term(s)}. If you only want to color the node‚Äôs text, use \\eqnmark[color]{node name}{equation term(s)}.\nFor example:\n\\begin{equation*}\n  \\eqnmark[purple]{node1}{GEBV}\n  \\tikzmarknode{node2}{=}\n  \\eqnmark[black]{node3}{\\sum_{i}^{n}}\n  \\eqnmarkbox[blue]{node4}{X_{i}}\n  \\eqnmarkbox[red]{node5}{b_{i}}\n\\end{equation*} \n\n\n\n\nAdd Annotations\nNow, introduce annotations to each node using the \\annotate[tikzoptions]{annotate keys}{node name[,‚Ä¶]}{annotation text} command:\n\nFor {annotate keys}, choose whether the annotation appears above or below, to the right or left of the node.\nFor [tikzoptions], include a yshift value to adjust the annotation‚Äôs position above or below (use negative values for below). You may need to fine-tune these values, especially in complex equations. xshift can be defined if necessary.\n\nFor example:\n\\begin{equation*}\n  \\eqnmark[purple]{node1}{GEBV}\n  \\tikzmarknode{node2}{=}\n  \\eqnmark[black]{node3}{\\sum_{i}^{n}}\n  \\eqnmarkbox[blue]{node4}{X_{i}}\n  \\eqnmarkbox[red]{node5}{b_{i}}\n\\end{equation*}\n\\annotate[yshift=1em]{left}{node3}{sum across n loci} \n\\annotate[yshift=-1em]{below,left}{node1}{Genomic Estimated Breeding Value} \n\\annotate[yshift=-1em]{below,right}{node5}{effect at locus i} \n\\annotate[yshift=-2.5em]{below,right}{node4}{genotype at locus i}\n\n\n\n\n\n\n\nOther example\n\nAnnotate to multiple nodes\nFor some equations a variable will appear multiple times. In these scenarios you may want your annotation to link both occurrences. To archive this you just use the \\annotatetwo definition and add both node names.\nAn example using the Hardy-Weinberg equation:\n\n\\begin{equation*}\n  \\eqnmarkbox[green]{node1}{p}\n  \\tikzmarknode{node2}{^{2}}\n  \\tikzmarknode{node3}{+}\n  \\tikzmarknode{node4}{2}\n  \\eqnmarkbox[green]{node5}{p}\n  \\eqnmarkbox[blue]{node6}{q}\n  \\tikzmarknode{node7}{+}\n  \\eqnmarkbox[blue]{node8}{q}\n  \\tikzmarknode{node9}{^{2}}\n  \\tikzmarknode{node10}{=}\n  \\tikzmarknode{node11}{1}\n\\end{equation*}\n\\annotatetwo[yshift=1.5em]{above, label above}{node1}{node5}{frequency of allele A} \n\\annotatetwo[yshift=-1.5em]{below, label below}{node6}{node8}{frequency of allele B} \n\n\n\n\n\nAdditional Information\nIf you want to see how this looks in a pdf and/or see the LaTeX code inside the correpsonding quarto .qmd file visit my github repo.\nFor more in-depth instructions and possibilities with annotate-equations in LaTeX, refer to the documentation at https://github.com/st‚Äì/annotate-equations/blob/main/annotate-equations.pdf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr Luke Pembleton",
    "section": "",
    "text": "Education\n\nüéì Doctor of Philosophy, Molecular Genetics from La Trobe University, 2014\nüéì Bachelor of Agricultural Science, Rural Technology from the University of Queensland, 2010\n\nAbout Me\n\nI am a Genomic Breeding Scientist and Strategic Science Manager at Barenbrug where I lead the development and implementation of commercial genomic breeding and associated ‚Äòomic‚Äô technologies.\n\nInterest and Skills\n\nMost of my technical work is focused on the fields of üß¨ Genomics, Genomic Selection and Genomic Breeding in plants, primarily forages üå±. Skilled in bioinformatics üë®‚Äçüíª and passionate about the R programing language. Enjoy developing cloud computing ‚òÅÔ∏è environments for research and production pipelines, often orchestrated with the workflow manager Nextflow üîÇ.\n\nExperience\n\n Genomic Breeding Scientist | Global Strategic Science Manager\nBarenbrug | August 2021 - Current\n~\n Senior Research Scientist\nAgriculture Victoria Research | April 2017 - August 2021\n~\n Research Scientist\nAgriculture Victoria Research | October 2014 - March 2017\n~\n Research Assistant\nVictorian Department of Primary Industries | June 2009 - December 2009"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Packing for AWS Batch EC2\n\n\n\n\n\n\nBatch\n\n\nEC2\n\n\nAWS\n\n\nNextflow\n\n\n\nAn guide for selecting resource allocations on AWS Batch EC2\n\n\n\n\n\nJan 23, 2024\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nEC2 Village\n\n\n\n\n\n\nEC2\n\n\nAWS\n\n\nBioinformatics\n\n\nS3\n\n\n\nA collection of mini guides, tips and tricks to using EC2 instances focused on bioinformatics\n\n\n\n\n\nDec 14, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating EC2 Pricing - The Better Way\n\n\n\n\n\n\nEC2\n\n\nAWS\n\n\nBioinformatics\n\n\n\nAn interative visual analysis of EC2 instance prices for better informed selection\n\n\n\n\n\nNov 18, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nStreamlining Your AWS CLI S3 File Operations\n\n\n\n\n\n\nAWS\n\n\nS3\n\n\n\nA guide to using the AWS Command Line Interface to interact with S3\n\n\n\n\n\nSep 25, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nPushing Docker Images to AWS ECR\n\n\n\n\n\n\nDocker\n\n\nAWS\n\n\nECR\n\n\n\nA guide to transfer your docker images to your private AWS container repository\n\n\n\n\n\nSep 24, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotate equations\n\n\n\n\n\n\nequations\n\n\nquarto\n\n\nannotate\n\n\nLaTeX\n\n\n\nA guide to annotating equations in quarto documents\n\n\n\n\n\nSep 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Cast Blue Iris UI3\n\n\n\n\n\n\nSmarthome\n\n\nBlue Iris\n\n\nSecurity\n\n\nChromecast\n\n\n\nA guide to casting Blue Iris UI3 to a Chromecast device and monitor\n\n\n\n\n\nAug 28, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Deploy Keys\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nssh key\n\n\nAWS\n\n\nremote\n\n\ncloud\n\n\n\nA guide to using GitHub deploy keys to deploy private repos in remote compute sessions\n\n\n\n\n\nJul 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nScreen Utility and Terminal Multiplexing\n\n\n\n\n\n\nScreen\n\n\nAWS\n\n\nTerminal\n\n\n\nA guide to using the screen command in remote sessions\n\n\n\n\n\nMay 23, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nEndpoints for AWS VPC\n\n\n\n\n\n\nAWS\n\n\nEndpoints\n\n\nGateway\n\n\nBatch\n\n\n\nA guide to setting up endpoints for your AWS VPC\n\n\n\n\n\nMay 18, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nEBS Auto-scaling on AWS Batch\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\nEBS\n\n\n\nAn introductory guide to setting up EBS auto-scaling on AWS Batch for use in Nextflow\n\n\n\n\n\nFeb 3, 2023\n\n\nLW Pembleton\n\n\n\n\n\n\n\n\n\n\n\n\nNextflow on AWS Batch\n\n\n\n\n\n\nNextflow\n\n\nAWS\n\n\nBatch\n\n\n\nAn introductory guide to setting up Nextflow with AWS Batch\n\n\n\n\n\nNov 25, 2022\n\n\nLW Pembleton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/conferences.html",
    "href": "pages/conferences.html",
    "title": "Virtual Conferences & Meetups",
    "section": "",
    "text": "Photo¬†by¬†Sigmund¬†on¬†Unsplash\n\n\nA list of virtual üßë‚Äçüíª (or hybrid) conferences and meetups for the data science üìä life science üß™üß¨ and agricultural science community üåæ\n\nReoccurring üîÅ\nüìåPosit Data Science Hangout - Live Every Thursday at 12 PM ET.\nEach week, host Rachael Dempsey invites an accomplished data science leader to talk about their experience and answer questions from the audience. The discussion focuses mainly on the human elements of data science leadership. There‚Äôs no sales or marketing fluff, just great insights from inspiring professionals.\nüìπ Past hangouts are also available to watch.\n\nüìåNew York Open Statistical Programming Meetup - Every Month.\nOriginally starting as the New York R Meetup it has now grown to incorporate knowledge of statistical programming techniques in open-source languages such as R, Python, Julia and Go, and data science in general.\nüìπ Past meetups are also available to watch.\nFollow on Twitter\nüìåChannels the nextflow podcast - Pre-recorded every fortnight.\nTalking about news in the Nextflow ecosytem and speaking with pioneers in the field.\n\n\n\nScheduled üìÜ\n\n2024\nüìåShinyConf2024 - April 17-19, 2024\nThe third edition of the largest virtual R Shiny conference.\nüìåSatRdays London - April 27, 2024\nSatRdays events are for everyone with an interest in R, whether you‚Äôre a seasoned pro or just getting started.\nRegister\nüìånf-core Hackathon - March 18-21 , 2024\nA virtual hackathon with local sites to develop nf-core together\nRegistration opens Feburary 2, 2024 | Follow on Twitter | Follow on Mastodon\nüìåNextflow SUMMIT | Boston - May 14-17, 2024\nA showcase of the latest developments and innovations from the Nextflow world held in Boston and typically streamed online.\nSubscribe for updates | Follow on Twitter\nüìåNYR10 New York R Conference - May 16-17, 2024\nThe 10th anniversary of the New York R Conference. Get ready for two days packed with cutting-edge talks, workshops (15th May) & a celebration of all things R and friends!\nMore details to come | Follow on Twitter\nüìåuseR! 2024 - July 8-11, 2024\nuseR! is the main meeting of the R user and developer community, its program consists of both invited and user-contributed presentations.\nRegister | Follow on Twitter | Follow on Mastodon\nüìåBioC2024: Bioconductor Conference July 24-26, 2024.\nWhere software and biology connect. Bioconductor conference highlights current developments within and beyond the Bioconductor project.\nWebsite üëà note link is currently broken | Mastodon\nüìåPosit::conf(2024) | Seattle - August 12-14, 2024.\nTwo days of workshops, two days of conference keynotes and talks, and endless opportunities to connect our data science community. Experience it in person or virtually.\nüìåEuroBioC2024: European Bioconductor Conference September 4-6, 2024.\nWhere software and biology connect. Bioconductor conference highlights current developments within and beyond the Bioconductor project.\nWebsite | Mastodon\nüìåNextflow SUMMIT | Barcelona - October 21-25, 2024\nA showcase of the latest developments and innovations from the Nextflow world held in Barcelona and typically streamed online.\nSubscribe for updates | Follow on Twitter\n\n\n\n\nPast Events üóÑÔ∏è\n‚úÖShinyConf2023 - 15-17 March, 2023\nShinyConf is a virtual event hosted by Appsilon with support from Posit. This year‚Äôs Shiny conference Showcases recent advancements in R Shiny technology such as open source packages, Shiny interoperability, and commercial applications\nConference recordings\n\n‚úÖNew York R Conference - July 11-12, 2023.\nThe R Conference hosts one of the most elite gatherings of data scientists and data professionals who come together to explore, share, and inspire ideas, and to promote the growth of open source ideals.\nConference recordings.\n\n\n‚úÖBioconductor Conference, August 2-4, 2023.\nBioconductor conference highlights current developments within and beyond the Bioconductor project.\nConference recordings.\n‚úÖPosit::conf(2023) - September 17-20, 2023.\nTwo days of workshops, two days of conference keynotes and talks, and endless opportunities to connect our data science community. Experience it in person or virtually.\nConference recordings.\n‚úÖdockercon 2023 - October 4-5, 2023.\nDockerCon returns in-person in Los Angeles and online from anywhere in the world this October. Meet and learn from thousands of developers and industry leaders to power up your technical knowledge.\nConference recordings.\n\n‚úÖNextflow SUMMIT - October 16-20, 2023 | November 28-30, 2023\nA showcase of the latest developments and innovations from the Nextflow world held in Barcelona and streamed online. Also includes a virtual nf-core Hackathon.\nConference recordings - Barcelona| Boston\n\n‚úÖGitHub Universe - November 8-9, 2023.\nThe annual developer event is back, better, and bigger than ever at San Francisco‚Äôs Yerba Buena Center for the Arts and online.\nConference recordings."
  },
  {
    "objectID": "posts/aws-batch-ebs-autoscale/index.html",
    "href": "posts/aws-batch-ebs-autoscale/index.html",
    "title": "EBS Auto-scaling on AWS Batch",
    "section": "",
    "text": "Photo¬†by¬†Simon¬†Goetz¬†on¬†Unsplash\n\n\nAnyone who has tried running bioinformatic pipelines on AWS batch with a workflow manager such as Nextflow will be well aware of the common error\nerror: No space left on device\nthat can plague a pipeline. Yes, you can adjust your EBS allocation with specific AMI images or launch configurations and tailor them to specific tasks, but the dynamic nature of bioinformaticslogy means this will likely be an ongoing cat üêà and mouse üêÅ game.\nYes, Nextflow has the fantastic resume feature if your pipeline has already completed a large proportion of tasks, unfortunately though the config file is not reanalysed upon resume, so you cannot point to a new AMI with an increased EBS volume.\nThe solution? Automatic scaling of your EBS volumes in real-time. Essentially there is a script that resides within your AMI that continously monitors disk usage, and just before you reach 100%, it provisions a new EBS volume mounting it directly to your running EC2 instance. You also get the added benefit of better EBS cost optimisation üí∞ as you no longer need to ‚Äòover provision‚Äô your batch EC2 instances.\n\nThe setup can be split into two components, installing the auto-scaling scripts in your AMI and updating your Batch compute environments with appropriate permissions.\n\nSetup an appropriate IAM Role\n\nClick Create role under the IAM AWS console and select AWS service as the trusted entity type and EC2 as the use case, then click Next.\nClick Create policy and select the JSON tab.\nPaste the following JSON code and click Next.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:AttachVolume\",\n                \"ec2:DescribeVolumeStatus\",\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeTags\",\n                \"ec2:ModifyInstanceAttribute\",\n                \"ec2:DescribeVolumeAttribute\",\n                \"ec2:CreateVolume\",\n                \"ec2:DeleteVolume\",\n                \"ec2:CreateTags\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\nAdd any tags if applicable and click Next\nGive your policy a name e.g.¬†amazon-ebs-autoscale-policy and click Create policy\nNow under the Add permission menu of your new IAM Role, select your newly created policy, i.e.¬†amazon-ebs-autoscale-policy and click Next\nGive your Role and name, e.g.¬†amazon-ebs-autoscale-role and click Create role\nYou also need to add the amazon-ebs-autoscale-policy policy role to the ecsInstanceRolerole you use in your AWS Batch compute environments.\nUnder Roles in the AWS IAM console, find and click the ecsInstanceRole role.\nClick Add permission and select Attach policies. Find/search for your new amazon-ebs-autoscale-policy, select it and click Attach policies.\n\n\n\nInstall the auto-scale scripts\n\nFetch or clone the amazon-ebs-autoscale repository to your local computer.\nEdit the EBS mount location to the volume that docker utilises by adding the -m /var/lib/docker parameter to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file\nSpecify the initial drive to use for the mount point to be /dev/xvdba with the -d parameter\nBy default, the 100GiB volume will be initially provisioned at startup to change this add the -s parameter again to the install.sh command in the amazon-ebs-autoscale/templates/cloud-init-userdata.yaml file. For example, to reduce it to 30GB use -s 30\nthe runcmd: section should now look something like this:\n\nruncmd:\n  - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\"\n  - unzip -q /tmp/awscliv2.zip -d /tmp && /tmp/aws/install\n  - EBS_AUTOSCALE_VERSION=$(curl --silent \"https://api.github.com/repos/awslabs/amazon-ebs-autoscale/releases/latest\" | jq -r .tag_name)\n  - cd /opt && git clone https://github.com/awslabs/amazon-ebs-autoscale.git\n  - cd /opt/amazon-ebs-autoscale && git checkout $EBS_AUTOSCALE_VERSION\n  - sh /opt/amazon-ebs-autoscale/install.sh -m /var/lib/docker -d /dev/xvdba -s 30 2&gt;&1 &gt; /var/log/ebs-autoscale-install.log\n\nTo install the amazon-ebs-autoscale scripts with your defined parameters into your chosen AMI you can use the aws ec2 run-instance command from the aws-cli. An example of launching your chosen AMI and installing the amazon-ebs-autoscale scripts is\n\naws ec2 run-instances --image-id YOUR-AMI-ID \\\n  --key-name YOUR-KEY-PAIR-NAME \\\n  --subnet-id YOUR-SUBNET-ID \\\n  --user-data file://./templates/cloud-init-userdata.yaml \\\n  --count 1 \\\n  --security-group-ids YOUR-SECURITY-GROUP-ID \\\n  --instance-type t2.micro \\\n  --iam-instance-profile Name=amazon-ebs-autoscale-role\n\nRunning this from your command line will launch an EC2 instance which you can then save as a new AMI with an appropriate name. (see my Nextflow on AWS Batch blog post for details on how to save AMIs and use them in later Batch compute environments)\n\nThe final step is to reconfigure your Batch compute environment to utilise the EBS autoscaling AMI\n\nClick Computer environments in the AWS Batch console and click Create\nSelect Amazon Elastic Compute Cloud (Amazon EC2) as the configuration and Managed as the orchestration type and enter an appropriate name.\nEnsure AWSServiceRoleForBatch is selected as the service role and ecsInstanceRole as the instance role and click Next page\n\n\n\n\n\n\nNote\n\n\n\nThe updated ecsInstanceRole now contains the permissions required for autoscaling.\n\n\nLeave Minimum and Desired vCPUs as 0. Maximum vCPUs controls the number of parallel vCPU tasks running in your compute environment. Increase or decrease this to an appropriate number based on your requirements.\nWith EBS autoscaling implemented, you can now generally use optimal as the allowed instance type without many of the previous risks of errors and failed pipelines.\nUnder ‚ÄòAdditional configuration‚Äô you can define the allocation strategy\nBEST_FIT (default) AWS Batch selects an instance type that best fits the needs of the jobs with a preference for the lowest-cost instance type. If additional instances of the selected instance type aren‚Äôt available, AWS Batch waits for the additional instances to be available. If there aren‚Äôt enough instances available, or if the user is reaching the Amazon EC2 service quotas, then additional jobs don‚Äôt run until currently running jobs are complete. This allocation strategy keeps costs lower but can limit scaling. If you‚Äôre using Spot Fleets with BEST_FIT, the Spot Fleet IAM Role must be specified. BEST_FIT isn‚Äôt supported when updating compute environments. For more information, see Updating compute environments.\nBEST_FIT_PROGRESSIVEAWS Batch selects additional instance types that are large enough to meet the requirements of the jobs in the queue. Instance types with a lower cost for each unit vCPU are preferred. If additional instances of the previously selected instance types aren‚Äôt available, AWS Batch selects new instance types.\nSPOT_CAPACITY_OPTIMIZEDAWS Batch selects one or more instance types that are large enough to meet the requirements of the jobs in the queue. Instance types that are less likely to be interrupted are preferred. This allocation strategy is only available for Spot Instance compute resources.\nUnder ‚ÄòEC2 configuration‚Äô click Add EC2 configuration and select Amazon Linux 2 as the image type and paste the AMI ID that you created earlier in the ‚ÄòImage ID override‚Äô box.\n\nClick Next page and enter the appropriate network configuration for your VPC\nClick Next page, check your settings and then click Create compute environment\n\nStill within the Batch AWS console and click on Job queues.\n\nClick Create and select ‚ÄòAmazon Elastic Compute Cloud (Amazon EC2)‚Äô as the compute environment.\nEnter a suitable name for your new job queue (üìù take note of this name you will need it later)\nUnder ‚ÄòConnected compute environments‚Äô select the compute environment that you just created\nClick Create job queue\n\nYou now need to update the config files of your Nextflow pipelines to point to the new Batch job queue.\nNow enjoy your Nextflow pipelines with no reduced errors."
  },
  {
    "objectID": "posts/aws-ec2-pricing/index.html",
    "href": "posts/aws-ec2-pricing/index.html",
    "title": "Navigating EC2 Pricing - The Better Way",
    "section": "",
    "text": "Photo¬†by¬†Growtika¬†on¬†Unsplash"
  },
  {
    "objectID": "posts/aws-ec2-pricing/index.html#navigating-ec2-pricing---the-better-way",
    "href": "posts/aws-ec2-pricing/index.html#navigating-ec2-pricing---the-better-way",
    "title": "Navigating EC2 Pricing - The Better Way",
    "section": "Navigating EC2 pricing - the better way",
    "text": "Navigating EC2 pricing - the better way\nWhen it comes to choosing the ideal AWS EC2 instance for your workload, navigating through the myriad of options can be a daunting task. Factors like memory, CPU, instance type, and family interplay in a complex pricing üíµ dance üï∫ that isn‚Äôt always logical (at least not on the surface), often leaving you puzzled about the most cost-effective choice üòï\nTo simplify this process for myself I went ahead and developed an interactive plot that maps out the relationship between AWS EC2 instance types, their associated memory, CPU configurations, and pricing in the AU ap-southeast-2 region. This tool allows me to make informed decisions by visualizing the cost-effectiveness of different instance types based on their specific needs.\nHow often doing bioinformatics dev work do you say I just need a couple of CPUs, but a good üí™ amount of RAM, maybe 64GB, but what if I max that out ü§î should I just go for more at the start, say 96GB? I wonder whether that will cost much extra? What even instance family do I want for this?‚Ä¶Sure go ahead trawl üîç through the endless AWS tables and lists of EC2 instance type, compare, then search for the hourly price, cross reference‚Ä¶ or just look at a plot üëá\n\n\n\n\n\n\nThe x-axis represents memory allocation while the y-axis denotes the hourly price. I chose memory or RAM to be be my x-axis as I feel it is most commonly the more important determining variable in a bioinformatic dev environment. Number of vCPUs is represented by the various colours and CPU clock speed (GHz) controls the point size. Each point correspond to a specific instance type available for EC2 on-demand usage, and full instance details are displayed when hovering over the point, revealing additional information such as network and storage configurations.\n\n\n\n\n\n\nNote\n\n\n\nTo keep the plot focused and useable I removed instances with more than 1000GB of RAM and/or more than 72 vCPUs. It is also unlikely you wouldn‚Äôt be after these for a dev environment - they are better suited to BATCH queues with multi job processing.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThese prices were valid in October 2023, however, they are likely to change overtime.\n\n\n\nIt pays to shop around\nWhile there is not surprisingly a correlation between memory and pricing, there are scenarios across the spectrum where you can get an instance with more RAM üêè at a lower price than another instance with less RAM, and the same with vCPU number, etc. It is about finding those sweet spots üç≠ that work for our needs.\nAn example of this: say you are after an instance with 16GB of RAM you might go for the standard compute optimised c5.2xlarge for $0.44/hr, orrr you could instead get a memory optimised z1d.xlarge with a higher clock speed and twice as much RAM (32GB) for pretty much the same price $0.452/hr üòäÔ∏è orrr the memory optimised r5a.xlarge also with 32GB RAM but at a much lower $0.272/hr üòÄ Yes nearly 40% cheaper than maybe your initial default choice but with twice as much RAM üòèüëç\n\n\nFuture Prospects:\nAs of now, the interactive scatter plot encompasses data solely from the AU ap-southeast-2 region. I‚Äôm open to expanding and updating this tool to include additional regions or further refine its functionalities based on the community‚Äôs interest and feedback. Some of things features I am thinking of adding are:\n\nthe ability to highlight instances with similar prices, when hovering over a point\nthe ability to search for a instance type/name\nadd jitter to prevent overlapping points - the ability to copy the instance name when hovering"
  },
  {
    "objectID": "posts/cast-bi-ui3/index.html",
    "href": "posts/cast-bi-ui3/index.html",
    "title": "How to Cast Blue Iris UI3",
    "section": "",
    "text": "When it comes to setting up a wall-mounted security display or displaying cameras discreetly in a bookcase, you may initially think incorporating it into your Home Assistant dashboard is the best option or that you need to mini PC attached to a monitor.\nHome Assistant, while an excellent smart home management platform, will struggle to handle multiple live camera streams resulting in unbearable delays or glitchy streams. The best option is actually relatively inexpensive and simple to setup. It involves using a Google TV chromecast and a computer monitor (with HDMI port) or a small TV, of which you might likely find you already have these lying around.\nIn this blog post, I will guide you through the process of casting the Blue Iris UI3 interface to both a Google TV Chromecast and a computer monitor, ensuring a high-quality display and lag-free security camera video streams. Although you can just use cast all the things to case the Blue Iris UI3 directly to your Chromecast, you will soon find that after 10min your Chromecast will enter ambient mode and you will lose connection with the display of your camera feeds. The following guide will sort all of this out for your\n\nPrerequisites.\nBefore we begin, make sure you have the following:\n\nA Google TV Chromecast connected to your monitor or TV.\nBlue Iris software installed and running on a computer.\nPython 3\n\n\n\n\n\n\n\nNote\n\n\n\nThis should work with other Chromecast devices, just note that some of the menus and steps listed below may differ.\n\n\n\n\nStep 1: Enable Developer Mode and USB Debugging on Chromecast\n\nConnect your Chromecast to the same Wi-Fi network as your computer and TV/monitor.\nOn your Chromecast device go to Settings and the System submenu.\nIn System click on about and scroll down until you find Android TV OS Build.\nPress the OK or Enter button 7 times to enable developer mode. - You should see a message saying ‚ÄòYou are now a developer.‚Äô\nNote that at about 5 presses it should start to notify that you are about the enable developer mode.\nNow, go back to the main settings menu and select Developer options.\nTurn on Enable developer options if not already\nThen turn on USB debugging.\n\n\n\nStep 2: Get the IP Address of the Chromecast\n\nOpen the Network and Internet menu in the Chromecast and the IP address should be displayed in the right-hand panel.\nOr alternatively;\nOpen the Google Home app on your smartphone or tablet.\nTap on your Chromecast device, then tap on the gear icon to access the device settings.\nThe IP address should be displayed under the Device information section.\n\n\n\nStep 3: Install Android Developer Kit (ADB)\n\nDownload the Android SDK Platform-Tools package from the official Android website.\nExtract the downloaded zip file to a location on your computer.\nOpen a command prompt or terminal window and navigate to the folder containing the extracted files.\nExecute the following command in the command prompt or terminal:\n\nadb connect &lt;YOUR_CHROMECAST_IP_ADDRESS&gt;\n\nReplace &lt;YOUR_CHROMECAST_IP_ADDRESS&gt; with the IP address you obtained earlier.\nOn your Chromecast-connected TV or monitor, you will see a pop-up notification or a prompt asking for your permission to allow the connection from the specified IP address.\nAccept the connection by selecting ‚ÄúAllow‚Äù or ‚ÄúAccept‚Äù on your Chromecast device.\n\nOnce you‚Äôve accepted the connection request, you can proceed with Step 4 and enter the command to change the screen timeout value.\n\n\nStep 4: Change the Screen Timeout Value on Chromecast\n\nEnter the following command to change the screen timeout value to one month (in seconds):\n\nadb shell settings put system screen_off_timeout 2592000000\n\n\nStep 5: Turn on ‚ÄòStay Awake‚Äô on Chromecast\n\nOn your Chromecast device still within the Developer options.\nTurn on Stay awake.\n\n\n\nStep 6: Set Up Camera Groups in Blue Iris\n\nLaunch the Blue Iris software on your computer.\nGo to the ‚ÄòCameras‚Äô tab and select the cameras you want to group together to display on the Chromecast.\nRight-click on the selected cameras and choose ‚ÄòGroup Cameras.‚Äô\nGive the camera group a name and click ‚ÄòOK.‚Äô\n\n\n\nStep 7: Setup Cast All The Things (CATT)\n\nOpen a command prompt or terminal window on your computer.\nInstall CATT using pipx\npipx install catt\n\n\n\n\n\n\nNote\n\n\n\nYou can using pip, however pipx is recommended by the developer of CATT\n\n\nNext you need to identify your chromecast device. To scan you local network run the following\n\ncatt scan\n\nTake note of the relevant chromecase device name. You will need it in the next step.\n\n\n\nStep 8: Cast Blue Iris UI3 to Chromecast\n\nOpen a web browser and navigate to http://&lt;BLUEIRIS-IP-ADDRESS&gt;:81/ui3.htm? to check the Blue Iris UI3 interface is functioning.\nTo cast the UI3 web interface to your chromecast and monitor enter the following command\n\ncatt cast_url http://\\&lt;BLUEIRIS-IP-ADDRESS\\&gt;:81/ui3.htm?timeout=0\n\nReplace  with the IP of your Blue Iris server.\n\n\n\n\n\n\nNote\n\n\n\nNote we have added timeout=0 to the url to prevent the web interface timing out\n\n\nTo stop casting or if there is a disconnection and you need to recast, you first need to run the stop command\n\ncatt -d &lt;name_of_chromecast&gt; stop\n\nTo cast only the camera group you created add the group=groupname parameter to the url\n\ncatt cast_url http://\\&lt;BLUEIRIS-IP-ADDRESS\\&gt;:81/ui3.htm?timeout=0&group=groupname\n\n\n\n\n\n\nNote\n\n\n\nI would also recommended adding the clipview=confirmed parameter if you have AI detection on. This will show thumbnails of confirmed events.\n\n\n\nFurther url parameters can been found in the UI3 documentation.\n\n\n\nConclusion\nBy following these steps, you can easily cast the Blue Iris UI3 interface to your Google TV Chromecast and attached screen. The use of CATT ensures a smooth and high-quality display, while the screen timeout changes and ‚Äòstay awake‚Äô settings on the Chromecast prevent disruptions during extended use. Enjoy easy access to your Blue Iris cameras on the big screen!"
  },
  {
    "objectID": "posts/github-deploy-keys/index.html",
    "href": "posts/github-deploy-keys/index.html",
    "title": "GitHub Deploy Keys",
    "section": "",
    "text": "Photo¬†by¬†Chunli¬†Ju¬†on¬†Unsplash\n\n\nIf you use private GitHub repositories and have wondered what is the best way of being able to deploy your repositories to remote compute environments such as AWS, you are not alone. However, there is an elegant and secure solution that doesn‚Äôt require you to store your high-level GitHub SSH key in a remote instance.\nGitHub has a feature called deploy keys which allows you to add a separate SSH key to your private repository without write access. Furthermore, you can secure this key on your remote compute instance with a password, similar to what you might do on your local machine.\n\nThe first step, remote into your remote compute instance ‚òÅÔ∏è\nRun the following command to generate your SSH key\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are only going to use one repository, accept the default name when prompted to ‚ÄúEnter a file in which to save the key‚Äù. However, if you want to connect to multiple repositories don‚Äôt press Enter to accept the defaults. Instead, enter a name that relates to the corresponding repository you are wanting to connect to.\n\n\n\nUse cat to print the public SSH key to the screen and then copy it to your clipboard üìã\ncat /home/&lt;USER-NAME&gt;/.ssh/&lt;KEY-NAME&gt;.pub\nGo to your GitHub repository and click Settings.\n\nThen click Deploy keys from the side menu and then click Add deploy key.\n\nPaste in your public ssh key and add a relevant title and click Add key\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure you leave ‚ÄòAllow write access‚Äô un-ticked.\n\n\nMultiple repositories?\nWhat‚Äôs that you say, you have more than one repository, I would be worried if you didn‚Äôt. However, what you will initially find is that GitHub will not let you use the same ssh deploy key for a second repo. The solution? Generate a second key, but now you have to manually change between keys when you move between repos, and even more so remember which key is for which.\nAs always with Linux and git there is an elegant solution, a SSH config file.\n\nCreate a ssh config file with the following commands\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nUsing a command line text editor like vi edit the config file\nvi ~/.ssh/config\nAdd the following configuration information\nHost repo-alias\n  Hostname github.com\n  User git\n  IdentityFile /home/&lt;USER-NAME&gt;/.ssh/&lt;KEY-NAME&gt;\nWhat are these fields?\n\nHost: An alias to assign to the corresponding repository (does not need to be the same as the repo name)\nHostname: the hosting service i.e.¬†github.com\nUser: service using the ssh key (in this case git)\nIdentityFile: file path to the ssh key you previously generated\n\n\n\n\n\n\n\nTip\n\n\n\nWhile the alias you choose doesn‚Äôt have to match the repository name, it‚Äôs crucial to make it memorable. Using the repository name as the alias is a straightforward and effective approach.\n\n\nRepeat this process for any other repositories and SSH keys\n\nNow that you have added all your SSH keys along with a corresponding alias you can easily direct git to use the relevant key when working with different repositories.\nTo do this you insert the alias into the repository address between git@ and :repo-owner-name/repo-name.git\ngit clone git@&lt;alias&gt;:repo-owner-name/repo-name.git .\nOnce you have cloned a repository and want to run other commands such as git pull all you need to do is include the repo alias and address similar to the clone command.\ngit pull &lt;alias&gt;:repo-owner-name/repo-name\nFor instance, imagine Harry Potter üßôüîÆ maintains a GitHub repository named lumos and wishes to link it to a remote compute instance using the alias pipeline1. In his SSH config file, he would set it up like this:\nHost pipeline1\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/pipeline1_id_ed25519\nNow, when he wants to clone the repository, he can simply execute:\ngit clone git@pipeline1:harrypotter/lumos.git .\nTo fetch updates, he can use:\ngit pull pipeline1:harrypotter/lumos\nWith these steps, you have now streamlined your deployment process and ensured the appropriate SSH key can be selected. I hope this guide has been helpful in demystifying the process of deploying private repositories and managing SSH keys.\nHappy coding! üßë‚Äçüíªüîë"
  },
  {
    "objectID": "posts/packing-for-ec2/index.html#packing-for-aws-batch-ec2",
    "href": "posts/packing-for-ec2/index.html#packing-for-aws-batch-ec2",
    "title": "Packing for AWS Batch EC2",
    "section": "Packing for AWS Batch EC2 üíº",
    "text": "Packing for AWS Batch EC2 üíº\nWhen it comes to optimizing AWS Batch EC2 resources, diving into the intricate technicalities might seem daunting. However, my experiences have led me to uncover some practical insights and considerations that can significantly impact both performance and costs. Here‚Äôs a breakdown of ‚Äòpacking for Batch EC2‚Äô strategies I‚Äôve found useful:\nWhen considering the following principles, you might find the ec2-instance-selector and/or my blog post on visual EC2 instance selection useful tools.\nThese principles apply not only when using AWS Batch natively, but also via workflow managers such as Nextflow.\n\nHarnessing appropriate vCPU Sizes üßÆ\nUnderstanding vCPU counts and their increments is pivotal. CPU silicon is square in shape, and therefore, in most cases, CPU core counts go up in 2s. While there are exceptions, particularly for large/high core count server CPUs, they often increase in powers of 2 (i.e., 2, 4, 8, 16, 32, 64, 128, and so forth). Deviating from these increments will often result in over-provisioning, leading to underutilisation of available cores. While memory tends to play a crucial role in bioinformatics, reevaluating vCPU requirements can be equally beneficial and should not be overlooked.\n\n\n\n\n\nFor instance, suppose a job is parameterized to demand 6 vCPUs and 16 GiB of RAM. A swift check using the ec2-instance-selector reveals that AWS Batch might assign this job to an m7i-flex.2xlarge Spot instance, priced at $0.0962/Hr.\nec2-instance-selector --memory-min 16 --vcpus-min 6 --cpu-architecture x86_64 -r ap-southeast-2 -o table-wide --max-results 10 --usage-class spot --sort-by spot-price\nHowever, tweaking the request to 4 vCPUs could downscale it to an m7i-flex.xlarge Spot instance, costing only $0.0484/Hr.\nec2-instance-selector --memory-min 16 --vcpus-min 4 --cpu-architecture x86_64 -r ap-southeast-2 -o table-wide --max-results 10 --usage-class spot --sort-by spot-price\nEssentially, unless the job‚Äôs runtime is twice as slow with the 33% reduction in vCPUs, it‚Äôs more cost-efficient to scale back.\nKawaklia et al.¬†2015 published performance statistics for the popular sequence aligner BWA-Mem, illustrating that the runtime halves each time the number of threads (vCPUs) are doubled. For the above example, it would be more cost-efficient to reduce the job parameters from 6 to 4 vCPUs. Alternatively, you could increase it to 8 vCPUs, where you will typically pay twice the hourly instance price, but the runtime would now be halved, essentially resulting in the same job cost as 4 vCPUs.\n\n\n\nPerformance of Multi-Threading. CPU-time and walltime usage of BWA-Mem and GATK HaplotypeCaller with different number of threads\n\n\n\n\n\n\n\n\nNote\n\n\n\nA lot of bioinformatics tools will refer to threads or cores, while cloud computing talks in vCPUs. Threads and cores are physical processing units, while vCPUs are virtual processing units provisioned by the physical processors with the ability to run one processor thread. Typically each CPU cores will have 2 threads, and therefore the maximum vCPU count will be #Cores X2. Some servers are capable of running dual CPUs so this number could double again. In general terms when converting threads to vCPUs its a 1:1 conversion.\n\n\n\n\nSizing Memory Prudently üìè\nIn bioinformatics, memory requirements often steer resource allocation. Similar to vCPUs, memory increments follow powers of 2, such as 16, 32, 64GiB, and so forth. Aligning your job‚Äôs memory needs with these blocks will help ensure efficient utilisation of resources.\nAWS Batch tends to favour allocating jobs to small or medium Spot instances (where possible), avoiding larger ones like 8xlarge or 16xlarge instances. This preference might stem from lower interruption frequencies and/or the wider availability of smaller instances. Ultimately it is all controlled by AWS‚Äôs SPOT_PRICE_CAPACITY_OPTIMIZED strategy, aiming for a fine balance between price and capacity.\n\n\n\n\n\nAll in all, I would recommend that you ignore 8xlarge and 16xlarge instances when calculating how jobs may get packed into an instance.\n\n\n\n\n\n\nNote\n\n\n\nWhen running AWS Batch with on-demand instances, rather than Spot, you often find AWS jams your jobs into much larger instances, however the following is still good practise and will still be beneficial.\n\n\nWhile it is theoretically possible that 5 individual jobs requesting 36GiB each could be grouped together onto a single instance with 192GiB of RAM (93.75% utilisation), it is more likely that they will be placed on separate individual instances, each with 64 GiB RAM (56.25% utilisation). This would not be ideal for your budget üí∏ What would likely be better is if you requested 32 GiB RAM; then, they could run on single 32GiB instances or be grouped in multiples on 64GiB or 96GiB instances without wasting RAM allocations. More often than not, delivering better bang for your buck üíµ\n\n\n\n\n\nIt‚Äôs wise to plan job requests around small or medium instance sizes rather than assuming AWS Batch will fit them neatly into larger instances.\n\n\nRuntime ‚è±Ô∏è\nAlthough not immediately related to ‚ÄòEC2 packing,‚Äô optimizing the runtime of your Batch spot instances can make a significant improvement in your cost efficiency.\nWhile there is the well-known concept that the longer your Spot instance runs, the more likely it is to be interrupted, there is also a sometimes lesser-known fact. If a spot instance is interrupted/reclaimed by AWS within the first hour of running, you do not incur any fees ü§Ø Yes, that means as long as your job runtimes are less than 1 hour, you essentially get free üÜì retries (excluding other costs such as EBS).\n\nFor jobs that run for extended periods, there could be justification to submit these to an on-demand Batch compute environment to avoid potentially expensive retry costs.\n\n\nReview Review Review üîç\nOptimization is a progressive process. You need to review your Batch jobs to determine how much RAM and vCPUs are actually being utilized for your data input types and which type of instances in your region AWS likes to provision. Using a workflow manager like Nextflow that reports on the actual resource usage of individual jobs is a great way to interrogate and optimize a larger number of job types. Even better the Seqera platform for Nextflow will attempt to optimise your resource requests based on past run info.\n\n\n\nNextflow resource usage with an example execution report. Source: (https://www.nextflow.io/docs/latest/tracing.html#summary)\n\n\n‚ÄòPacking for AWS Batch EC2‚Äô is more than just fitting jobs into instances; it‚Äôs about optimising performance and costs through savvy üòé resource allocation."
  },
  {
    "objectID": "posts/screen/index.html",
    "href": "posts/screen/index.html",
    "title": "Screen Utility and Terminal Multiplexing",
    "section": "",
    "text": "Photo¬†by¬†Leif¬†Christoph¬†Gottwald¬†on¬†Unsplash\n\n\nIf you are using AWS (or other cloud computing platforms), particularly in an interactive or development mode, you will have experienced the frustration of network disconnects resulting in either termination of processes or loss of access to console output.\nEven when running production pipelines that harness AWS batch with workflow managers such as Nextflow it is still valuable to maintain access to console output and messages over the lifetime of the pipelines, which can be days.\nThankfully there is a very simple Linux command that solves all these problems, üåü screen üåü\n\nScreen is a linux utility, known as a terminal multiplier, which in very simple terms, allows you to detach and reattach to different terminal sessions. Importantly this also allows remote processes to continue even after you (purposely or unexpectedly) disconnect.\nIt is likely that this utility is already installed on your remote instance. Screen comes pre-installed in both the Amazon Linux 2023 and Ubuntu Server 22.04 LTS AMIs.\nTo get you started I have compiled a few simple commands that should keep your covered.\n\nStarting a screen session\nTo get started, you‚Äôll want to initiate and label a screen session. While the -S flag and name are optional, it‚Äôs advisable to label your session, sparing your brain üß† for more meaningful tasks. Here‚Äôs how to do it:\n\nNow that you have your screen session set up, you can run your interactive scripts or initiate a pipeline with a workflow manager such as Nextflow. Screen ensures that these processes continue running even if you need to step away, disconnect, or work on other tasks.\n\n\nDetach from screen session\nIf you want to step away from your active session to disconnect or to run a different script use the follow keyboard shortcut\n\n\n\nList screen sessions\nTo see a list of available screen sessions\n\n\n\nRe-attached to a screen session\nTo return to an active screen session.\n\n\n\n\n\n\n\nWarning\n\n\n\nEncountering the error message ‚Äòthere is no screen to be resumed‚Äô? Don‚Äôt worry, there‚Äôs a simple solution. If you unexpectedly disconnected from your previous session you may find that screen fails to recognise the detachment. Adding the -d flag to your command can save the day. This flag instructs screen to detach from any existing session before you attempt to reattach. By doing so, you‚Äôre ensuring a clean reconnection even in scenarios where your previous session‚Äôs state might not have been properly recognized due to an abrupt disconnection.\n\n\n\n\nTerminate a screen session\nIf you are currently within the screen session you wish to terminate it is as simple as typing exit and hitting enter\nIf you are detached from the session you want to terminate then you need to pass a quit command through to the relevant screen session using the -X flag.\n\n\n\nScrolling in a screen session\nYou will soon notice that you cannot scroll inside a screen session as you would in your normal terminal. Initially this is pretty frustrating üòß especially when you are trying to work through large log files or errors printed to the terminal.\nHowever, there is a simple way to scroll within a screen session. First, use the following keyboard shortcut\n\nNow press\n\nYou can now scroll up and down with the keyboard or mouse wheel.\n\nWhen finished just press q or\n\nThis is certainly not the complete list of screen functions. For power users üßë‚Äçüíª there is an array of other great features. Maybe something for a future post‚Ä¶"
  },
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "Publications",
    "section": "",
    "text": "See also:"
  },
  {
    "objectID": "publications/publications.html#patents",
    "href": "publications/publications.html#patents",
    "title": "Publications",
    "section": "Patents",
    "text": "Patents\nüí°Application Number: 2022260858\nTitle: Methods for the selection of plants having a high yield phenotype.\nDate: 19-04-2022\nüí°Application Number: 2014218508\nTitle: Manipulation of self-incompatibility in plants.\nDate: 19-02-2014"
  },
  {
    "objectID": "publications/publications.html#publications-by-date",
    "href": "publications/publications.html#publications-by-date",
    "title": "Publications",
    "section": "Publications by date",
    "text": "Publications by date\nüìÑ M. M. Malmberg, C. Smith, P. Thakur, M. C. Drayton, J. Wilson, M. Shinozuka, W. Clayton, C. Inch, G. C. Spangenberg, K. F. Smith, N. O. I. Cogan, L. W. Pembleton (2023). Developing an integrated genomic selection approach beyond biomass for varietal protection and nutritive traits in perennial ryegrass (Lolium perenne L.) Theoretical and Applied Genetics 136, 44.\nüìÑ Erez Naim-Feil, Edmond J. Breen, Luke W. Pembleton, Laura E. Spooner, German C. Spangenberg, Noel O. I. Cogan (2022). Empirical Evaluation of Inflorescences‚Äô Morphological Attributes for Yield Optimization of Medicinal Cannabis Cultivars. Frontiers in Plant Science, 13.\nüìÑ Yongjun Li, Sukhjiwan Kaur, Luke W. Pembleton, Hossein Valipour-Kahrood, Garry M. Rosewarne, Hans D. Daetwyler (2022). Strategies of preserving genetic diversity while maximizing genetic response from implementing genomic selection in pulse breeding programs. Theoretical and Applied Genetics, 1-16.\nüìÑ Erez Naim-Feil, Luke W. Pembleton, Laura E. Spooner, Alix L. Malthouse, Amy Miner, Melinda Quinn, Renata M. Polotnianka, Rebecca C. Baillie, German C. Spangenberg, Noel O. I. Cogan (2021). The characterization of key physiological traits of medicinal cannabis (Cannabis sativa L.) as a tool for precision breeding. BMC Plant Biology, 21(1).\nüìÑ Lydia M. Cranston, Keith G. Pembleton, Lucy L. Burkitt, Andrew Curtis, Daniel J. Donaghy, Cameron J. P. Gourley, Kerry C. Harrington, James L. Hills, Luke W. Pembleton, Richard P. Rawnsley (2020). The role of forage management in addressing challenges facing Australasian dairy farming. Anim. Prod. Sci., 60(1).\nüìÑ Abdulqader Jighly, Zibei Lin, Luke W. Pembleton, Noel O. I. Cogan, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2019). Boosting Genetic Gain in Allogamous Crops via Speed Breeding and Genomic Selection. Frontiers in Plant Science, 10.\nüìÑ Junping Wang, Pieter Badenhorst, Andrew Phelan, Luke Pembleton, Fan Shi, Noel Cogan, German Spangenberg, Kevin Smith (2019). Using Sensors and Unmanned Aircraft Systems for High-Throughput Phenotyping of Biomass in Perennial Ryegrass Breeding Trials. Frontiers in Plant Science, 10.\nüìÑ B. M. Caruana, L. W. Pembleton, F. Constable, B. Rodoni, A. T. Slater, N. O. I. Cogan (2019). Validation of Genotyping by Sequencing Using Transcriptomics for Diversity and Application of Genomic Selection in Tetraploid Potato. Frontiers in Plant Science, 10.\nüìÑ Luke W. Pembleton, Courtney Inch, Rebecca C. Baillie, Michelle C. Drayton, Preeti Thakur, Yvonne O. Ogaji, German C. Spangenberg, John W. Forster, Hans D. Daetwyler, Noel O. I. Cogan (2018). Exploitation of data from breeding programs supports rapid implementation of genomic selection for key agronomic traits in perennial ryegrass. Theoretical and Applied Genetics, 131(9).\nüìÑ M. Michelle Malmberg, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Shimna Sudheesh, Sukhjiwan Kaur, Hiroshi Shinozuka, Preeti Verma, German C. Spangenberg, Hans D. Daetwyler, John W. Forster, Noel O.I. Cogan (2018). Genotyping-by-sequencing through transcriptomics: implementation in a range of crop species with varying reproductive habits and ploidy levels. Plant Biotechnology Journal, 16(4).\nüìÑ Rebecca C. Baillie, Michelle C. Drayton, Luke W. Pembleton, Sukhjiwan Kaur, Richard A. Culvenor, Kevin F. Smith, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2017). Generation and Characterisation of a Reference Transcriptome for Phalaris (Phalaris aquatica L.). Agronomy, 7(1).\nüìÑ Zibei Lin, Junping Wang, Noel O.I. Cogan, Luke W. Pembleton, Pieter Badenhorst, John W. Forster, German C. Spangenberg, Ben J. Hayes, Hans D. Daetwyler (2017). Optimizing Resource Allocation in a Genomic Breeding Program for Perennial Ryegrass to Balance Genetic Gain, Cost, and Inbreeding. Crop Science, 57(1).\nüìÑ Luke W. Pembleton, Michelle C. Drayton, Melissa Bain, Rebecca C. Baillie, Courtney Inch, German C. Spangenberg, Junping Wang, John W. Forster, Noel O. I. Cogan (2016). Targeted genotyping-by-sequencing permits cost-effective identification and discrimination of pasture grass species and cultivars.Theoretical and Applied Genetics, 129(5).\nüìÑ Junping Wang, Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2016). Evidence for Heterosis in Italian Ryegrass (Lolium multiflorum Lam.) Based on Inbreeding Depression in F2 Generation Offspring from Biparental Crosses. Agronomy, 6(4).\nüìÑ L. W. Pembleton, J. Wang, G. C. Spangenberg, J. W. Forster, N. O. I. Cogan (2016). Low-cost automated biochemical phenotyping for optimised nutrient quality components in ryegrass breeding. Crop Pasture Sci., 67(8).\nüìÑ Zibei Lin, Noel O. I. Cogan, Luke W. Pembleton, German C. Spangenberg, John W. Forster, Ben J. Hayes, Hans D. Daetwyler (2016). Genetic Gain and Inbreeding from Genomic Selection in a Simulated Commercial Breeding Program for Perennial Ryegrass. The Plant Genome, 9(1).\nüìÑ Luke Pembleton, Hiroshi Shinozuka, Junping Wang, German Spangenberg, John Forster, Noel Cogan (2015). Design of an F1 hybrid breeding strategy for ryegrasses based on selection of self-incompatibility locus-specific alleles. Frontiers in Plant Science, 6.\nüìÑ J. Wang, N. O. I. Cogan, L. W. Pembleton, J. W. Forster (2015). Variance, inter-trait correlation, heritability and trait-marker association of herbage yield, nutritive values, and morphological characteristics in Italian ryegrass (Lolium multiflorum Lam.). Crop Pasture Sci., 66(9).\nüìÑ Junping Wang, Luke W. Pembleton, Rebecca C. Baillie, Michelle C. Drayton, Melanie L. Hand, Melissa Bain, Timothy I. Sawbridge, German C. Spangenberg, John W. Forster, Noel O. I. Cogan (2014). Development and implementation of a multiplexed single nucleotide polymorphism genotyping tool for differentiation of ryegrass species and cultivars. Molecular Breeding, 33(2).\nüìÑ L. W. Pembleton, J. Wang, N. O. I. Cogan, J. E. Pryce, G. Ye, C. K. Bandaranayake, M. L. Hand, R. C. Baillie, M. C. Drayton, K. Lawless, S. Erb, M. P. Dobrowolski, T. I. Sawbridge, G. C. Spangenberg, K. F. Smith, J. W. Forster (2013). Candidate gene-based association genetics analysis of herbage quality traits in perennial ryegrass (Lolium perenne L.). Crop Pasture Sci., 64(3).\nüìÑ Benjamin J. Hayes, Noel O. I. Cogan, Luke W. Pembleton, Michael E. Goddard, Junping Wang, German C. Spangenberg, John W. Forster (2013). Prospects for genomic selection in forage plant species. Plant Breeding, 132(2).\nüìÑ Luke W. Pembleton, Noel O. I. Cogan, John W. Forster (2013). StAMPP: an R package for calculation of genetic differentiation and structure of mixed-ploidy level populations. Molecular Ecology Resources, 13(5).\nüìÑ Sukhjiwan Kaur, Luke W. Pembleton, Noel O. I. Cogan, Keith W. Savin, Tony Leonforte, Jeffrey Paull, Michael Materne, John W. Forster (2012). Transcriptome sequencing of field pea and faba bean for discovery and validation of SSR genetic markers. BMC Genomics, 13(1).\nüìÑ Sukhjiwan Kaur, Noel O. I. Cogan, Luke W. Pembleton, Maiko Shinozuka, Keith W. Savin, Michael Materne, John W. Forster (2011).Transcriptome sequencing of lentil based on second-generation technology permits large-scale unigene assembly and SSR marker discovery. BMC Genomics, 12(1)."
  },
  {
    "objectID": "ramblings/Colours/index.html",
    "href": "ramblings/Colours/index.html",
    "title": "Colours",
    "section": "",
    "text": "Eerie Black #1F1E1F\n\n\n\n\nDim gray #66666E\n\n\n\n\nPrussian Blue #023047\n\n\n\n\nAir Force Blue #598392\n\n\n\n\nDark Spring Green #226F54\n\n\n\n\nPistachio #87C38F\n\n\n\n\nBurnt Sienna #E76F51\n\n\n\n\nSandy Brown #F4A261\n\n\n\n\nagriculture &lt;- c(\"#1f1e1f\", \"#66666e\", \"#023047\", \"#598392\", \"#226f54\", \"#87c38f\", \"#e76f51\", \"#f4a261\")"
  },
  {
    "objectID": "ramblings/Colours/index.html#agriculture",
    "href": "ramblings/Colours/index.html#agriculture",
    "title": "Colours",
    "section": "",
    "text": "Eerie Black #1F1E1F\n\n\n\n\nDim gray #66666E\n\n\n\n\nPrussian Blue #023047\n\n\n\n\nAir Force Blue #598392\n\n\n\n\nDark Spring Green #226F54\n\n\n\n\nPistachio #87C38F\n\n\n\n\nBurnt Sienna #E76F51\n\n\n\n\nSandy Brown #F4A261\n\n\n\n\nagriculture &lt;- c(\"#1f1e1f\", \"#66666e\", \"#023047\", \"#598392\", \"#226f54\", \"#87c38f\", \"#e76f51\", \"#f4a261\")"
  },
  {
    "objectID": "ramblings/Colours/index.html#agricultural-seasons",
    "href": "ramblings/Colours/index.html#agricultural-seasons",
    "title": "Colours",
    "section": "Agricultural Seasons",
    "text": "Agricultural Seasons\nA subset of the colour blind üé®üëì friendly palette from Bang Wong 2011, with predefined allocation to agriculture seasons. The colour associations are meant to be semi-logical (taken from typical seasonal colour associations) to enable more natural interpretation, without constant legend referencing.\n\n\n\nWinter #0072B2\n\n\n\n\nEarly Spring #F0E442\n\n\n\n\nLate Spring #009E73\n\n\n\n\nSummer #56B4E9\n\n\n\n\nAutumn #D55E00\n\n\n\n\nag_seasons &lt;- c(\"#0072B2\", \"#F0E442\", \"#009E73\", \"#56B4E9\", \"#D55E00\")"
  },
  {
    "objectID": "ramblings/Colours/index.html#halloween",
    "href": "ramblings/Colours/index.html#halloween",
    "title": "Colours",
    "section": "Halloween",
    "text": "Halloween\n\n\n\nMantis #79C250\n\n\n\n\nPurple Haze #52307D\n\n\n\n\nCarrot Orange #FE8622\n\n\n\n\nSand #ECCC60\n\n\n\n\nThunder #2F2D2E\n\n\n\n\nhalloween &lt;- c(\"#79C250\", \"#52307D\", \"#FE8622\", \"#ECCC60\", \"#2F2D2E\")"
  },
  {
    "objectID": "ramblings/Colours/index.html#sequencing-companies",
    "href": "ramblings/Colours/index.html#sequencing-companies",
    "title": "Colours",
    "section": "Sequencing Companies",
    "text": "Sequencing Companies\nMore to be used as a colour reference rather than a palette, as the contrast between some (i.e.¬†Nanopore & MGI) is too low.\n\n\n\nIllumina #79C250\n\n\n\n\nOxford Nanopore #2F2D2E\n\n\n\n\nMGI #52307D\n\n\n\n\nElement Biosciences #FE8622\n\n\n\n\nSingular Genomics #ECCC60\n\n\n\n\nseq_companies &lt;- c(\"#F9A31E\", \"#0083A9\", \"#006AB7\", \"#0A7537\", \"#1E1E1E\")"
  },
  {
    "objectID": "ramblings/Colours/index.html#windows-desktops",
    "href": "ramblings/Colours/index.html#windows-desktops",
    "title": "Colours",
    "section": "Windows Desktops",
    "text": "Windows Desktops\n\n\n\n95 Green #79C250\n\n\n\n\n95 Grey #2F2D2E\n\n\n\n\nME Blue #52307D\n\n\n\n\nME Grey #FE8622\n\n\n\n\nXP Blue #ECCC60\n\n\n\n\nXP Green #ECCC60\n\n\n\n\nwin_desktops &lt;- c('#008080', '#C0C0C0', '#3A6EA5', '#D4D0C8', '#255FDC', '#48AF48')"
  },
  {
    "objectID": "ramblings/R/index.html",
    "href": "ramblings/R/index.html",
    "title": "R",
    "section": "",
    "text": "Photo¬†by¬†Daria¬†Nepriakhina¬†on¬†Unsplash\nA worthy collection of short R commands and tricks. üëç As featured in R Weekly 2023-W44"
  },
  {
    "objectID": "ramblings/R/index.html#using-custom_annotation-to-embed-one-plots-inside-another.",
    "href": "ramblings/R/index.html#using-custom_annotation-to-embed-one-plots-inside-another.",
    "title": "R",
    "section": "Using {custom_annotation} to embed one plots inside another.",
    "text": "Using {custom_annotation} to embed one plots inside another.\nEmbedded plots can be a powerful tool for showcasing data across an extended axis while still emphasizing specific sections.\nCreating these are very easy. Here is a quick guide and example:\nStep 1: Create your base plot.\nBegin by establishing your base plot, which will serve as the canvas for your embedded plot. Typically, the base plot is the primary focus of your visualization.\n\nlibrary(tidyverse)\nlibrary(mdthemes)\ndata(mtcars)\n\nbase_plot &lt;- ggplot(mtcars, aes(hp, mpg)) +\n  geom_point(size = 4, alpha = 0.6, colour = \"#22333b\") +\n  geom_smooth(size = 1.5, se = F, colour = \"#3a86ff\") +\n  labs(title = \"Example of a plot **embedded** within another\",\n    subtitle = \"Using the function *'annotation_custom()'*\") +\n  md_theme_bw() +\n  xlim(50, 200)\n\nStep 2: Create your plot that you want to embed within the canvas.\nYour embedded plot doesn‚Äôt have to be the same type as your base plot. Feel free to customize it according to your data visualization needs.\n\nembedded_plot &lt;- ggplot(mtcars, aes(hp, mpg)) +\n  geom_point(size = 4, alpha = 0.6, colour = \"#22333b\") +\n  geom_smooth(size = 1.5, se = F, colour = \"#3a86ff\") +\n  md_theme_bw()\n\nStep 3: Use custom_annotate() to embed the plot.\nNow, it‚Äôs time to embed the plot within the canvas using the annotation_custom() function. You‚Äôll need to specify the X and Y positions for the embedded plot:\n\nbase_plot + annotation_custom(ggplotGrob(embedded_plot), \n                              xmin = 125, \n                              xmax = 200, \n                              ymin = 22, \n                              ymax = 35)"
  },
  {
    "objectID": "ramblings/R/index.html#adding-linking-colours-to-plot-titles-instead-of-a-legend",
    "href": "ramblings/R/index.html#adding-linking-colours-to-plot-titles-instead-of-a-legend",
    "title": "R",
    "section": "Adding linking colours to plot titles instead of a legend",
    "text": "Adding linking colours to plot titles instead of a legend\nEnhancing your plot titles with linking colours üåà is a clever strategy to maximise your plot realestate by eliminating legends, all while looking fantastic!\n\n\n\n\n\n\n\n\n\nTo achieve this style you will need:\n\nthe {ggtext} package which will perform the HTML rendering, in this case in the subtitle to define the text colour.\nAdding plot.subtitle = element_markdown() to the theme, {ggtext} will perform markdown rendering, for example making specific text bold, or in the above example, italicised.\nand finally, adding legend.position = \"none\" also to the theme will remove the old clunky default legend\n\nHere is the full code üëá enjoy!\n\nlibrary(ggplot2) # For plotting\nlibrary(palmerpenguins) # For the example penguin dataset\nlibrary(ggtext) # For HTML rendering of text to support colour\n                # Also for Markdown rendering of text\n\nggplot(data = penguins, \n       aes(x = flipper_length_mm, \n           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n                 size = 2) +\n  scale_color_manual(values = c(\"#FF8C00\",\"#9932CC\",\"#008B8B\")) +\n  labs(title = \"Penguin flipper length versus body mass\",\n       subtitle = \"Penguin species\n       &lt;span style='color:#FF8C00;'&gt;*Pygoscelis adeliae*&lt;/span&gt;, \n       &lt;span style='color:#9932CC;'&gt;*Pygoscelis papua*&lt;/span&gt; and \n       &lt;span style='color:#008B8B;'&gt;*Pygoscelis antarcticus*&lt;/span&gt;\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")  +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.subtitle = element_markdown(),\n        legend.position = \"none\")\n\nIf you want to go a step further you can add the corresponding point shape/symbol to the subtitle as wellüéâ\n\n\n\n\n\n\n\n\n\nAll you need to do is add the HTML Unicode (e.g.¬†&#9679 for ‚óè) for the matching symbol/shape to the subtitle. You can look up the HTML Unicode here.\n\n\n\n\n\n\nNote\n\n\n\nFor some reason when viewing the plot in the Rstudio plot tab, additional spaces (relative to the length of the Unicode) will appear next to the symbols. However, this disappears when you render the image in a quarto document or save the plot as an image.\n\n\n\nHere is the full code üëá enjoy!\n\nlibrary(ggplot2) # For plotting\nlibrary(palmerpenguins) # For the example penguin dataset\nlibrary(ggtext) # For HTML rendering of text to support colour\n# Also for Markdown rendering of text\n# Get HTML code for genometric symbols from:\n# https://www.htmlsymbols.xyz/geometric-symbols\n\nggplot(data = penguins, \n       aes(x = flipper_length_mm, \n           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 2) +\n  scale_color_manual(values = c(\"#FF8C00\",\"#9932CC\",\"#008B8B\")) +\n  labs(title = \"Penguin flipper length versus body mass\",\n       subtitle = \"Penguin species\n       &lt;span style='color:#FF8C00;'&gt;*Pygoscelis adeliae* &#9679;&lt;/span&gt;, \n       &lt;span style='color:#9932CC;'&gt;*Pygoscelis papua* &#11205;&lt;/span&gt; and \n       &lt;span style='color:#008B8B;'&gt;*Pygoscelis antarcticus* &#11200;&lt;/span&gt;\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")  +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        plot.subtitle = element_markdown(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "ramblings/R/index.html#transform-axis-numbers-into-more-readable-formats",
    "href": "ramblings/R/index.html#transform-axis-numbers-into-more-readable-formats",
    "title": "R",
    "section": "Transform axis numbers into more readable formats",
    "text": "Transform axis numbers into more readable formats\nWant to avoid number labels on your plots that are difficult to read or interpret such as the all too common scientific e notation as seen in this example? üëá\n\n\n\n\n\n\n\n\n\nThe label_number_si() function from the {scales} package will conveniently convert your unclear labels to a more appropriate format. Lets apply it to our example from above. ü•≥üéâ\n\n\n\n\n\n\n\n\n\nHere is the full code üëá enjoy!\n\nlibrary(ggplot2) # For plotting\nlibrary(ggtext) # For HTML rendering of text to support colour\n                # Also for Markdown rendering of text\nlibrary(scales)\n\ndata2 &lt;- tibble::tribble(\n             ~Month,   ~Year,     ~Region, ~Production_Litres,\n             \"July\", \"22/23\",       \"NSW\",        80742825.35,\n           \"August\", \"22/23\",       \"NSW\",        85515267.53,\n        \"September\", \"22/23\",       \"NSW\",        89642883.77,\n          \"October\", \"22/23\",       \"NSW\",        92774430.63,\n         \"November\", \"22/23\",       \"NSW\",         86431638.4,\n         \"December\", \"22/23\",       \"NSW\",        86230113.71,\n          \"January\", \"22/23\",       \"NSW\",        81030159.32,\n         \"February\", \"22/23\",       \"NSW\",        72891273.03,\n            \"March\", \"22/23\",       \"NSW\",        78366274.94,\n            \"April\", \"22/23\",       \"NSW\",         75805741.9,\n              \"May\", \"22/23\",       \"NSW\",        79986532.53,\n             \"June\", \"22/23\",       \"NSW\",        80446060.32,\n             \"July\", \"22/23\",       \"VIC\",       386373862.86,\n           \"August\", \"22/23\",       \"VIC\",        450615318.9,\n        \"September\", \"22/23\",       \"VIC\",       527734837.67,\n          \"October\", \"22/23\",       \"VIC\",        575715726.1,\n         \"November\", \"22/23\",       \"VIC\",       519338047.94,\n         \"December\", \"22/23\",       \"VIC\",       499009041.08,\n          \"January\", \"22/23\",       \"VIC\",       423251928.39,\n         \"February\", \"22/23\",       \"VIC\",       323689836.55,\n            \"March\", \"22/23\",       \"VIC\",        333750042.9,\n            \"April\", \"22/23\",       \"VIC\",       340058086.75,\n              \"May\", \"22/23\",       \"VIC\",        388157951.5,\n             \"June\", \"22/23\",       \"VIC\",       373346197.82,\n             \"July\", \"22/23\", \"Australia\",        570199372.1,\n           \"August\", \"22/23\", \"Australia\",       659242110.71,\n        \"September\", \"22/23\", \"Australia\",       797167795.77,\n          \"October\", \"22/23\", \"Australia\",       888535142.24,\n         \"November\", \"22/23\", \"Australia\",        818914946.8,\n         \"December\", \"22/23\", \"Australia\",       790237598.05,\n          \"January\", \"22/23\", \"Australia\",       693408820.19,\n         \"February\", \"22/23\", \"Australia\",       553467890.76,\n            \"March\", \"22/23\", \"Australia\",       580325765.96,\n            \"April\", \"22/23\", \"Australia\",       577153026.75,\n              \"May\", \"22/23\", \"Australia\",       623257518.13,\n             \"June\", \"22/23\", \"Australia\",       576627597.06,\n             \"July\", \"23/24\",       \"NSW\",        87662230.03,\n           \"August\", \"23/24\",       \"NSW\",        93724243.26,\n        \"September\", \"23/24\",       \"NSW\",        96012475.27,\n             \"July\", \"23/24\",       \"VIC\",       382943134.47,\n           \"August\", \"23/24\",       \"VIC\",       447394128.72,\n        \"September\", \"23/24\",       \"VIC\",       526453329.78,\n             \"July\", \"23/24\", \"Australia\",       576952581.71,\n           \"August\", \"23/24\", \"Australia\",       670893737.51,\n        \"September\", \"23/24\", \"Australia\",       809301895.87\n        )\ndata &lt;- data |&gt; mutate(Month = factor(Month, levels = c(\"July\", \"August\", \"September\", \"October\", \"November\", \"December\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\")))\n\n\nggplot(data = data, aes(x = Month, y = Production_Litres, colour = Region, linetype = Year, group = interaction(Region, Year))) +\n  geom_smooth(se=FALSE) +\n  scale_color_manual(values = c(\"#000000\",\"#428bca\",\"#b5b682\")) +\n  scale_y_continuous(labels = scales::label_number_si()) +\n  labs(title = \"Australian Dairy Milk Production\",\n       subtitle = \"&lt;span style='color:#000000;'&gt;**National Production**&lt;/span&gt; and key dairy states \n         &lt;span style='color:#b5b682;'&gt;**Victoria**&lt;/span&gt; and \n         &lt;span style='color:#428bca;'&gt;**New South Wales.**&lt;/span&gt;\",\n       x = \"\",\n       y = \"Production (Litres)\",\n       caption = \"Data Source: Dairy Australia Milk Production Report September 2023\")  +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        plot.subtitle = element_markdown(),\n        legend.position = \"top\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  guides(colour = \"none\", linetype = guide_legend(override.aes = list(color = \"#000000\")))"
  },
  {
    "objectID": "ramblings/R/index.html#elegantly-join-3-tibbles-using-a-common-key",
    "href": "ramblings/R/index.html#elegantly-join-3-tibbles-using-a-common-key",
    "title": "R",
    "section": "Elegantly Join 3+ tibbles using a common key",
    "text": "Elegantly Join 3+ tibbles using a common key\nJoining two tibbles is simple and concise using the join commands from dplyr. However, when you have three or more tibbles to join, you will often find yourself either writing a long pipe or taking a stepwise approach of joining the first two into a new object and using that to join with the third tibble, and so on.\nNevertheless, you can avoid all this by combining the reduce function from the üêà‚Äç‚¨õ purrr package with the relevant join command. Below is a simple example. üëá\n\nlibrary(dplyr)\nlibrary(purrr)\n\ndt1 &lt;- tibble(character = c(\"Harry\", \"Hermione\", \"Draco\"),\n              house = c(\"Gryffindor\", \"Gryffindor\", \"Slytherin\"))\n\ndt2 &lt;- tibble(character = c(\"Harry\", \"Hermione\", \"Draco\"),\n              wand = c(\"Holly-phonenix-feather\", \"Vine-dragon-heartstring\", \"Hawthorn-unicorn-hair\"))\n\ndt3 &lt;- tibble(character = c(\"Harry\", \"Hermione\", \"Draco\"),\n              gender = c(\"Male\", \"Female\", \"Male\"))            \n\nreduce(list(dt1, dt2, dt3), left_join, by = 'character')\n\n# A tibble: 3 x 4\n  character house      wand                    gender\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;                   &lt;chr&gt; \n1 Harry     Gryffindor Holly-phonenix-feather  Male  \n2 Hermione  Gryffindor Vine-dragon-heartstring Female\n3 Draco     Slytherin  Hawthorn-unicorn-hair   Male"
  },
  {
    "objectID": "ramblings/R/index.html#add-a-prefix-to-column-names-when-pivoting-from-long-data-to-wide-data",
    "href": "ramblings/R/index.html#add-a-prefix-to-column-names-when-pivoting-from-long-data-to-wide-data",
    "title": "R",
    "section": "Add a prefix to column names when pivoting from long-data to wide-data",
    "text": "Add a prefix to column names when pivoting from long-data to wide-data\nAt times, you may find it necessary to apply a prefix to the column names created when transitioning from long-format data to wide-format data. The good news is that the pivot_wider() function streamlines this process by offering the names_prefix argument. This feature proves particularly valuable when working with multiple tibbles that share similar naming conventions and structures. After all, nobody enjoys dealing with default conflict resultion of identical column names like colname1.x, colname1.y, colname2.x, colname2.y, and so on.\n\nlibrary(tidyr)\nlibrary(tibble)\n\nweather &lt;- tribble(\n                 ~Date, ~Measurement, ~Value,\n             20100101L,   \"T.Max_oC\",   31.2,\n             20100102L,   \"T.Max_oC\",   31.7,\n             20100103L,   \"T.Max_oC\",   32.5,\n             20100104L,   \"T.Max_oC\",   28.9,\n             20100101L,   \"T.Min_oC\",   21.3,\n             20100102L,   \"T.Min_oC\",   21.5,\n             20100103L,   \"T.Min_oC\",   22.2,\n             20100104L,   \"T.Min_oC\",   22.1,\n             20100101L,    \"Rain_mm\",    0.6,\n             20100102L,    \"Rain_mm\",    0.4,\n             20100103L,    \"Rain_mm\",    0.8,\n             20100104L,    \"Rain_mm\",    6.8,\n             20100101L,    \"Evap_mm\",    4.1,\n             20100102L,    \"Evap_mm\",    4.6,\n             20100103L,    \"Evap_mm\",    5.9,\n             20100104L,    \"Evap_mm\",    4.4\n             )\nweather\n\n# A tibble: 16 x 3\n       Date Measurement Value\n      &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 20100101 T.Max_oC     31.2\n 2 20100102 T.Max_oC     31.7\n 3 20100103 T.Max_oC     32.5\n 4 20100104 T.Max_oC     28.9\n 5 20100101 T.Min_oC     21.3\n 6 20100102 T.Min_oC     21.5\n 7 20100103 T.Min_oC     22.2\n 8 20100104 T.Min_oC     22.1\n 9 20100101 Rain_mm       0.6\n10 20100102 Rain_mm       0.4\n11 20100103 Rain_mm       0.8\n12 20100104 Rain_mm       6.8\n13 20100101 Evap_mm       4.1\n14 20100102 Evap_mm       4.6\n15 20100103 Evap_mm       5.9\n16 20100104 Evap_mm       4.4\n\nweather |&gt; \n  pivot_wider(names_from = Measurement, \n              names_prefix = \"UQGatton_\",\n              values_from = Value)\n\n# A tibble: 4 x 5\n      Date UQGatton_T.Max_oC UQGatton_T.Min_oC UQGatton_Rain_mm UQGatton_Evap_mm\n     &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 20100101              31.2              21.3              0.6              4.1\n2 20100102              31.7              21.5              0.4              4.6\n3 20100103              32.5              22.2              0.8              5.9\n4 20100104              28.9              22.1              6.8              4.4"
  },
  {
    "objectID": "ramblings/R/index.html#initalise-an-empty-tibble-with-predined-column-names",
    "href": "ramblings/R/index.html#initalise-an-empty-tibble-with-predined-column-names",
    "title": "R",
    "section": "Initalise an empty tibble with predined column names",
    "text": "Initalise an empty tibble with predined column names\nBefore diving into a loop of calculations, it‚Äôs often beneficial to initialise an empty tibble to seamlessly store results from each iteration. Instead of manually specifying column names, you can streamline the process by leveraging the power of the map_dfc() command in {purrr} üêà Simply pass a vector of column names to tibble, and watch your data structure effortlessly take shape.\n\nvec_colnames &lt;- c(\"Wizard\", \"House\", \"Wand\")\n\nempty_tibble &lt;- vec_colnames |&gt;\n  purrr::map_dfc(setNames, object = list(character()))\n\nempty_tibble\n\n# A tibble: 0 x 3\n# i 3 variables: Wizard &lt;chr&gt;, House &lt;chr&gt;, Wand &lt;chr&gt;\n\n\nmap_dfc() has now been superseded üèó by map() and I have also updated the code to be more explicate and inline with {purrr} recommendations. Thanks Jim Gardner for the suggestions and edits.\n\nvec_colnames &lt;- c(\"Wizard\", \"House\", \"Wand\")\n\nempty_tibble &lt;- vec_colnames |&gt; \n  purrr::map(\\(x) setNames(object = tibble::tibble(character()), nm = x)) |&gt; \n  purrr::list_cbind()\n\nempty_tibble\n\n# A tibble: 0 x 3\n# i 3 variables: Wizard &lt;chr&gt;, House &lt;chr&gt;, Wand &lt;chr&gt;"
  },
  {
    "objectID": "ramblings/R/index.html#highlight-points-of-interest-with-annotations",
    "href": "ramblings/R/index.html#highlight-points-of-interest-with-annotations",
    "title": "R",
    "section": "Highlight points of interest with annotations",
    "text": "Highlight points of interest with annotations\nUtilising arrow and text annotations can significantly elevate the impact of your visualisations by directing attention to crucial data points or subsets within a plot. These annotations serve as a powerful tool to effectively convey your message to the reader. Checkout the following example for a demonstration.\n\n\n\n\n\n\n\n\n\nHere is the full code üëá showing how to use the annotate function, enjoy!\n\nlibrary(ggplot2)\nlibrary(ggtext)\n\nngs_data &lt;- tibble::tribble(\n                                   ~Platform, ~Yield, ~Price_per_Gb,            ~Company, ~Flowcells, ~Yield_per_flowcell,\n                   \"ILMN NextSeq 550 1fcell\",   120L,          43.8,          \"Illumina\",         1L,                120L,\n                   \"ILMN HiSeq 4000 2fcells\",  1500L,            25,          \"Illumina\",         2L,                750L,\n            \"ILMN NextSeq 1000 P1/P2 1fcell\",   120L,         30.61,          \"Illumina\",         1L,                120L,\n               \"ILMN NextSeq 2000 P3 1fcell\",   360L,          17.3,          \"Illumina\",         1L,                360L,\n                   \"ILMN NovaSeq SP 2fcells\",   800L,          10.9,          \"Illumina\",         2L,                400L,\n                   \"ILMN NovaSeq S1 2fcells\",  1000L,          10.9,          \"Illumina\",         2L,                500L,\n                   \"ILMN NovaSeq S2 2fcells\",  2000L,          7.97,          \"Illumina\",         2L,               1000L,\n              \"ILMN NovaSeq S4 v1.5 2fcells\",  6000L,          4.84,          \"Illumina\",         2L,               3000L,\n  \"ILMN NovaSeq X & X Plus 1.5B 10B 2fcells\",  6000L,           3.2,          \"Illumina\",         2L,               3000L,\n                \"ILMN NovaSeq X 25B 1fcells\",  8000L,             2,          \"Illumina\",         1L,               8000L,\n     \"ElemBio AVITI 2fcell x3 pricing model\",  1800L,             2,        \"ElementBio\",         2L,                900L,\n     \"ElemBio AVITI 2fcell 2x150 Cloudbreak\",   600L,             5,        \"ElementBio\",         2L,                300L,\n     \"ElemBio AVITI 2fcell 2x300 Cloudbreak\",   360L,   13.55555556,        \"ElementBio\",         2L,                180L,\n   \"Singular Genomics G4 F2 4fcell Standard\",   200L,            16, \"Singular Genomics\",         4L,                 50L,\n   \"Singular Genomics G4 F2 4fcell Max Read\",   200L,            16, \"Singular Genomics\",         4L,                 50L,\n            \"Singular Genomics G4 F3 4fcell\",   540L,             8, \"Singular Genomics\",         4L,                135L,\n                     \"MGI DNBSEQ-T7 4fcells\",  6000L,           1.5,               \"MGI\",         4L,               1500L,\n         \"MGI DNBSEQ-G400C  CoolMPS 2fcells\",  1440L,             5,               \"MGI\",         2L,                720L,\n         \"MGI DNBSEQ-G400RS  HotMPS 2fcells\",   720L,           5.8,               \"MGI\",         2L,                360L\n  )\n\nggplot(data = ngs_data, aes(x=Yield_per_flowcell, y=Price_per_Gb, colour=Company, fill=Company)) +\n  geom_point(size=2) +\n  scale_y_continuous(trans = \"log10\") +\n  scale_x_continuous(trans = \"log10\") +\n  scale_color_manual(values = c(\"Illumina\" = \"#FFB542\", \n                                \"MGI\" =\"#006AB7\", \n                                \"ElementBio\" = \"#12B460\", \n                                \"Singular Genomics\" = \"#1E1E1E\")) +\n  theme_classic() +\n  ylab(\"Price per Gb ($)\") +\n  xlab(\"Yield per Flowcell (Gb)\") +\n  annotate(geom = \"curve\", \n           x = 400, y = 2.5, \n           xend = 800, yend = 2, \n           curvature = .3, \n           arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 300, y = 3, label = paste(strwrap(\"Newer companies like Element Biosciences are challanging Illumina's price point in the mid-tier benchtop sequencing category\", width = 70), collapse = \"\\n\"), hjust = \"centre\", size=3) +\n  labs(title = \"Sequencing costs for current benchtop and production-scale platforms\", \n       subtitle = \"Companies\n       &lt;span style='color:#FFB542;'&gt;**Illumina**&lt;/span&gt;, \n       &lt;span style='color:#006AB7;'&gt;**MGI**&lt;/span&gt;,\n       &lt;span style='color:#12B460;'&gt;**Element Biosciences**&lt;/span&gt; and\n       &lt;span style='color:#1E1E1E;'&gt;**Singular Genomics**&lt;/span&gt;\",\n       caption = \"Note: X & Y axis log10 transformed. \\n Data Source: Next-Generation-Sequencing.v1.10.31 @albertvilella\") +\n  theme(panel.grid = element_blank(),\n        plot.subtitle = element_markdown(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "ramblings/R/index.html#order-a-boxplot-for-improved-across-axis-comparisions",
    "href": "ramblings/R/index.html#order-a-boxplot-for-improved-across-axis-comparisions",
    "title": "R",
    "section": "Order a boxplot for improved across axis comparisions",
    "text": "Order a boxplot for improved across axis comparisions\nIf you are using a boxplot to demonstrate differences across multiple groups you may encounter the problem whereby the standard order of groups results in a saw tough pattern which can make it difficult to compare across groups üòïüëá\nWhen creating a boxplot to showcase differences and variations among multiple groups, you might run into an issue where the default order of groups creates a jagged pattern üëá This can really complicate the comparison process across these groups üòï\n\n\n\n\n\n\n\n\n\nTo enhance the clarity of your visual representation and make comparisons easier, you should consider reordering the groups in a more intuitive or meaningful sequence. This can help in presenting the data in a way that is more easily understandable and conducive to drawing accurate conclusions.\nTo reorder x from low to high based on y-values use the reorder function within the ggplot mappings.\n\n\n\n\n\n\n\n\n\nHere is the full code üëá showing how to use the reorder function, enjoy!\n\nlibrary(tidyverse)\nggplot(mpg, aes(x = reorder(manufacturer, cty, na.rm = TRUE), y = cty)) + \n  geom_boxplot(fill = \"#219ebc\") + \n  geom_dotplot(binaxis='y', \n               stackdir='center', \n               dotsize=0.5,\n               fill = \"#f4a261\") +\n  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + \n  labs(title=\"City Mileage vs Class\", \n       subtitle=\"Ordered x axis\",\n       caption=\"Source: mpg\",\n       x=\"Class of Vehicle\",\n       y=\"City Mileage\")"
  },
  {
    "objectID": "ramblings/R/index.html#labelling-legends",
    "href": "ramblings/R/index.html#labelling-legends",
    "title": "R",
    "section": "Labelling Legends",
    "text": "Labelling Legends\nLegend titles and value labels in visual representations by default often lack clarity and may appear disorganized. This happens as they‚Äôre directly derived from data and column headings, typically designed to be concise and format-free in programming. Consequently, they might seem puzzling or cluttered. Consider the plot below: if you‚Äôre acquainted with the mpg dataset, you‚Äôd recognise drv as denoting drive train type‚Äîwhere 4 stands for 4-wheel drive, f for front-wheel drive, and r for rear-wheel drive, while cyl indicates the number of cylinders.\nHowever, if you do not know of the mpg dataset, which lets be honest is the likely scenario of your target audience, then you might just leave them as puzzled as a possum üòï\n\n\nWarning: package 'ggdist' was built under R version 4.1.3\n\n\n\n\n\n\n\n\n\nHowever correcting this in your plots is dead simple. Just add label and name (title) details to the scale_colour_* and scale_fill_* functions as demonstrated below üëá\n\nggplot(data = mpg |&gt; \n         filter(cyl != 5) |&gt;\n         mutate(cyl = as.factor(cyl)),\n       aes(x = hwy, y = cyl, fill = cyl)) +\n  stat_halfeye(\n    point_color = NA, .width = 0, height = 0.5,\n    position = position_nudge(y = 0.2),\n    alpha = 0.7,\n    slab_colour = \"black\"\n  ) +\n  geom_point(position = position_jitter(width = 0.2, height = 0.15, seed = 1),\n             alpha = 0.7,\n             size = 3,\n             aes(colour = drv)) +\n  scale_colour_manual(values = c(\"#fe7f2d\", \"#619b8a\", \"#233d4d\"),\n1                      labels = c(\"4 wheel drive\",\n                                 \"Front wheel drive\",\n                                 \"Rear wheel drive\"),\n2                      name = \"Drive Train\") +\n  scale_fill_manual(values = c(\"#90e0ef\", \"#0077b6\", \"#03045e\"),\n3                    name = \"Cylinders\") +\n  theme_bw() +\n  ylab(\"Cylinders\") +\n  xlab(\"Highway Miles per Gallon\") +\n  ggtitle(\"Fuel economy for 38 popular car models from 1999 to 2008\")\n\n\n1\n\nAdd new labels to the three point data categories\n\n2\n\nAdd a name/title to the point data legend\n\n3\n\nAdd a name/title to the distribution data legend\n\n\n\n\n\n\n\n\n\n\n\nBe a champion and help out your audience üëç"
  },
  {
    "objectID": "ramblings/R/index.html#extracting-array-indexes-of-the-maximum-value",
    "href": "ramblings/R/index.html#extracting-array-indexes-of-the-maximum-value",
    "title": "R",
    "section": "Extracting array indexes of the maximum value",
    "text": "Extracting array indexes of the maximum value\nQuick one here, you might have become accustomed to the ability to return array indexes (row & col numbers) within the which() function by adding the arr.ind = T parameter. Turns out though this isn‚Äôt available in the which.max() function. I will leave you to ponder the ‚Äòwhy‚Äô background behind that‚Ä¶ü§î\nSimple solution, all you need to do is pass the index value returned from which.max() to the arrayInd() function. See below, with and without pipes depending on your style üëá\n\nmat &lt;- matrix(rnorm(400), nrow = 20)\n\n# With pipes\nmat |&gt; (\\(.)arrayInd(which.max(.), .dim = dim(.)))()\n\n# Without pipes\narrayInd(which.max(mat), .dim = dim(mat))\n\nHappy days üçπ"
  }
]